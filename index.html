<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>NP-Completeness - Textbook Chapter</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      font-family: serif;
      line-height: 1.6;
      max-width: 700px;
      margin: auto;
      padding: 2rem;
      color: #111;
      background: #fff;
    }
    h1, h2, h3 {
      font-weight: bold;
      margin-top: 2rem;
    }
    h1 { font-size: 1.8em; }
    h2 { font-size: 1.5em; }
    h3 { font-size: 1.3em; }
    p {
      margin: 1em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 1em 0 1em 2em;
    }
    code {
      background: #f4f4f4;
      padding: 0.2em 0.4em;
      font-family: monospace;
    }
  </style>
</head>
<body>
  <main>
    <ol><li data-list-text="34"><h1>NP-Completeness</h1><p>Almost all the algorithms we have studied thus far have been <i><b>polynomial-time al- gorithms</b></i>: on inputs of size n, their worst-case running time is O(nk) for some con- stant k. You might wonder whether <i>all </i>problems can be solved in polynomial time. The answer is no. For example, there are problems, such as Turing’s famous “Halt- ing Problem,” that cannot be solved by any computer, no matter how much time we allow. There are also problems that can be solved, but not in time O(nk) for any constant k. Generally, we think of problems that are solvable by polynomial-time algorithms as being tractable, or easy, and problems that require superpolynomial time as being intractable, or hard.</p><p>¤</p><p>The subject of this chapter, however, is an interesting class of problems, called the “NP-complete” problems, whose status is unknown. No polynomial-time al- gorithm has yet been discovered for an NP-complete problem, nor has anyone yet been able to prove that no polynomial-time algorithm can exist for any one of them. This so-called P  NP question has been one of the deepest, most perplexing open research problems in theoretical computer science since it was first posed in 1971. Several NP-complete problems are particularly tantalizing because they seem on the surface to be similar to problems that we know how to solve in polynomial time. In each of the following pairs of problems, one is solvable in polynomial time and the other is NP-complete, but the difference between problems appears to</p><p>be slight:</p><p>=</p><p><b>Shortest vs. longest simple paths: </b>In Chapter 24, we saw that even with negative edge weights, we can find <i>shortest </i>paths from a single source in a directed graph G  (V, E) in O(VE) time. Finding a <i>longest </i>simple path between two vertices is difficult, however. Merely determining whether a graph contains a simple path with at least a given number of edges is NP-complete.</p><p>=</p><p><b>Euler tour vs. hamiltonian cycle: </b>An <i><b>Euler tour </b></i>of a connected, directed graph G   (V, E) is a cycle that traverses each <i>edge </i>of G exactly once, although it is allowed to visit each vertex more than once. By Problem 22-3, we can determine whether a graph has an Euler tour in only O(E) time and, in fact,</p><p>=</p><p>we can find the edges of the Euler tour in O(E) time. A <i><b>hamiltonian cycle </b></i>of a directed graph G   (V, E) is a simple cycle that contains each <i>vertex </i>in V . Determining whether a directed graph has a hamiltonian cycle is NP-complete. (Later in this chapter, we shall prove that determining whether an <i>undirected </i>graph has a hamiltonian cycle is NP-complete.)</p><p>¬</p><p>ˆ   v</p><p>=   =   =</p><p>v ¬  ˆ ¬  v  ˆ ¬  v ¬</p><p>2-CNF satisfiability vs. 3-CNF satisfiability: A boolean formula contains vari- ables whose values are 0 or l; boolean connectives such as  (AND),  (OR), and  (NOT); and parentheses. A boolean formula is satisfiable if there exists some assignment of the values 0 and l to its variables that causes it to evaluate to l. We shall define terms more formally later in this chapter, but informally, a boolean formula is in k-conjunctive normal form, or k-CNF, if it is the AND of clauses of ORs of exactly k variables or their negations. For example, the boolean formula (x1 x2)  ( x1 x3)  ( x2 x3) is in 2-CNF. (It has the satisfying assignment x1 l, x2 0, x3 l.) Although we can deter- mine in polynomial time whether a 2-CNF formula is satisfiable, we shall see later in this chapter that determining whether a 3-CNF formula is satisfiable is NP-complete.</p><p>NP-completeness and the classes P and NP</p><p>Throughout this chapter, we shall refer to three classes of problems: P, NP, and NPC, the latter class being the NP-complete problems. We describe them infor- mally here, and we shall define them more formally later on.</p><p>The class P consists of those problems that are solvable in polynomial time. More specifically, they are problems that can be solved in time O(nk) for some constant k, where n is the size of the input to the problem. Most of the problems examined in previous chapters are in P.</p><p>The class NP consists of those problems that are “verifiable” in polynomial time. What do we mean by a problem being verifiable? If we were somehow given a “certificate” of a solution, then we could verify that the certificate is correct in time polynomial in the size of the input to the problem. For example, in the hamiltonian- cycle problem, given a directed graph G = (V, E), a certificate would be a se-</p><p>quence (v1<i>, </i>v2<i>, </i>v3<i>,. . . , </i>vjV j) of |<i>V </i>| vertices. We could easily check in polynomial</p><p>e    =      | | —         e</p><p>time that (vi , viC1)  E for i  l, 2, 3, . . . , V  l and that (vjV j, v1)  E as well.</p><p>As another example, for 3-CNF satisfiability, a certificate would be an assignment of values to variables. We could check in polynomial time that this assignment satisfies the boolean formula.</p><p>⊆</p><p>Any problem in P is also in NP, since if a problem is in P then we can solve it in polynomial time without even being supplied a certificate. We shall formalize this notion later in this chapter, but for now we can believe that P  NP. The open question is whether or not P is a proper subset of NP.</p><p>Informally, a problem is in the class NPC—and we refer to it as being <i><b>NP- complete</b></i>—if it is in NP and is as “hard” as any problem in NP. We shall formally define what it means to be as hard as any problem in NP later in this chapter. In the meantime, we will state without proof that if <i>any </i>NP-complete problem can be solved in polynomial time, then <i>every </i>problem in NP has a polynomial- time algorithm. Most theoretical computer scientists believe that the NP-complete problems are intractable, since given the wide range of NP-complete problems that have been studied to date—without anyone having discovered a polynomial- time solution to any of them—it would be truly astounding if all of them could be solved in polynomial time. Yet, given the effort devoted thus far to proving that NP-complete problems are intractable—without a conclusive outcome—we cannot rule out the possibility that the NP-complete problems are in fact solvable in polynomial time.</p><p>To become a good algorithm designer, you must understand the rudiments of the theory of NP-completeness. If you can establish a problem as NP-complete, you provide good evidence for its intractability. As an engineer, you would then do better to spend your time developing an approximation algorithm (see Chapter 35) or solving a tractable special case, rather than searching for a fast algorithm that solves the problem exactly. Moreover, many natural and interesting problems that on the surface seem no harder than sorting, graph searching, or network flow are in fact NP-complete. Therefore, you should become familiar with this remarkable class of problems.</p><p>Overview of showing problems to be NP-complete</p><p>The techniques we use to show that a particular problem is NP-complete differ fundamentally from the techniques used throughout most of this book to design and analyze algorithms. When we demonstrate that a problem is NP-complete, we are making a statement about how hard it is (or at least how hard we think it is), rather than about how easy it is. We are not trying to prove the existence of an efficient algorithm, but instead that no efficient algorithm is likely to exist. In this way, NP-completeness proofs bear some similarity to the proof in Section 8.1 of an Ω(n lg n)-time lower bound for any comparison sort algorithm; the specific techniques used for showing NP-completeness differ from the decision-tree method used in Section 8.1, however.</p><p>We rely on three key concepts in showing a problem to be NP-complete:</p><h3>Decision problems vs. optimization problems</h3><p>Many problems of interest are <i><b>optimization problems</b></i>, in which each feasible (i.e., “legal”) solution has an associated value, and we wish to find a feasible solution with the best value. For example, in a problem that we call SHORTEST-PATH,</p><p>we are given an undirected graph G and vertices u and v, and we wish to find a path from u to v that uses the fewest edges. In other words, SHORTEST-PATH is the single-pair shortest-path problem in an unweighted, undirected graph. NP- completeness applies directly not to optimization problems, however, but to <i><b>deci- sion problems</b></i>, in which the answer is simply “yes” or “no” (or, more formally, “1” or “0”).</p><p>Although NP-complete problems are confined to the realm of decision problems, we can take advantage of a convenient relationship between optimization problems and decision problems. We usually can cast a given optimization problem as a re- lated decision problem by imposing a bound on the value to be optimized. For example, a decision problem related to SHORTEST-PATH is PATH: given a di- rected graph G, vertices u and v, and an integer k, does a path exist from u to v consisting of at most k edges?</p><p>The relationship between an optimization problem and its related decision prob- lem works in our favor when we try to show that the optimization problem is “hard.” That is because the decision problem is in a sense “easier,” or at least “no harder.” As a specific example, we can solve PATH by solving SHORTEST-PATH and then comparing the number of edges in the shortest path found to the value of the decision-problem parameter k. In other words, if an optimization prob- lem is easy, its related decision problem is easy as well. Stated in a way that has more relevance to NP-completeness, if we can provide evidence that a decision problem is hard, we also provide evidence that its related optimization problem is hard. Thus, even though it restricts attention to decision problems, the theory of NP-completeness often has implications for optimization problems as well.</p><h3>Reductions</h3><p>The above notion of showing that one problem is no harder or no easier than an- other applies even when both problems are decision problems. We take advantage of this idea in almost every NP-completeness proof, as follows. Let us consider a decision problem A, which we would like to solve in polynomial time. We call the input to a particular problem an <i><b>instance </b></i>of that problem; for example, in PATH, an instance would be a particular graph G, particular vertices u and v of G, and a particular integer k. Now suppose that we already know how to solve a different decision problem B in polynomial time. Finally, suppose that we have a procedure that transforms any instance ˛ of A into some instance ˇ of B with the following characteristics:</p><ul><li data-list-text="●"><p>The transformation takes polynomial time.</p></li><li data-list-text="●"><p>The answers are the same. That is, the answer for ˛ is “yes” if and only if the answer for ˇ is also “yes.”</p></li></ul><p>instance  of <i>B</i></p><p>polynomial-time algorithm to decide <i>A</i></p><p>yes</p><p>no</p><div><p>polynomial-time reduction algorithm</p></div></li></ol><div><p>polynomial-time algorithm to decide <i>B</i></p></div><p> instance   </p><p>of <i>A</i></p><p>yes no</p><p><b>Figure 34.1 </b>How to use a polynomial-time reduction algorithm to solve a decision problem A in polynomial time, given a polynomial-time decision algorithm for another problem B. In polynomial time, we transform an instance ˛ of A into an instance ˇ of B, we solve B in polynomial time, and we use the answer for ˇ as the answer for ˛.</p><p>We call such a procedure a polynomial-time <i><b>reduction algorithm </b></i>and, as Fig- ure 34.1 shows, it provides us a way to solve problem A in polynomial time:</p><ol><li data-list-text="1."><p>Given an instance ˛ of problem A, use a polynomial-time reduction algorithm to transform it to an instance ˇ of problem B.</p></li><li data-list-text="2."><p>Run the polynomial-time decision algorithm for B on the instance ˇ.</p></li><li data-list-text="3."><p>Use the answer for ˇ as the answer for ˛.</p></li></ol><p>As long as each of these steps takes polynomial time, all three together do also, and so we have a way to decide on ˛ in polynomial time. In other words, by “reducing” solving problem A to solving problem B, we use the “easiness” of B to prove the “easiness” of A.</p><p>Recalling that NP-completeness is about showing how hard a problem is rather than how easy it is, we use polynomial-time reductions in the opposite way to show that a problem is NP-complete. Let us take the idea a step further, and show how we could use polynomial-time reductions to show that no polynomial-time algorithm can exist for a particular problem B. Suppose we have a decision problem A for which we already know that no polynomial-time algorithm can exist. (Let us not concern ourselves for now with how to find such a problem A.) Suppose further that we have a polynomial-time reduction transforming instances of A to instances of B. Now we can use a simple proof by contradiction to show that no polynomial- time algorithm can exist for B. Suppose otherwise; i.e., suppose that B has a polynomial-time algorithm. Then, using the method shown in Figure 34.1, we would have a way to solve problem A in polynomial time, which contradicts our assumption that there is no polynomial-time algorithm for A.</p><p>For NP-completeness, we cannot assume that there is absolutely no polynomial- time algorithm for problem A. The proof methodology is similar, however, in that we prove that problem B is NP-complete on the assumption that problem A is also NP-complete.</p><h3>A first NP-complete problem</h3><p>Because the technique of reduction relies on having a problem already known to be NP-complete in order to prove a different problem NP-complete, we need a “first” NP-complete problem. The problem we shall use is the circuit-satisfiability problem, in which we are given a boolean combinational circuit composed of AND, OR, and NOT gates, and we wish to know whether there exists some set of boolean inputs to this circuit that causes its output to be l. We shall prove that this first problem is NP-complete in Section 34.3.</p><p>Chapter outline</p><p>¤</p><p>This chapter studies the aspects of NP-completeness that bear most directly on the analysis of algorithms. In Section 34.1, we formalize our notion of “problem” and define the complexity class P of polynomial-time solvable decision problems. We also see how these notions fit into the framework of formal-language theory. Sec- tion 34.2 defines the class NP of decision problems whose solutions are verifiable in polynomial time. It also formally poses the P  NP question.</p><p>Section 34.3 shows we can relate problems via polynomial-time “reductions.” It defines NP-completeness and sketches a proof that one problem, called “circuit satisfiability,” is NP-complete. Having found one NP-complete problem, we show in Section 34.4 how to prove other problems to be NP-complete much more simply by the methodology of reductions. We illustrate this methodology by showing that two formula-satisfiability problems are NP-complete. With additional reductions, we show in Section 34.5 a variety of other problems to be NP-complete.</p><ol><ol><li data-list-text="34.1"><h2>Polynomial time</h2><p>We begin our study of NP-completeness by formalizing our notion of polynomial- time solvable problems. We generally regard these problems as tractable, but for philosophical, not mathematical, reasons. We can offer three supporting argu- ments.</p><p>First, although we may reasonably regard a problem that requires time ①(n100) to be intractable, very few practical problems require time on the order of such a high-degree polynomial. The polynomial-time computable problems encountered in practice typically require much less time. Experience has shown that once the first polynomial-time algorithm for a problem has been discovered, more efficient algorithms often follow. Even if the current best algorithm for a problem has a running time of ①(n100), an algorithm with a much better running time will likely soon be discovered.</p><h3>A first NP-complete problem</h3><p>Because the technique of reduction relies on having a problem already known to be NP-complete in order to prove a different problem NP-complete, we need a “first” NP-complete problem. The problem we shall use is the circuit-satisfiability problem, in which we are given a boolean combinational circuit composed of AND, OR, and NOT gates, and we wish to know whether there exists some set of boolean inputs to this circuit that causes its output to be l. We shall prove that this first problem is NP-complete in Section 34.3.</p><p>Chapter outline</p><p>¤</p><p>This chapter studies the aspects of NP-completeness that bear most directly on the analysis of algorithms. In Section 34.1, we formalize our notion of “problem” and define the complexity class P of polynomial-time solvable decision problems. We also see how these notions fit into the framework of formal-language theory. Sec- tion 34.2 defines the class NP of decision problems whose solutions are verifiable in polynomial time. It also formally poses the P  NP question.</p><p>Section 34.3 shows we can relate problems via polynomial-time “reductions.” It defines NP-completeness and sketches a proof that one problem, called “circuit satisfiability,” is NP-complete. Having found one NP-complete problem, we show in Section 34.4 how to prove other problems to be NP-complete much more simply by the methodology of reductions. We illustrate this methodology by showing that two formula-satisfiability problems are NP-complete. With additional reductions, we show in Section 34.5 a variety of other problems to be NP-complete.</p><ol><ol><li data-list-text="34.1"><h2>Polynomial time</h2><p>We begin our study of NP-completeness by formalizing our notion of polynomial- time solvable problems. We generally regard these problems as tractable, but for philosophical, not mathematical, reasons. We can offer three supporting argu- ments.</p><p>First, although we may reasonably regard a problem that requires time ①(n100) to be intractable, very few practical problems require time on the order of such a high-degree polynomial. The polynomial-time computable problems encountered in practice typically require much less time. Experience has shown that once the first polynomial-time algorithm for a problem has been discovered, more efficient algorithms often follow. Even if the current best algorithm for a problem has a running time of ①(n100), an algorithm with a much better running time will likely soon be discovered.</p><p>Second, for many reasonable models of computation, a problem that can be solved in polynomial time in one model can be solved in polynomial time in an- other. For example, the class of problems solvable in polynomial time by the serial random-access machine used throughout most of this book is the same as the class of problems solvable in polynomial time on abstract Turing machines.1 It is also the same as the class of problems solvable in polynomial time on a parallel com- puter when the number of processors grows polynomially with the input size.</p><p>Third, the class of polynomial-time solvable problems has nice closure proper- ties, since polynomials are closed under addition, multiplication, and composition. For example, if the output of one polynomial-time algorithm is fed into the input of another, the composite algorithm is polynomial. Exercise 34.1-5 asks you to show that if an algorithm makes a constant number of calls to polynomial-time subrou- tines and performs an additional amount of work that also takes polynomial time, then the running time of the composite algorithm is polynomial.</p><p>Abstract problems</p><p>D</p><p>D</p><p>D h   i</p><p>f g</p><p>To understand the class of polynomial-time solvable problems, we must first have a formal notion of what a “problem” is. We define an <i><b>abstract problem </b></i>Q to be a binary relation on a set I of problem <i><b>instances </b></i>and a set S of problem <i><b>solutions</b></i>. For example, an instance for SHORTEST-PATH is a triple consisting of a graph and two vertices. A solution is a sequence of vertices in the graph, with perhaps the empty sequence denoting that no path exists. The problem SHORTEST-PATH itself is the relation that associates each instance of a graph and two vertices with a shortest path in the graph that connects the two vertices. Since shortest paths are not necessarily unique, a given problem instance may have more than one solution. This formulation of an abstract problem is more general than we need for our purposes. As we saw above, the theory of NP-completeness restricts attention to <i><b>decision problems</b></i>: those having a yes/no solution. In this case, we can view an abstract decision problem as a function that maps the instance set I to the solution set 0, l . For example, a decision problem related to SHORTEST-PATH is the problem PATH that we saw earlier. If i   G, u, v, k is an instance of the decision problem PATH, then PATH(i )   l (yes) if a shortest path from u to v has at most k edges, and PATH(i )  0 (no) otherwise. Many abstract problems are not decision problems, but rather <i><b>optimization problems</b></i>, which require some value to be minimized or maximized. As we saw above, however, we can usually recast an</p><p>optimization problem as a decision problem that is no harder.</p><p>1See Hopcroft and Ullman [180] or Lewis and Papadimitriou [236] for a thorough treatment of the Turing-machine model.</p><p>Encodings</p><p>f         g              D</p><p>D f     g</p><p>In order for a computer program to solve an abstract problem, we must represent problem instances in a way that the program understands. An <i><b>encoding </b></i>of a set S of abstract objects is a mapping e from S to the set of binary strings.2 For example, we are all familiar with encoding the natural numbers N   0, l, 2, 3, 4, . . .  as the strings 0, l, l0, ll, l00, . . . . Using this encoding, e(l7)   l000l. If you have looked at computer representations of keyboard characters, you probably have seen the ASCII code, where, for example, the encoding of A is l00000l. We can encode a compound object as a binary string by combining the representations of its constituent parts. Polygons, graphs, functions, ordered pairs, programs—all can be encoded as binary strings.</p><p>D j j</p><p>Thus, a computer algorithm that “solves” some abstract decision problem actu- ally takes an encoding of a problem instance as input. We call a problem whose instance set is the set of binary strings a <i><b>concrete problem</b></i>. We say that an algo- rithm <i><b>solves </b></i>a concrete problem in time O(T (n)) if, when it is provided a problem instance i of length n   i , the algorithm can produce the solution in O(T (n)) time.3 A concrete problem is <i><b>polynomial-time solvable</b></i>, therefore, if there exists an algorithm to solve it in time O(nk) for some constant k.</p><p>We can now formally define the <i><b>complexity class </b></i><b>P </b>as the set of concrete deci- sion problems that are polynomial-time solvable.</p><p>2 f g</p><p>2     2 f  g</p><p>W ! f g</p><p>f g</p><p>We can use encodings to map abstract problems to concrete problems. Given an abstract decision problem Q mapping an instance set I to 0, l , an encoding e  I   0, l * can induce a related concrete decision problem, which we denote by e(Q).4 If the solution to an abstract-problem instance i  I is Q(i)  0, l , then the solution to the concrete-problem instance e(i)  0, l * is also Q(i). As a technicality, some binary strings might represent no meaningful abstract-problem instance. For convenience, we shall assume that any such string maps arbitrarily to 0. Thus, the concrete problem produces the same solutions as the abstract prob- lem on binary-string instances that represent the encodings of abstract-problem instances.</p><p>We would like to extend the definition of polynomial-time solvability from con- crete problems to abstract problems by using encodings as the bridge, but we would</p><p>2The codomain of e need not be <i>binary </i>strings; any set of strings over a finite alphabet having at least 2 symbols will do.</p><p>3We assume that the algorithm’s output is separate from its input. Because it takes at least one time step to produce each bit of the output and the algorithm takes O(T (n)) time steps, the size of the output is O(T (n)).</p><p>4We denote by f0, lg* the set of all strings composed of symbols from the set f0, lg.</p><p>D</p><p>D b  c C</p><p>like the definition to be independent of any particular encoding. That is, the ef- ficiency of solving a problem should not depend on how the problem is encoded. Unfortunately, it depends quite heavily on the encoding. For example, suppose that an integer k is to be provided as the sole input to an algorithm, and suppose that the running time of the algorithm is ①(k). If the integer k is provided in <i><b>unary</b></i>—a string of k 1s—then the running time of the algorithm is O(n) on length-n inputs, which is polynomial time. If we use the more natural binary representation of the integer k, however, then the input length is n   lg k   l. In this case, the run- ning time of the algorithm is ①(k)  ①(2n), which is exponential in the size of the input. Thus, depending on the encoding, the algorithm runs in either polynomial or superpolynomial time.</p><p>How we encode an abstract problem matters quite a bit to how we understand polynomial time. We cannot really talk about solving an abstract problem without first specifying an encoding. Nevertheless, in practice, if we rule out “expensive” encodings such as unary ones, the actual encoding of a problem makes little dif- ference to whether the problem can be solved in polynomial time. For example, representing integers in base 3 instead of binary has no effect on whether a prob- lem is solvable in polynomial time, since we can convert an integer represented in base 3 to an integer represented in base 2 in polynomial time.</p><p>D</p><p>2          D</p><p>2 f g</p><p>W f g ! f g</p><p>We say that a function f   0, l * 0, l * is polynomial-time computable if there exists a polynomial-time algorithm A that, given any input x   0, l *, produces as output f(x). For some set I of problem instances, we say that two en- codings e1 and e2 are polynomially related if there exist two polynomial-time com- putable functions f12 and f21 such that for any i  I , we have f12(e1(i))  e2(i) and f21(e2(i))  e1(i).5 That is, a polynomial-time algorithm can compute the en- coding e2(i) from the encoding e1(i), and vice versa. If two encodings e1 and e2 of an abstract problem are polynomially related, whether the problem is polynomial- time solvable or not is independent of which encoding we use, as the following lemma shows.</p><h3>Lemma 34.1</h3><p>Let Q be an abstract decision problem on an instance set I , and let e1 and e2 be polynomially related encodings on I . Then, e1(Q) 2 P if and only if e2(Q) 2 P.</p><p>5Technically, we also require the functions f12 and f21 to “map noninstances to noninstances.” A noninstance of an encoding e is a string x 2 f0, lg* such that there is no instance i for which e(i) D x. We require that f12(x) D y for every noninstance x of encoding e1, where y is some non- instance of e2, and that f21(x0) D y0 for every noninstance x0 of e2, where y0 is some noninstance of e1.</p><p>D j  j</p><p>j  j D</p><p>j  j D</p><p>Proof  We need only prove the forward direction, since the backward direction is symmetric. Suppose, therefore, that e1(Q) can be solved in time O(nk) for some constant k. Further, suppose that for any problem instance i , the encoding e1(i) can be computed from the encoding e2(i) in time O(nc) for some constant c, where n   e2(i) . To solve problem e2(Q), on input e2(i), we first compute e1(i) and then run the algorithm for e1(Q) on e1(i). How long does this take? Converting encodings takes time O(nc), and therefore e1(i)   O(nc), since the output of a serial computer cannot be longer than its running time. Solving the problem on e1(i) takes time O( e1(i) k)  O(nck), which is polynomial since both c and k are constants.</p><p>h i</p><p>Thus, whether an abstract problem has its instances encoded in binary or base 3 does not affect its “complexity,” that is, whether it is polynomial-time solvable or not; but if instances are encoded in unary, its complexity may change. In order to be able to converse in an encoding-independent fashion, we shall generally assume that problem instances are encoded in any reasonable, concise fashion, unless we specifically say otherwise. To be precise, we shall assume that the encoding of an integer is polynomially related to its binary representation, and that the encoding of a finite set is polynomially related to its encoding asa list of its elements, enclosed in braces and separated by commas. (ASCII is one such encoding scheme.) With such a “standard” encoding in hand, we can derive reasonable encodings of other mathematical objects, such as tuples, graphs, and formulas. To denote the standard encoding of an object, we shall enclose the object in angle braces. Thus, G denotes the standard encoding of a graph G.</p><p>As long as we implicitly use an encoding that is polynomially related to this standard encoding, we can talk directly about abstract problems without reference to any particular encoding, knowing that the choice of encoding has no effect on whether the abstract problem is polynomial-time solvable. Henceforth, we shall generally assume that all problem instances are binary strings encoded using the standard encoding, unless we explicitly specify the contrary. We shall also typically neglect the distinction between abstract and concrete problems. You should watch out for problems that arise in practice, however, in which a standard encoding is not obvious and the encoding does make a difference.</p><p>A formal-language framework</p><p>By focusing on decision problems, we can take advantage of the machinery of formal-language theory. Let’s review some definitions from that theory. An alphabet Σ is a finite set of symbols. A language L over Σ is any set of strings made up of symbols from Σ. For example, if Σ D f0, lg, the set</p><p>L D fl0, ll, l0l, lll, l0ll, ll0l, l000l, . . .g is the language of binary represen-</p><p>D f            g</p><p>;                               D f  g</p><p>tations of prime numbers. We denote the <i><b>empty string </b></i>by г, the <i><b>empty language </b></i>by , and the language of all strings over Σ by Σ*. For example, if Σ   0, l , then Σ* г, 0, l, 00, 0l, l0, ll, 000, . . . is the set of all binary strings. Every language L over Σ is a subset of Σ*.</p><p>D —</p><p>We can perform a variety of operations on languages. Set-theoretic operations, such as <i><b>union </b></i>and <i><b>intersection</b></i>, follow directly from the set-theoretic definitions. We define the <i><b>complement </b></i>of L by L  Σ* L. The <i><b>concatenation </b></i>L1L2 of two languages L1 and L2 is the language</p><p>L D fx1x2 W x1 2 L1 and x2 2 L2g .</p><p>The <i><b>closure </b></i>or <i><b>Kleene star </b></i>of a language L is the language</p><p>L* D fгg [ L [ L2 [ L3 [ · · · ,</p><p>where Lk is the language obtained by concatenating L to itself k times.</p><p>D f g</p><p>From the point of view of language theory, the set of instances for any decision problem Q is simply the set Σ*, where Σ  0, l . Since Q is entirely character- ized by those problem instances that produce a 1 (yes) answer, we can view Q as a language L over Σ D f0, lg, where</p><p>L D fx 2 Σ* W Q(x) D lg .</p><p>2</p><p>u, v  V,</p><p>For example, the decision problem PATH has the corresponding language PATH D fhG, u, v, kiW G D (V, E) is an undirected graph,</p><p>≥</p><p>k  0 is an integer, and</p><p>there exists a path from u to v in G</p><p>consisting of at most k edgesg .</p><p>(Where convenient, we shall sometimes use the same name—PATH in this case— to refer to both a decision problem and its corresponding language.)</p><p>D</p><p>2 f g</p><p>D f 2 f  g W   D g</p><p>The formal-language framework allows us to express concisely the relation be- tween decision problems and algorithms that solve them. We say that an al- gorithm A <i><b>accepts </b></i>a string x   0, l * if, given input x, the algorithm’s out- put A(x) is l. The language <i><b>accepted </b></i>by an algorithm A is the set of strings L  x  0, l * A(x)  l , that is, the set of strings that the algorithm accepts. An algorithm A <i><b>rejects </b></i>a string x if A(x)  0.</p><p>/2</p><p>Even if language L is accepted by an algorithm A, the algorithm will not neces- sarily reject a string x  L provided as input to it. For example, the algorithm may loop forever. A language L is <i><b>decided </b></i>by an algorithm A if every binary string in L is accepted by A and every binary string not in L is rejected by A. A lan- guage L is <i><b>accepted in polynomial time </b></i>by an algorithm A if it is accepted by A and if in addition there exists a constant k such that for any length-n string x 2 L,</p><p>f g</p><p>2 f  g                     2</p><p>algorithm A accepts x in time O(nk). A language L is decided in polynomial time by an algorithm A if there exists a constant k such that for any length-n string x  0, l *, the algorithm correctly decides whether x  L in time O(nk). Thus, to accept a language, an algorithm need only produce an answer when provided a string in L, but to decide a language, it must correctly accept or reject every string in 0, l *.</p><p>As an example, the language PATH can be accepted in polynomial time. One polynomial-time accepting algorithm verifies that G encodes an undirected graph, verifies that u and v are vertices in G, uses breadth-first search to compute a short- est path from u to v in G, and then compares the number of edges on the shortest path obtained with k. If G encodes an undirected graph and the path found from u to v has at most k edges, the algorithm outputs l and halts. Otherwise, the algo- rithm runs forever. This algorithm does not decide PATH, however, since it does not explicitly output 0 for instances in which a shortest path has more than k edges. A decision algorithm for PATH must explicitly reject binary strings that do not be- long to PATH. For a decision problem such as PATH, such a decision algorithm is easy to design: instead of running forever when there is not a path from u to v with at most k edges, it outputs 0 and halts. (It must also output 0 and halt if the input encoding is faulty.) For other problems, such as Turing’s Halting Problem, there exists an accepting algorithm, but no decision algorithm exists.</p><p>We can informally define a <i><b>complexity class </b></i>as a set of languages, membership in which is determined by a <i><b>complexity measure</b></i>, such as running time, of an algorithm that determines whether a given string x belongs to language L. The actual definition of a complexity class is somewhat more technical.6</p><p>Using this language-theoretic framework, we can provide an alternative defini- tion of the complexity class P:</p><p>P D fL $ f0, lg* W there exists an algorithm A that decides L</p><p>in polynomial timeg .</p><p>In fact, P is also the class of languages that can be accepted in polynomial time.</p><h3>Theorem 34.2</h3><p>P D fL W L is accepted by a polynomial-time algorithmg .</p><p><i><b>Proof  </b></i>Because the class of languages decided by polynomial-time algorithms is a subset of the class of languages accepted by polynomial-time algorithms, we need only show that if L is accepted by a polynomial-time algorithm, it is de- cided by a polynomial-time algorithm. Let L be the language accepted by some</p><p>6For more on complexity classes, see the seminal paper by Hartmanis and Stearns [162].</p><p>polynomial-time algorithm A. We shall use a classic “simulation” argument to construct another polynomial-time algorithm A0 that decides L. Because A ac- cepts L in time O(nk) for some constant k, there also exists a constant c such that A accepts L in at most cnk steps. For any input string x, the algorithm A0 simulates cnk steps of A. After simulating cnk steps, algorithm A0 inspects the be- havior of A. If A has accepted x, then A0 accepts x by outputting a l. If A has not accepted x, then A0 rejects x by outputting a 0. The overhead of A0 simulating A does not increase the running time by more than a polynomial factor, and thus A0 is a polynomial-time algorithm that decides L.</p><p>2</p><p>Note that the proof of Theorem 34.2 is nonconstructive. For a given language L  P, we may not actually know a bound on the running time for the algorithm A that accepts L. Nevertheless, we know that such a bound exists, and therefore, that an algorithm A0 exists that can check the bound, even though we may not be able to find the algorithm A0 easily.</p><p>Exercises</p><h3>34.1-1</h3><p>g</p><p>2  ≥</p><p>D fh    i W  D</p><p>Define the optimization problem LONGEST-PATH-LENGTH as the relation that associates each instance of an undirected graph and two vertices with the num- ber of edges in a longest simple path between the two vertices. Define the de- cision problem LONGEST-PATH    G, u, v, k   G   (V, E) is an undi- rected graph, u, v   V , k   0 is an integer, and there exists a simple path from u to v in G consisting of at least k edges . Show that the optimization prob- lem LONGEST-PATH-LENGTH can be solved in polynomial time if and only if</p><p>LONGEST-PATH 2 P.</p><h3>34.1-2</h3><p>Give a formal definition for the problem of finding the longest simple cycle in an undirected graph. Give a related decision problem. Give the language correspond- ing to the decision problem.</p><h3>34.1-3</h3><p>Give a formal encoding of directed graphs as binary strings using an adjacency- matrix representation. Do the same using an adjacency-list representation. Argue that the two representations are polynomially related.</p><h3>34.1-4</h3><p>Is the dynamic-programming algorithm for the 0-1 knapsack problem that is asked for in Exercise 16.2-2 a polynomial-time algorithm? Explain your answer.</p><h3>34.1-5</h3><p>Show that if an algorithm makes at most a constant number of calls to polynomial- time subroutines and performs an additional amount of work that also takes polyno- mial time, then it runs in polynomial time. Also show that a polynomial number of calls to polynomial-time subroutines may result in an exponential-time algorithm.</p><h3>34.1-6</h3><p>Show that the class P, viewed as a set of languages, is closed under union, inter- section, concatenation, complement, and Kleene star. That is, if L1, L2 2 P, then L1 [ L2 2 P, L1 \ L2 2 P, L1L2 2 P, L1 2 P, and L1* 2 P.</p></li><li data-list-text="34.2"><h2>Polynomial-time verification</h2></li></ol></ol><p>h   i</p><p>We now look at algorithms that verify membership in languages. For example, suppose that for a given instance G, u, v, k of the decision problem PATH, we are also given a path p from u to v. We can easily check whether p is a path in G and whether the length of p is at most k, and if so, we can view p as a “certificate” that the instance indeed belongs to PATH. For the decision problem PATH, this certificate doesn’t seem to buy us much. After all, PATH belongs to P—in fact, we can solve PATH in linear time—and so verifying membership from a given certificate takes as long as solving the problem from scratch. We shall now examine a problem for which we know of no polynomial-time decision algorithm and yet, given a certificate, verification is easy.</p><p>Hamiltonian cycles</p><p>D</p><p>The problem of finding a hamiltonian cycle in an undirected graph has been stud- ied for over a hundred years. Formally, a <i><b>hamiltonian cycle </b></i>of an undirected graph G   (V, E) is a simple cycle that contains each vertex in V . A graph that con- tains a hamiltonian cycle is said to be <i><b>hamiltonian</b></i>; otherwise, it is <i><b>nonhamilto- nian</b></i>. The name honors W. R. Hamilton, who described a mathematical game on the dodecahedron (Figure 34.2(a)) in which one player sticks five pins in any five consecutive vertices and the other player must complete the path to form a cycle</p><h3>34.1-5</h3><p>Show that if an algorithm makes at most a constant number of calls to polynomial- time subroutines and performs an additional amount of work that also takes polyno- mial time, then it runs in polynomial time. Also show that a polynomial number of calls to polynomial-time subroutines may result in an exponential-time algorithm.</p><h3>34.1-6</h3><p>Show that the class P, viewed as a set of languages, is closed under union, inter- section, concatenation, complement, and Kleene star. That is, if L1, L2 e P, then L1 [ L2 e P, L1 \ L2 e P, L1L2 e P, L1 e P, and L1* e P.</p></li><li data-list-text="34.2"><h2>Polynomial-time verification</h2><p>(   )</p><p>We now look at algorithms that verify membership in languages. For example, suppose that for a given instance G, u, v, k of the decision problem PATH, we are also given a path p from u to v. We can easily check whether p is a path in G and whether the length of p is at most k, and if so, we can view p as a “certificate” that the instance indeed belongs to PATH. For the decision problem PATH, this certificate doesn’t seem to buy us much. After all, PATH belongs to P—in fact, we can solve PATH in linear time—and so verifying membership from a given certificate takes as long as solving the problem from scratch. We shall now examine a problem for which we know of no polynomial-time decision algorithm and yet, given a certificate, verification is easy.</p><p>Hamiltonian cycles</p><p>=</p><p>The problem of finding a hamiltonian cycle in an undirected graph has been stud- ied for over a hundred years. Formally, a <i><b>hamiltonian cycle </b></i>of an undirected graph G   (V, E) is a simple cycle that contains each vertex in V . A graph that con- tains a hamiltonian cycle is said to be <i><b>hamiltonian</b></i>; otherwise, it is <i><b>nonhamilto- nian</b></i>. The name honors W. R. Hamilton, who described a mathematical game on the dodecahedron (Figure 34.2(a)) in which one player sticks five pins in any five consecutive vertices and the other player must complete the path to form a cycle</p><p>(a)                         (b)</p><h4>Figure 34.2  (a) A graph representing the vertices, edges, and faces of a dodecahedron, with a hamiltonian cycle shown by shaded edges. (b) A bipartite graph with an odd number of vertices. Any such graph is nonhamiltonian.</h4><p>containing all the vertices.7 The dodecahedron is hamiltonian, and Figure 34.2(a) shows one hamiltonian cycle. Not all graphs are hamiltonian, however. For ex- ample, Figure 34.2(b) shows a bipartite graph with an odd number of vertices. Exercise 34.2-2 asks you to show that all such graphs are nonhamiltonian.</p><p>We can define the <i><b>hamiltonian-cycle problem</b></i>, “Does a graph G have a hamil- tonian cycle?” as a formal language:</p><p>HAM-CYCLE = {(G): G is a hamiltonian graph} .</p><p>,</p><p>( )</p><p>How might an algorithm decide the language HAM-CYCLE? Given a problem instance G , one possible decision algorithm lists all permutations of the vertices of G and then checks each permutation to see if it is a hamiltonian path. What is the running time of this algorithm? If we use the “reasonable” encoding of a graph as its adjacency matrix, the number m of vertices in the graph is Ω(  n), where n = j(G)j is the length of the encoding of G. There are m! possible permutations</p><p>7In a letter dated 17 October 1856 to his friend John T. Graves, Hamilton [157, p. 624] wrote, “I have found that some young persons have been much amused by trying a new mathematical game which the Icosion furnishes, one person sticking five pins in any five consecutive points . . . and the other player then aiming to insert, which by the theory in this letter can always be done, fifteen other pins, in cyclical succession, so as to cover all the other points, and to end in immediate proximity to the pin wherewith his antagonist had begun.”</p><p>=   =</p><p>of the vertices, and therefore the running time is Ω(m!)   Ω(,n !)   Ω(2pn), which is not O(nk) for any constant k. Thus, this naive algorithm does not run in polynomial time. In fact, the hamiltonian-cycle problem is NP-complete, as we shall prove in Section 34.5.</p><p>Verification algorithms</p><p>Consider a slightly easier problem. Suppose that a friend tells you that a given graph G is hamiltonian, and then offers to prove it by giving you the vertices in order along the hamiltonian cycle. It would certainly be easy enough to verify the proof: simply verify that the provided cycle is hamiltonian by checking whether it is a permutation of the vertices of V and whether each of the consecutive edges along the cycle actually exists in the graph. You could certainly implement this verification algorithm to run in O(n2) time, where n is the length of the encoding of G. Thus, a proof that a hamiltonian cycle exists in a graph can be verified in polynomial time.</p><p>=</p><p>We define a <i><b>verification algorithm </b></i>as being a two-argument algorithm A, where one argument is an ordinary input string x and the other is a binary string y called a <i><b>certificate</b></i>. A two-argument algorithm A <i><b>verifies </b></i>an input string x if there exists a certificate y such that A(x, y)  l. The <i><b>language verified </b></i>by a verification algorithm A is</p><p>L = {x e {0, l}* : there exists y e {0, l}* such that A(x, y) = l} .</p><p>/e                      e</p><p>e</p><p>e</p><p>Intuitively, an algorithm A verifies a language L if for any string x  L, there exists a certificate y that A can use to prove that x  L. Moreover, for any string x   L, there must be no certificate proving that x   L. For example, in the hamiltonian-cycle problem, the certificate is the list of vertices in some hamilto- nian cycle. If a graph is hamiltonian, the hamiltonian cycle itself offers enough information to verify this fact. Conversely, if a graph is not hamiltonian, there can be no list of vertices that fools the verification algorithm into believing that the graph is hamiltonian, since the verification algorithm carefully checks the proposed “cycle” to be sure.</p><p>The complexity class NP</p><p>The <i><b>complexity class </b></i><b>NP </b>is the class of languages that can be verified by a poly- nomial-time algorithm.8 More precisely, a language L belongs to NP if and only if there exist a two-input polynomial-time algorithm A and a constant c such that</p><p>L = {x e {0, l}* : there exists a certificate y with jyj = O(jxjc )</p><p>such that A(x, y) = l} .</p><p>We say that algorithm A <i><b>verifies </b></i>language L <i><b>in polynomial time</b></i>.</p><p>⊆</p><p>e     e</p><p>e</p><p>From our earlier discussion on the hamiltonian-cycle problem, we now see that HAM-CYCLE  NP. (It is always nice to know that an important set is nonempty.) Moreover, if L  P, then L  NP, since if there is a polynomial-time algorithm to decide L, the algorithm can be easily converted to a two-argument verification algorithm that simply ignores any certificate and accepts exactly those input strings it determines to be in L. Thus, P  NP.</p><p>=</p><p>It is unknown whether P  NP, but most researchers believe that P and NP are not the same class. Intuitively, the class P consists of problems that can be solved quickly. The class NP consists of problems for which a solution can be verified quickly. You may have learned from experience that it is often more difficult to solve a problem from scratch than to verify a clearly presented solution, especially when working under time constraints. Theoretical computer scientists generally believe that this analogy extends to the classes P and NP, and thus that NP includes languages that are not in P.</p><p>≠</p><p>There is more compelling, though not conclusive, evidence that P  NP—the existence of languages that are “NP-complete.” We shall study this class in Sec- tion 34.3.</p><p>≠</p><p>=</p><p>e</p><p>e      e</p><p>Many other fundamental questions beyond the P  NP question remain unre- solved. Figure 34.3 shows some possible scenarios. Despite much work by many researchers, no one even knows whether the class NP is closed under comple- ment. That is, does L  NP imply L  N<u>P?</u> We can define the <i><b>complexity class </b></i><b>co-NP </b>as the set of languages L such that L  NP. We can restate the question of whether NP is closed under complement as whether NP  co-NP. Since P is closed under complement (Exercise 34.1-6), it follows from Exercise 34.2-9 that</p><p>P ⊆ NP \ co-NP. Once again, however, no one knows whether P = NP \ co-NP or whether there is some language in NP \ co-NP — P.</p><p>8The name “NP” stands for “nondeterministic polynomial time.” The class NP was originally studied in the context of nondeterminism, but this book uses the somewhat simpler yet equivalent notion of verification. Hopcroft and Ullman [180] give a good presentation of NP-completeness in terms of nondeterministic models of computation.</p><p>P = NP = co-NP</p><p>NP = co-NP P</p><p>(a)                    (b)</p><p>co-NP</p><p>P = NP  co-NP</p><p>NP</p><p>co-NP  NP  co-NP  NP P</p><p>(c)                    (d)</p><p><b>Figure 34.3  </b>Four possibilities for relationships among complexity classes. In each diagram, one region enclosing another indicates a proper-subset relation. <b>(a) </b>P = NP = co-NP. Most researchers regard this possibility as the most unlikely. <b>(b) </b>If NP is closed under complement, then NP = co-NP, but it need not be the case that P = NP. <b>(c) </b>P = NP\co-NP, but NP is not closed under complement.</p><p><b>(d) </b>NP ≠ co-NP and P ≠ NP \ co-NP. Most researchers regard this possibility as the most likely.</p><p>Thus, our understanding of the precise relationship between P and NP is woe- fully incomplete. Nevertheless, even though we might not be able to prove that a particular problem is intractable, if we can prove that it is NP-complete, then we have gained valuable information about it.</p><p>Exercises</p><h3>34.2-1</h3><p>}                 e</p><p>= {(  ) : </p><p>Consider the language GRAPH-ISOMORPHISM   G1, G2 G1 and G2 are isomorphic graphs . Prove that GRAPH-ISOMORPHISM  NP by describing a polynomial-time algorithm to verify the language.</p><h3>34.2-2</h3><p>Prove that if G is an undirected bipartite graph with an odd number of vertices, then G is nonhamiltonian.</p><h3>34.2-3</h3><p>e</p><p>Show that if HAM-CYCLE  P, then the problem of listing the vertices of a hamiltonian cycle, in order, is polynomial-time solvable.</p><h3>34.2-4</h3><p>Prove that the class NP of languages is closed under union, intersection, concate- nation, and Kleene star. Discuss the closure of NP under complement.</p><h3>34.2-5</h3><p>Show that any language in NP can be decided by an algorithm running in time 2O(nk) for some constant k.</p><h3>34.2-6</h3><p>A <i><b>hamiltonian path </b></i>in a graph is a simple path that visits every vertex exactly once. Show that the language HAM-PATH = {(G, u, v) : there is a hamiltonian path from u to v in graph G} belongs to NP.</p><h3>34.2-7</h3><p>Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved in polynomial time on directed acyclic graphs. Give an efficient algorithm for the problem.</p><h3>34.2-8</h3><p>Let ø be a boolean formula constructed from the boolean input variables x1, x2,</p><p>¬    ˆ   v</p><p>..., xk, negations ( ), ANDs ( ), ORs ( ), and parentheses. The formula ø is a <i><b>tautology </b></i>if it evaluates to l for every assignment of l and 0 to the input variables. Define TAUTOLOGY as the language of boolean formulas that are tautologies. Show that TAUTOLOGY e co-NP.</p><h3>34.2-9</h3><p>Prove that P ⊆ co-NP.</p><h3>34.2-10</h3><p>Prove that if NP ≠ co-NP, then P ≠ NP.</p><h3>34.2-11</h3><p>Let G be a connected, undirected graph with at least 3 vertices, and let G3 be the graph obtained by connecting all pairs of vertices that are connected by a path in G of length at most 3. Prove that G3 is hamiltonian. (<i>Hint: </i>Construct a spanning tree for G, and use an inductive argument.)</p></li><li data-list-text="34.3"><h2>NP-completeness and reducibility</h2><p>≠</p><p>=</p><p>Perhaps the most compelling reason why theoretical computer scientists believe that P  NP comes from the existence of the class of “NP-complete” problems. This class has the intriguing property that if <i>any </i>NP-complete problem can be solved in polynomial time, then <i>every </i>problem in NP has a polynomial-time solu- tion, that is, P  NP. Despite years of study, though, no polynomial-time algorithm has ever been discovered for any NP-complete problem.</p><p>e  —</p><p>—</p><p>The language HAM-CYCLE is one NP-complete problem. If we could decide HAM-CYCLE in polynomial time, then we could solve every problem in NP in polynomial time. In fact, if NP  P should turn out to be nonempty, we could say with certainty that HAM-CYCLE  NP  P.</p><p>The NP-complete languages are, in a sense, the “hardest” languages in NP. In this section, we shall show how to compare the relative “hardness” of languages using a precise notion called “polynomial-time reducibility.” Then we formally define the NP-complete languages, and we finish by sketching a proof that one such language, called CIRCUIT-SAT, is NP-complete. In Sections 34.4 and 34.5, we shall use the notion of reducibility to show that many other problems are NP- complete.</p><p>Reducibility</p><p>C =</p><p>C  =            C  C  =</p><p>Intuitively, a problem Q can be reduced to another problem Q0 if any instance of Q can be “easily rephrased” as an instance of Q0, the solution to which provides a solution to the instance of Q. For example, the problem of solving linear equations in an indeterminate x reduces to the problem of solving quadratic equations. Given an instance ax  b   0, we transform it to 0x2 ax  b   0, whose solution provides a solution to ax   b   0. Thus, if a problem Q reduces to another problem Q0, then Q is, in a sense, “no harder to solve” than Q0.</p><p>Returning to our formal-language framework for decision problems, we say that a language L1 is polynomial-time reducible to a language L2, written L1 ≤P L2, if there exists a polynomial-time computable function f W {0, l}* → {0, l}* such that for all x e {0, l}*,</p><p>x e L1 if and only if f(x) e L2 .                       (34.1)</p><p>We call the function f the <i><b>reduction function</b></i>, and a polynomial-time algorithm F that computes f is a <i><b>reduction algorithm</b></i>.</p><p>Figure 34.4 illustrates the idea of a polynomial-time reduction from a lan- guage L1 to another language L2. Each language is a subset of {0, l}*. The reduction function f provides a polynomial-time mapping such that if x e L1,</p><p>{0,1}*</p><p>f</p><p>{0,1}*</p><p>L1</p><p>L2</p><p>Figure 34.4  An illustration of a polynomial-time reduction from a language L1 to a language L2 via a reduction function f . For any input x e {0, l}*, the question of whether x e L1 has the same answer as the question of whether f(x) e L2.</p><p>e                     e</p><p>e         /e       /e</p><p>then f(x)  L2. Moreover, if x  L1, then f(x)  L2. Thus, the reduction func- tion maps any instance x of the decision problem represented by the language L1 to an instance f(x) of the problem represented by L2. Providing an answer to whether f(x)  L2 directly provides the answer to whether x  L1.</p><p>Polynomial-time reductions give us a powerful tool for proving that various lan- guages belong to P.</p><h3>Lemma 34.3</h3><p>If L1, L2 ⊆ {0, l}* are languages such that L1 ≤P L2, then L2 e P implies</p><p>L1 e P.</p><p><i><b>Proof </b></i>Let A2 be a polynomial-time algorithm that decides L2, and let F be a polynomial-time reduction algorithm that computes the reduction function f . We shall construct a polynomial-time algorithm A1 that decides L1.</p><p>e</p><p>e { }</p><p>Figure 34.5 illustrates how we construct A1. For a given input x    0, l *, algorithm A1 uses F to transform x into f(x), and then it uses A2 to test whether f(x)  L2. Algorithm A1 takes the output from algorithm A2 and produces that answer as its own output.</p><p>The correctness of A1 follows from condition (34.1). The algorithm runs in poly- nomial time, since both F and A2 run in polynomial time (see Exercise 34.1-5).</p><p>NP-completeness</p><p>Polynomial-time reductions provide a formal means for showing that one prob- lem is at least as hard as another, to within a polynomial-time factor. That is, if L1 ≤P L2, then L1 is not more than a polynomial factor harder than L2, which is</p><p>x</p><p>f(x)</p><p>yes, f(x) e L2 yes, x e L1</p><p>A1</p><p>no, f(x) /e L2 no, x /e L1</p><div><p>F</p></div></li></ol></ol><div><p>A2</p></div><p>Figure 34.5  The proof of Lemma 34.3. The algorithm F is a reduction algorithm that computes the reduction function f from L1 to L2 in polynomial time, and A2 is a polynomial-time algorithm that decides L2. Algorithm A1 decides whether x e L1 by using F to transform any input x into f(x)</p><p>and then using A2 to decide whether f(x) e L2.</p><p>why the “less than or equal to” notation for reduction is mnemonic. We can now define the set of NP-complete languages, which are the hardest problems in NP.</p><p>A language L ⊆ {0, l}* is <i><b>NP-complete </b></i>if</p><ol><li data-list-text="1."><p>L e NP, and</p></li><li data-list-text="2."><p>L0 ≤P L for every L0 e NP.</p></li></ol><p>If a language L satisfies property 2, but not necessarily property 1, we say that L</p><p>is <i><b>NP-hard</b></i>. We also define NPC to be the class of NP-complete languages.</p><p>As the following theorem shows, NP-completeness is at the crux of deciding whether P is in fact equal to NP.</p><h3>Theorem 34.4</h3><p>=</p><p>If any NP-complete problem is polynomial-time solvable, then P  NP. Equiva- lently, if any problem in NP is not polynomial-time solvable, then no NP-complete problem is polynomial-time solvable.</p><p>e</p><p>≤</p><p>e          e          e</p><p><i><b>Proof  </b></i>Suppose that L   P and also that L   NPC. For any L0 NP, we have L0 P L by property 2 of the definition of NP-completeness.  Thus, by Lemma 34.3, we also have that L0 P, which proves the first statement of the theorem.</p><p>To prove the second statement, note that it is the contrapositive of the first state- ment.</p><p>≠</p><p>≠</p><p>=</p><p>It is for this reason that research into the P  NP question centers around the NP-complete problems. Most theoretical computer scientists believe that P  NP, which leads to the relationships among P, NP, and NPC shown in Figure 34.6. But, for all we know, someone may yet come up with a polynomial-time algo- rithm for an NP-complete problem, thus proving that P  NP. Nevertheless, since no polynomial-time algorithm for any NP-complete problem has yet been discov-</p><p>NP</p><p>NPC</p><p>P</p><p><b>Figure 34.6  </b>How most theoretical computer scientists view the relationships among P, NP, and NPC. Both P and NPC are wholly contained within NP, and P \ NPC = ;.</p><p>ered, a proof that a problem is NP-complete provides excellent evidence that it is intractable.</p><p>Circuit satisfiability</p><p>We have defined the notion of an NP-complete problem, but up to this point, we have not actually proved that any problem is NP-complete. Once we prove that at least one problem is NP-complete, we can use polynomial-time reducibility as a tool to prove other problems to be NP-complete. Thus, we now focus on demon- strating the existence of an NP-complete problem: the circuit-satisfiability prob- lem.</p><p>Unfortunately, the formal proof that the circuit-satisfiability problem is NP- complete requires technical detail beyond the scope of this text. Instead, we shall informally describe a proof that relies on a basic understanding of boolean combi- national circuits.</p><p>{ }</p><p>Boolean combinational circuits are built from boolean combinational elements that are interconnected by wires. A <i><b>boolean combinational element </b></i>is any circuit element that has a constant number of boolean inputs and outputs and that performs a well-defined function. Boolean values are drawn from the set 0, l , where 0 represents FALSE and l represents TRUE.</p><p>The boolean combinational elements that we use in the circuit-satisfiability prob- lem compute simple boolean functions, and they are known as <i><b>logic gates</b></i>. Fig- ure 34.7 shows the three basic logic gates that we use in the circuit-satisfiability problem: the <i><b>NOT gate </b></i>(or <i><b>inverter</b></i>), the <i><b>AND gate</b></i>, and the <i><b>OR gate</b></i>. The NOT gate takes a single binary <i><b>input </b></i>x, whose value is either 0 or l, and produces a binary <i><b>output </b></i>´ whose value is opposite that of the input value. Each of the other two gates takes two binary inputs x and y and produces a single binary output ´.</p><p>We can describe the operation of each gate, and of any boolean combinational element, by a <i><b>truth table</b></i>, shown under each gate in Figure 34.7. A truth table gives the outputs of the combinational element for each possible setting of the inputs. For</p><p>x             x</p><p>x        z              z z y              y</p><p>x ¬x</p><p>0</p><p>1</p><p>1</p><p>0</p><p>x y x ˆ y</p><p>0 0  0</p><p>0 1  0</p><p>1 0  0</p><p>1 1  1</p><p>x y x v y</p><p>0 0  0</p><p>0 1  1</p><p>1 0  1</p><p>1 1  1</p><p>(a)             (b)             (c)</p><h4>Figure 34.7 Three basic logic gates, with binary inputs and outputs. Under each gate is the truth table that describes the gate’s operation. (a) The NOT gate. (b) The AND gate. (c) The OR gate.</h4><p>v =</p><p>ˆ               v</p><p>=            =            ¬</p><p>=</p><p>example, the truth table for the OR gate tells us that when the inputs are x  0 and y  l, the output value is ´  l. We use the symbols  to denote the NOT function,  to denote the AND function, and  to denote the OR function. Thus, for example, 0  l  l.</p><p>We can generalize AND and OR gates to take more than two inputs. An AND gate’s output is l if all of its inputs are l, and its output is 0 otherwise. An OR gate’s output is l if any of its inputs are l, and its output is 0 otherwise.</p><p>(  =   =   = )</p><p>A <i><b>boolean combinational circuit </b></i>consists of one or more boolean combinational elements interconnected by <i><b>wires</b></i>. A wire can connect the output of one element to the input of another, thereby providing the output value of the first element as an input value of the second. Figure 34.8 shows two similar boolean combinational circuits, differing in only one gate. Part (a) of the figure also shows the values on the individual wires, given the input x1 l, x2 l, x3 0 . Although a single wire may have no more than one combinational-element output connected to it, it can feed several element inputs. The number of element inputs fed by a wire is called the <i><b>fan-out </b></i>of the wire. If no element output is connected to a wire, the wire is a <i><b>circuit input</b></i>, accepting input values from an external source. If no element input is connected to a wire, the wire is a <i><b>circuit output</b></i>, providing the results of the circuit’s computation to the outside world. (An internal wire can also fan out to a circuit output.) For the purpose of defining the circuit-satisfiability problem, we limit the number of circuit outputs to l, though in actual hardware design, a boolean combinational circuit may have multiple outputs.</p><p>=</p><p>Boolean combinational circuits contain no cycles. In other words, suppose we create a directed graph G  (V, E) with one vertex for each combinational element and with k directed edges for each wire whose fan-out is k; the graph contains a directed edge (u, v) if a wire connects the output of element u to an input of element v. Then G must be acyclic.</p><p>1</p><p>1</p><p>1</p><p>1</p><p>1</p><p>0</p><p>1</p><p>0</p><p>1</p><p>1</p><p>1</p><p>1</p><p>1</p><p>0</p><div><p>1</p></div><div><p>1</p></div><p>x1                                x1</p><p>x2                                x2</p><p> 1 </p><p>x3                                x3</p><p>(a)                           (b)</p><p>=  = )</p><p>Figure 34.8 Two instances of the circuit-satisfiability problem. (a) The assignment (x1 = l, x2 l, x3 0 to the inputs of this circuit causes the output of the circuit to be l. The circuit is therefore satisfiable. (b) No assignment to the inputs of this circuit can cause the output of the</p><p>circuit to be l. The circuit is therefore unsatisfiable.</p><p>(  =   =   = )</p><p>A <i><b>truth assignment </b></i>for a boolean combinational circuit is a set of boolean input values. We say that a one-output boolean combinational circuit is <i><b>satisfiable </b></i>if it has a <i><b>satisfying assignment</b></i>: a truth assignment that causes the output of the circuit to be l. For example, the circuit in Figure 34.8(a) has the satisfying assignment x1 l, x2 l, x3 0 , and so it is satisfiable. As Exercise 34.3-1 asks you to show, no assignment of values to x1, x2, and x3 causes the circuit in Figure 34.8(b) to produce a l output; it always produces 0, and so it is unsatisfiable.</p><p>( )</p><p>The <i><b>circuit-satisfiability problem </b></i>is, “Given a boolean combinational circuit composed of AND, OR, and NOT gates, is it satisfiable?” In order to pose this question formally, however, we must agree on a standard encoding for circuits. The <i><b>size </b></i>of a boolean combinational circuit is the number of boolean combina- tional elements plus the number of wires in the circuit. We could devise a graphlike encoding that maps any given circuit S into a binary string S  whose length is polynomial in the size of the circuit itself. As a formal language, we can therefore define</p><p>CIRCUIT-SAT = {(S )W S is a satisfiable boolean combinational circuit} .</p><p>The circuit-satisfiability problem arises in the area of computer-aided hardware optimization. If a subcircuit always produces 0, that subcircuit is unnecessary; the designer can replace it by a simpler subcircuit that omits all logic gates and provides the constant 0 value as its output. You can see why we would like to have a polynomial-time algorithm for this problem.</p><p>Given a circuit S , we might attempt to determine whether it is satisfiable by simply checking all possible assignments to the inputs. Unfortunately, if the circuit has k inputs, then we would have to check up to 2k possible assignments. When</p><p>the size of S is polynomial in k, checking each one takes Ω(2k) time, which is superpolynomial in the size of the circuit.9 In fact, as we have claimed, there is strong evidence that no polynomial-time algorithm exists that solves the circuit- satisfiability problem because circuit satisfiability is NP-complete. We break the proof of this fact into two parts, based on the two parts of the definition of NP- completeness.</p><h3>Lemma 34.5</h3><p>The circuit-satisfiability problem belongs to the class NP.</p><p><i><b>Proof  </b></i>We shall provide a two-input, polynomial-time algorithm A that can verify CIRCUIT-SAT. One of the inputs to A is (a standard encoding of) a boolean com- binational circuit S . The other input is a certificate corresponding to an assignment of boolean values to the wires in S . (See Exercise 34.3-4 for a smaller certificate.) We construct the algorithm A as follows. For each logic gate in the circuit, it checks that the value provided by the certificate on the output wire is correctly computed as a function of the values on the input wires. Then, if the output of the entire circuit is l, the algorithm outputs l, since the values assigned to the inputs</p><p>of S provide a satisfying assignment. Otherwise, A outputs 0.</p><p>Whenever a satisfiable circuit S is input to algorithm A, there exists a certificate whose length is polynomial in the size of S and that causes A to output a l. When- ever an unsatisfiable circuit is input, no certificate can fool A into believing that the circuit is satisfiable. Algorithm A runs in polynomial time: with a good imple- mentation, linear time suffices. Thus, we can verify CIRCUIT-SAT in polynomial time, and CIRCUIT-SAT e NP.</p><p>The second part of proving that CIRCUIT-SAT is NP-complete is to show that the language is NP-hard. That is, we must show that every language in NP is polynomial-time reducible to CIRCUIT-SAT. The actual proof of this fact is full of technical intricacies, and so we shall settle for a sketch of the proof based on some understanding of the workings of computer hardware.</p><p>A computer program is stored in the computer memory as a sequence of in- structions. A typical instruction encodes an operation to be performed, addresses of operands in memory, and an address where the result is to be stored. A spe- cial memory location, called the <i><b>program counter</b></i>, keeps track of which instruc-</p><p>9On the other hand, if the size of the circuit S is ①(2k ), then an algorithm whose running time is O(2k ) has a running time that is polynomial in the circuit size. Even if P ≠ NP, this situa- tion would not contradict the NP-completeness of the problem; the existence of a polynomial-time algorithm for a special case does not imply that there is a polynomial-time algorithm for all cases.</p><p>tion is to be executed next. The program counter automatically increments upon fetching each instruction, thereby causing the computer to execute instructions se- quentially. The execution of an instruction can cause a value to be written to the program counter, however, which alters the normal sequential execution and allows the computer to loop and perform conditional branches.</p><p>At any point during the execution of a program, the computer’s memory holds the entire state of the computation. (We take the memory to include the program itself, the program counter, working storage, and any of the various bits of state that a computer maintains for bookkeeping.) We call any particular state of com- puter memory a <i><b>configuration</b></i>. We can view the execution of an instruction as mapping one configuration to another. The computer hardware that accomplishes this mapping can be implemented as a boolean combinational circuit, which we denote by M in the proof of the following lemma.</p><h3>Lemma 34.6</h3><p>The circuit-satisfiability problem is NP-hard.</p><p>=        e         e</p><p><i><b>Proof </b></i>Let L be any language in NP. We shall describe a polynomial-time algo- rithm F computing a reduction function f that maps every binary string x to a circuit S  f(x) such that x  L if and only if S  CIRCUIT-SAT.</p><p>e</p><p>Since L  NP, there must exist an algorithm A that verifies L in polynomial time. The algorithm F that we shall construct uses the two-input algorithm A to compute the reduction function f .</p><p>$             =</p><p>Let T (n) denote the worst-case running time of algorithm A on length-n input strings, and let k  l be a constant such that T (n)  O(nk) and the length of the certificate is O(nk). (The running time of A is actually a polynomial in the total input size, which includes both an input string and a certificate, but since the length of the certificate is polynomial in the length n of the input string, the running time is polynomial in n.)</p><p>The basic idea of the proof is to represent the computation of A as a sequence of configurations. As Figure 34.9 illustrates, we can break each configuration into parts consisting of the program for A, the program counter and auxiliary machine state, the input x, the certificate y, and working storage. The combinational cir- cuit M , which implements the computer hardware, maps each configuration ci to the next configuration ciC1, starting from the initial configuration c0. Algorithm A writes its output—0 or l—to some designated location by the time it finishes ex- ecuting, and if we assume that thereafter A halts, the value never changes. Thus, if the algorithm runs for at most T (n) steps, the output appears as one of the bits in cT (n).</p><p>The reduction algorithm F constructs a single combinational circuit that com-</p><p>putes all configurations produced by a given initial configuration. The idea is to</p><p>A</p><p>PC  aux machine state</p><p>x</p><p>y  working storage</p><div><p>M</p></div><div><p>M</p></div><div><p>M</p></div><div><p>M</p></div><table cellspacing="0"><tr><td bgcolor="#C6C8CA"><p>A</p></td><td bgcolor="#DCDDDE"><p>PC</p></td><td bgcolor="#C6C8CA"><p>aux machine state</p></td><td bgcolor="#DCDDDE"><p>x</p></td><td bgcolor="#C6C8CA"><p>y</p></td><td bgcolor="#DCDDDE"><p>working storage</p></td></tr></table><p>c0</p><table cellspacing="0"><tr><td bgcolor="#C6C8CA"><p>A</p></td><td bgcolor="#DCDDDE"><p>PC</p></td><td bgcolor="#C6C8CA"><p>aux machine state</p></td><td bgcolor="#DCDDDE"><p>x</p></td><td bgcolor="#C6C8CA"><p>y</p></td><td bgcolor="#DCDDDE"><p>working storage</p></td></tr></table><p>c1</p><p>…</p><table cellspacing="0"><tr><td bgcolor="#C6C8CA"><p>A</p></td><td bgcolor="#DCDDDE"><p>PC</p></td><td bgcolor="#C6C8CA"><p>aux machine state</p></td><td bgcolor="#DCDDDE"><p>x</p></td><td bgcolor="#C6C8CA"><p>y</p></td><td bgcolor="#DCDDDE"><p>working storage</p></td></tr></table><p>c2</p><p>c<i>T</i>(<i>n</i>)</p><p>0/1 output</p><p><b>Figure 34.9  </b>The sequence of configurations produced by an algorithm A running on an input x and certificate y. Each configuration represents the state of the computer for one step of the computation and, besides A, x, and y, includes the program counter (PC), auxiliary machine state, and working storage. Except for the certificate y, the initial configuration c0 is constant. A boolean combinational circuit M maps each configuration to the next configuration. The output is a distinguished bit in the working storage.</p><p>C</p><p>paste together T (n) copies of the circuit M . The output of the i th circuit, which produces configuration ci , feeds directly into the input of the (i  l)st circuit. Thus, the configurations, rather than being stored in the computer’s memory, simply re- side as values on the wires connecting copies of M .</p><p>= j j</p><p>=</p><p>=</p><p>Recall what the polynomial-time reduction algorithm F must do. Given an in- put x, it must compute a circuit S  f(x) that is satisfiable if and only if there exists a certificate y such that A(x, y)  l. When F obtains an input x, it first computes n   x and constructs a combinational circuit S 0 consisting of T (n) copies of M . The input to S 0 is an initial configuration corresponding to a compu- tation on A(x, y), and the output is the configuration cT (n).</p><p>=</p><p>=</p><p>Algorithm F modifies circuit S 0 slightly to construct the circuit S   f(x). First, it wires the inputs to S 0 corresponding to the program for A, the initial pro- gram counter, the input x, and the initial state of memory directly to these known values. Thus, the only remaining inputs to the circuit correspond to the certifi- cate y. Second, it ignores all outputs from S 0, except for the one bit of cT (n) corresponding to the output of A.  This circuit S , so constructed, computes S(y)  A(x, y) for any input y of length O(nk). The reduction algorithm F , when provided an input string x, computes such a circuit S and outputs it.</p><p>=</p><p>We need to prove two properties. First, we must show that F correctly computes a reduction function f . That is, we must show that S is satisfiable if and only if there exists a certificate y such that A(x, y)  l. Second, we must show that F runs in polynomial time.</p><p>=</p><p>=</p><p>=   =</p><p>=</p><p>To show that F correctly computes a reduction function, let us suppose that there exists a certificate y of length O(nk) such that A(x, y)  l. Then, if we apply the bits of y to the inputs of S , the output of S is S(y)  A(x, y)  l. Thus, if a certificate exists, then S is satisfiable. For the other direction, suppose that S is satisfiable. Hence, there exists an input y to S such that S(y)  l, from which we conclude that A(x, y)  l. Thus, F correctly computes a reduction function.</p><p>= j j</p><p>To complete the proof sketch, we need only show that F runs in time polynomial in n   x . The first observation we make is that the number of bits required to represent a configuration is polynomial in n. The program for A itself has constant size, independent of the length of its input x. The length of the input x is n, and the length of the certificate y is O(nk). Since the algorithm runs for at most O(nk) steps, the amount of working storage required by A is polynomial in n as well. (We assume that this memory is contiguous; Exercise 34.3-5 asks you to extend the argument to the situation in which the locations accessed by A are scattered across a much larger region of memory and the particular pattern of scattering can differ for each input x.)</p><p>The combinational circuit M implementing the computer hardware has size polynomial in the length of a configuration, which is O(nk); hence, the size of M is polynomial in n. (Most of this circuitry implements the logic of the memory</p><p>=</p><p>system.) The circuit S consists of at most t   O(nk) copies of M , and hence it has size polynomial in n. The reduction algorithm F can construct S from x in polynomial time, since each step of the construction takes polynomial time.</p><p>The language CIRCUIT-SAT is therefore at least as hard as any language in NP, and since it belongs to NP, it is NP-complete.</p><h3>Theorem 34.7</h3><p>The circuit-satisfiability problem is NP-complete.</p><h3>Proof Immediate from Lemmas 34.5 and 34.6 and from the definition of NP- completeness.</h3><p>Exercises</p><h3>34.3-1</h3><p>Verify that the circuit in Figure 34.8(b) is unsatisfiable.</p><h3>34.3-2</h3><p>Show that the ≤P relation is a transitive relation on languages. That is, show that if</p><p>L1 ≤P L2 and L2 ≤P L3, then L1 ≤P L3.</p><h3>34.3-3</h3><p>Prove that L ≤P L if and only if L ≤P L.</p><h3>34.3-4</h3><p>Show that we could have used a satisfying assignment as a certificate in an alter- native proof of Lemma 34.5. Which certificate makes for an easier proof?</p><h3>34.3-5</h3><p>The proof of Lemma 34.6 assumes that the working storage for algorithm A occu- pies a contiguous region of polynomial size. Where in the proof do we exploit this assumption? Argue that this assumption does not involve any loss of generality.</p><h3>34.3-6</h3><p>e     ≤       e       ;   {  }</p><p>A language L is <i><b>complete </b></i>for a language class S with respect to polynomial-time reductions if L  S and L0 P L for all L0 S . Show that  and 0, l * are the only languages in P that are not complete for P with respect to polynomial-time reductions.</p><h3>34.3-7</h3><p>Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), L is complete for NP if and only if L is complete for co-NP.</p><h3>34.3-8</h3><p>=</p><p>The reduction algorithm F in the proof of Lemma 34.6 constructs the circuit S  f(x) based on knowledge of x, A, and k. Professor Sartre observes that the string x is input to F , but only the existence of A, k, and the constant factor implicit in the O(nk) running time is known to F (since the language L belongs to NP), not their actual values. Thus, the professor concludes that F can’t possi- bly construct the circuit S and that the language CIRCUIT-SAT is not necessarily NP-hard. Explain the flaw in the professor’s reasoning.</p><ol><ol><li data-list-text="34.4"><h2>NP-completeness proofs</h2><p>≤                  e</p><p>We proved that the circuit-satisfiability problem is NP-complete by a direct proof that L  P CIRCUIT-SAT for every language L  NP. In this section, we shall show how to prove that languages are NP-complete without directly reducing <i>every </i>language in NP to the given language. We shall illustrate this methodology by proving that various formula-satisfiability problems are NP-complete. Section 34.5 provides many more examples of the methodology.</p><p>The following lemma is the basis of our method for showing that a language is NP-complete.</p><h3>Lemma 34.8</h3><p>If L is a language such that L0 ≤P L for some L0 e NPC, then L is NP-hard. If, in addition, L e NP, then L e NPC.</p><p><i><b>Proof  </b></i>Since L0 is NP-complete, for all L00 e NP, we have L00 ≤P L0. By sup- position, L0 ≤P L, and thus by transitivity (Exercise 34.3-2), we have L00 ≤P L, which shows that L is NP-hard. If L e NP, we also have L e NPC.</p><p>In other words, by reducing a known NP-complete language L0 to L, we implic- itly reduce every language in NP to L. Thus, Lemma 34.8 gives us a method for proving that a language L is NP-complete:</p><ol><li data-list-text="1."><p>Prove L e NP.</p></li><li data-list-text="2."><p>Select a known NP-complete language L0.</p></li><li data-list-text="3."><p>Describe an algorithm that computes a function f mapping every instance</p><p>x e {0, l}* of L0 to an instance f(x) of L.</p></li><li data-list-text="4."><p>Prove that the function f satisfies x e L0 if and only if f(x) e L for all</p><p>x e {0, l}*.</p></li><li data-list-text="5."><p>Prove that the algorithm computing f runs in polynomial time.</p></li></ol><p>e</p><p>(Steps 2–5 show that L is NP-hard.) This methodology of reducing from a sin- gle known NP-complete language is far simpler than the more complicated pro- cess of showing directly how to reduce from every language in NP. Proving CIRCUIT-SAT  NPC has given us a “foot in the door.” Because we know that the circuit-satisfiability problem is NP-complete, we now can prove much more easily that other problems are NP-complete. Moreover, as we develop a catalog of known NP-complete problems, we will have more and more choices for languages from which to reduce.</p><p>Formula satisfiability</p><p>We illustrate the reduction methodology by giving an NP-completeness proof for the problem of determining whether a boolean formula, not a circuit, is satisfiable. This problem has the historical honor of being the first problem ever shown to be NP-complete.</p><p>We formulate the <i><b>(formula) satisfiability </b></i>problem in terms of the language SAT as follows. An instance of SAT is a boolean formula ø composed of</p><ol><li data-list-text="1."><p>n boolean variables: x1, x2,. . . , xn;</p><p>ˆ    v    ¬    →       ↔</p></li><li data-list-text="2."><p>m boolean connectives: any boolean function with one or two inputs and one output, such as  (AND),  (OR),  (NOT),  (implication),  (if and only if); and</p></li><li data-list-text="3."><p>parentheses. (Without loss of generality, we assume that there are no redundant parentheses, i.e., a formula contains at most one pair of parentheses per boolean connective.)</p></li></ol><p>C</p><p>We can easily encode a boolean formula ø in a length that is polynomial in n  m. As in boolean combinational circuits, a <i><b>truth assignment </b></i>for a boolean formula ø is a set of values for the variables of ø, and a <i><b>satisfying assignment </b></i>is a truth assignment that causes it to evaluate to l. A formula with a satisfying assignment is a <i><b>satisfiable </b></i>formula. The satisfiability problem asks whether a given boolean formula is satisfiable; in formal-language terms,</p><p>SAT = {(ø)W ø is a satisfiable boolean formula} .</p><p>As an example, the formula</p><p>ø = ((x1 → x2) v ¬((¬x1 ↔ x3) v x4)) ˆ ¬x2</p><p>has the satisfying assignment (x1 = 0, x2 = 0, x3 = l, x4 = l), since</p><p>ø  = ((0 → 0) v ¬((¬0 ↔ l) v l)) ˆ ¬0                 (34.2)</p><p>= (l v ¬(l v l)) ˆ l</p><p>= (l v 0) ˆ l</p><p>= l , </p><p>and thus this formula ø belongs to SAT.</p><p>( )</p><p>( )</p><p>The naive algorithm to determine whether an arbitrary boolean formula is satis- fiable does not run in polynomial time. A formula with n variables has 2n possible assignments. If the length of ø is polynomial in n, then checking every assign- ment requires Ω(2n) time, which is superpolynomial in the length of ø . As the following theorem shows, a polynomial-time algorithm is unlikely to exist.</p><h3>Theorem 34.9</h3><p>Satisfiability of boolean formulas is NP-complete.</p><p>≤</p><p>e</p><p><i><b>Proof  </b></i>We start by arguing that SAT  NP. Then we prove that SAT is NP-hard by showing that CIRCUIT-SAT  P SAT; by Lemma 34.8, this will prove the theorem. To show that SAT belongs to NP, we show that a certificate consisting of a satisfying assignment for an input formula ø can be verified in polynomial time. The verifying algorithm simply replaces each variable in the formula with its cor- responding value and then evaluates the expression, much as we did in equa- tion (34.2) above. This task is easy to do in polynomial time. If the expression evaluates to l, then the algorithm has verified that the formula is satisfiable. Thus,</p><p>the first condition of Lemma 34.8 for NP-completeness holds.</p><p>≤</p><p>To prove that SAT is NP-hard, we show that CIRCUIT-SAT  P SAT. In other words, we need to show how to reduce any instance of circuit satisfiability to an instance of formula satisfiability in polynomial time. We can use induction to express any boolean combinational circuit as a boolean formula. We simply look at the gate that produces the circuit output and inductively express each of the gate’s inputs as formulas. We then obtain the formula for the circuit by writing an expression that applies the gate’s function to its inputs’ formulas.</p><p>Unfortunately, this straightforward method does not amount to a polynomial- time reduction. As Exercise 34.4-1 asks you to show, shared subformulas—which arise from gates whose output wires have fan-out of 2 or more—can cause the size of the generated formula to grow exponentially. Thus, the reduction algorithm must be somewhat more clever.</p><p>Figure 34.10 illustrates how we overcome this problem, using as an example the circuit from Figure 34.8(a). For each wire xi in the circuit S , the formula ø</p><p>x5</p><p>x8</p><p>x6</p><p>x9</p><p>x4</p><p>x7</p><p>x1 x2</p><p> x10   </p><p>x3</p><h4>Figure 34.10 Reducing circuit satisfiability to formula satisfiability. The formula produced by the reduction algorithm has a variable for each wire in the circuit.</h4><p>↔  ˆ ˆ</p><p>has a variable xi . We can now express how each gate operates as a small formula involving the variables of its incident wires. For example, the operation of the output AND gate is x10 (x7 x8 x9). We call each of these small formulas a clause.</p><p>The formula ø produced by the reduction algorithm is the AND of the circuit- output variable with the conjunction of clauses describing the operation of each gate. For the circuit in the figure, the formula is</p><p>ø = x10 ˆ (x4 ↔ ¬x3)</p><p>ˆ (x5 ↔ (x1 v x2)) ˆ (x6 ↔ ¬x4)</p><p>ˆ (x7 ↔ (x1 ˆ x2 ˆ x4)) ˆ (x8 ↔ (x5 v x6))</p><p>ˆ (x9 ↔ (x6 v x7))</p><p>ˆ (x10 ↔ (x7 ˆ x8 ˆ x9)) .</p><p>Given a circuit S , it is straightforward to produce such a formula ø in polynomial time.</p><p>Why is the circuit S satisfiable exactly when the formula ø is satisfiable? If S has a satisfying assignment, then each wire of the circuit has a well-defined value, and the output of the circuit is l. Therefore, when we assign wire values to variables in ø, each clause of ø evaluates to l, and thus the conjunction of all evaluates to l. Conversely, if some assignment causes ø to evaluate to l, the circuit S is satisfiable by an analogous argument. Thus, we have shown that</p><p>CIRCUIT-SAT ≤P SAT, which completes the proof.</p><p>3-CNF satisfiability</p><p>We can prove many problems NP-complete by reducing from formula satisfiability. The reduction algorithm must handle any input formula, though, and this require- ment can lead to a huge number of cases that we must consider. We often prefer to reduce from a restricted language of boolean formulas, so that we need to con- sider fewer cases. Of course, we must not restrict the language so much that it becomes polynomial-time solvable. One convenient language is 3-CNF satisfiabil- ity, or 3-CNF-SAT.</p><p>We define 3-CNF satisfiability using the following terms. A <i><b>literal </b></i>in a boolean formula is an occurrence of a variable or its negation. A boolean formula is in <i><b>conjunctive normal form</b></i>, or <i><b>CNF</b></i>, if it is expressed as an AND of <i><b>clauses</b></i>, each of which is the OR of one or more literals. A boolean formula is in <i><b>3-conjunctive normal form</b></i>, or <i><b>3-CNF</b></i>, if each clause has exactly three distinct literals.</p><p>For example, the boolean formula</p><p>(x1 v ¬x1 v ¬x2) ˆ (x3 v x2 v x4) ˆ (¬x1 v ¬x3 v ¬x4)</p><p>¬  ¬</p><p>v ¬ v ¬</p><p>is in 3-CNF. The first of its three clauses is (x1 x1 x2), which contains the three literals x1,  x1, and  x2.</p><p>In 3-CNF-SAT, we are asked whether a given boolean formula ø in 3-CNF is satisfiable. The following theorem shows that a polynomial-time algorithm that can determine the satisfiability of boolean formulas is unlikely to exist, even when they are expressed in this simple normal form.</p><h3>Theorem 34.10</h3><p>Satisfiability of boolean formulas in 3-conjunctive normal form is NP-complete.</p><p>≤</p><p>e</p><p>e</p><p><i><b>Proof  </b></i>The argument we used in the proof of Theorem 34.9 to show that SAT NP applies equally well here to show that 3-CNF-SAT  NP. By Lemma 34.8, therefore, we need only show that SAT  P 3-CNF-SAT.</p><p>We break the reduction algorithm into three basic steps. Each step progressively transforms the input formula ø closer to the desired 3-conjunctive normal form.</p><p>≤</p><p>The first step is similar to the one used to prove CIRCUIT-SAT  P SAT in Theorem 34.9. First, we construct a binary “parse” tree for the input formula ø, with literals as leaves and connectives as internal nodes. Figure 34.11 shows such a parse tree for the formula</p><p>ø = ((x1 → x2) v ¬((¬x1 ↔ x3) v x4)) ˆ ¬x2 .            (34.3) Should the input formula contain a clause such as the OR of several literals, we use associativity to parenthesize the expression fully so that every internal node in the</p><p>resulting tree has l or 2 children. We can now think of the binary parse tree as a</p><p>circuit for computing the function.</p><p>y1</p><p>y2</p><p>ˆ</p><p>y3</p><p>v</p><p>y4</p><p>¬x2</p><p>→</p><p>¬</p><p>y5</p><p>x2</p><p>y6</p><p>↔</p><p>v</p><p>x4</p><p>x1</p><p>¬x1 x3</p><p>Figure 34.11  The tree corresponding to the formula ø = ((x1 → x2)v¬((¬x1 ↔ x3)vx4))ˆ¬x2.</p><p>Mimicking the reduction in the proof of Theorem 34.9, we introduce a vari- able yi for the output of each internal node. Then, we rewrite the original for- mula ø as the AND of the root variable and a conjunction of clauses describing the operation of each node. For the formula (34.3), the resulting expression is</p><p>ø0 = y1 ˆ (y1 ↔ (y2 ˆ ¬x2))</p><p>ˆ (y2 ↔ (y3 v y4))</p><p>ˆ (y3 ↔ (x1 → x2))</p><p>ˆ (y4 ↔ ¬y5)</p><p>ˆ (y5 ↔ (y6 v x4))</p><p>ˆ (y6 ↔ (¬x1 ↔ x3)) .</p><p>Observe that the formula ø0 thus obtained is a conjunction of clauses øi0 , each of which has at most 3 literals. The only requirement that we might fail to meet is that each clause has to be an OR of 3 literals.</p><p>¬</p><p>The second step of the reduction converts each clause øi0 into conjunctive normal form. We construct a truth table for øi0 by evaluating all possible assignments to its variables. Each row of the truth table consists of a possible assignment of the variables of the clause, together with the value of the clause under that assignment. Using the truth-table entries that evaluate to 0, we build a formula in <i><b>disjunctive normal form </b></i>(or <i><b>DNF</b></i>)—an OR of ANDs—that is equivalent to  øi0 . We then negate this formula and convert it into a CNF formula øi00 by using <i><b>DeMorgan’s</b></i></p><p>y1 y2</p><p>1  1</p><p>1  1</p><p>1  0</p><p>1  0</p><p>0  1</p><p>0  1</p><p>0  0</p><p>0  0</p><p>x2</p><p>1</p><p>0</p><p>1</p><p>0</p><p>1</p><p>0</p><p>1</p><p>0</p><p>(y1 ↔ (y2 ˆ ¬x2))</p><p>0</p><p>1</p><p>0</p><p>0</p><p>1</p><p>0</p><p>1</p><p>1</p><p>Figure 34.12 The truth table for the clause (y1 ↔ (y2 ˆ ¬x2)).</p><h3>laws for propositional logic,</h3><p>¬(a ˆ b) = ¬a v ¬b , </p><p>¬(a v b) = ¬a ˆ ¬b , </p><p>to complement all literals, change ORs into ANDs, and change ANDs into ORs.</p><p>In our example, we convert the clause ø10 = (y1 ↔ (y2 ˆ ¬x2)) into CNF</p><p>as follows. The truth table for ø10 appears in Figure 34.12. The DNF formula equivalent to ¬ø10 is</p><p>(y1 ˆ y2 ˆ x2) v (y1 ˆ ¬y2 ˆ x2) v (y1 ˆ ¬y2 ˆ ¬x2) v (¬y1 ˆ y2 ˆ ¬x2) . </p><p>Negating and applying DeMorgan’s laws, we get the CNF formula</p><p>ø100</p><p>= (¬y1 v ¬y2 v ¬x2) ˆ (¬y1 v y2 v ¬x2) ˆ (¬y1 v y2 v x2) ˆ (y1 v ¬y2 v x2) , </p><p>which is equivalent to the original clause ø10 .</p><p>At this point, we have converted each clause øi0 of the formula ø0 into a CNF formula øi00, and thus ø0 is equivalent to the CNF formula ø00 consisting of the conjunction of the øi00. Moreover, each clause of ø00 has at most 3 literals.</p><p>The third and final step of the reduction further transforms the formula so that</p><p>each clause has <i>exactly </i>3 distinct literals. We construct the final 3-CNF formula ø000 from the clauses of the CNF formula ø00. The formula ø000 also uses two auxiliary variables that we shall call p and q. For each clause Si of ø00, we include the following clauses in ø000:</p><ul><li data-list-text="●"><p>If Si has 3 distinct literals, then simply include Si as a clause of ø000.</p></li><li data-list-text="●"><p>If Si has 2 distinct literals, that is, if Si = (l1 v l2), where l1 and l2 are literals, then include (l1 v l2 v p) ˆ (l1 v l2 v ¬p) as clauses of ø000. The literals p and ¬p merely fulfill the syntactic requirement that each clause of ø000 has</p><p>exactly 3 distinct literals. Whether p = 0 or p = l, one of the clauses is equivalent to l1 v l2, and the other evaluates to l, which is the identity for AND.</p><p>v ¬ v ˆ v ¬ v ¬</p><p>v  v  ˆ  v  v ¬  ˆ</p><ul><li data-list-text="●"><p>If Si has just l distinct literal l, then include (l  p  q)  (l  p   q) (l   p  q)  (l   p   q) as clauses of ø000. Regardless of the values of p and q, one of the four clauses is equivalent to l, and the other 3 evaluate to l.</p></li></ul></li></ul><p>We can see that the 3-CNF formula ø000 is satisfiable if and only if ø is satisfiable by inspecting each of the three steps. Like the reduction from CIRCUIT-SAT to SAT, the construction of ø0 from ø in the first step preserves satisfiability. The second step produces a CNF formula ø00 that is algebraically equivalent to ø0. The third step produces a 3-CNF formula ø000 that is effectively equivalent to ø00, since any assignment to the variables p and q produces a formula that is algebraically equivalent to ø00.</p><p>=</p><p>We must also show that the reduction can be computed in polynomial time. Con- structing ø0 from ø introduces at most l variable and l clause per connective in ø. Constructing ø00 from ø0 can introduce at most 8 clauses into ø00 for each clause from ø0, since each clause of ø0 has at most 3 variables, and the truth table for each clause has at most 23 8 rows. The construction of ø000 from ø00 introduces at most 4 clauses into ø000 for each clause of ø00. Thus, the size of the resulting formula ø000 is polynomial in the length of the original formula. Each of the con- structions can easily be accomplished in polynomial time.</p><p>Exercises</p><h3>34.4-1</h3><p>Consider the straightforward (nonpolynomial-time) reduction in the proof of The- orem 34.9. Describe a circuit of size n that, when converted to a formula by this method, yields a formula whose size is exponential in n.</p><h3>34.4-2</h3><p>Show the 3-CNF formula that results when we use the method of Theorem 34.10 on the formula (34.3).</p><h3>34.4-3</h3><p>≤</p><p>¬</p><p>Professor Jagger proposes to show that SAT  P 3-CNF-SAT by using only the truth-table technique in the proof of Theorem 34.10, and not the other steps. That is, the professor proposes to take the boolean formula ø, form a truth table for its variables, derive from the truth table a formula in 3-DNF that is equivalent to  ø, and then negate and apply DeMorgan’s laws to produce a 3-CNF formula equivalent to ø. Show that this strategy does not yield a polynomial-time reduction.</p><h3>34.4-4</h3><p>Show that the problem of determining whether a boolean formula is a tautology is complete for co-NP. (<i>Hint: </i>See Exercise 34.3-7.)</p><h3>34.4-5</h3><p>Show that the problem of determining the satisfiability of boolean formulas in dis- junctive normal form is polynomial-time solvable.</p><h3>34.4-6</h3><p>Suppose that someone gives you a polynomial-time algorithm to decide formula satisfiability. Describe how to use this algorithm to find satisfying assignments in polynomial time.</p><h3>34.4-7</h3><p>v        ¬ →</p><p>e</p><p>Let 2-CNF-SAT be the set of satisfiable boolean formulas in CNF with exactly 2 literals per clause. Show that 2-CNF-SAT  P. Make your algorithm as efficient as possible. (<i>Hint: </i>Observe that x  y is equivalent to  x  y. Reduce 2-CNF-SAT to an efficiently solvable problem on a directed graph.)</p></li><li data-list-text="34.5"><h2>NP-complete problems</h2><p>NP-complete problems arise in diverse domains: boolean logic, graphs, arithmetic, network design, sets and partitions, storage and retrieval, sequencing and schedul- ing, mathematical programming, algebra and number theory, games and puzzles, automata and language theory, program optimization, biology, chemistry, physics, and more. In this section, we shall use the reduction methodology to provide NP- completeness proofs for a variety of problems drawn from graph theory and set partitioning.</p><p>Figure 34.13 outlines the structure of the NP-completeness proofs in this section and Section 34.4. We prove each language in the figure to be NP-complete by reduction from the language that points to it. At the root is CIRCUIT-SAT, which we proved NP-complete in Theorem 34.7.</p><ol><li data-list-text="34.5.1"><p>The clique problem</p><p>=         ⊆</p><p>A <i><b>clique </b></i>in an undirected graph G   (V, E) is a subset V 0 V of vertices, each pair of which is connected by an edge in E. In other words, a clique is a complete subgraph of G. The <i><b>size </b></i>of a clique is the number of vertices it contains. The <i><b>clique problem </b></i>is the optimization problem of finding a clique of maximum size in</p><h3>34.4-4</h3><p>Show that the problem of determining whether a boolean formula is a tautology is complete for co-NP. (<i>Hint: </i>See Exercise 34.3-7.)</p><h3>34.4-5</h3><p>Show that the problem of determining the satisfiability of boolean formulas in dis- junctive normal form is polynomial-time solvable.</p><h3>34.4-6</h3><p>Suppose that someone gives you a polynomial-time algorithm to decide formula satisfiability. Describe how to use this algorithm to find satisfying assignments in polynomial time.</p><h3>34.4-7</h3><p>v        ¬ →</p><p>e</p><p>Let 2-CNF-SAT be the set of satisfiable boolean formulas in CNF with exactly 2 literals per clause. Show that 2-CNF-SAT  P. Make your algorithm as efficient as possible. (<i>Hint: </i>Observe that x  y is equivalent to  x  y. Reduce 2-CNF-SAT to an efficiently solvable problem on a directed graph.)</p><ol><ol><li data-list-text="34.5"><h2>NP-complete problems</h2><p>NP-complete problems arise in diverse domains: boolean logic, graphs, arithmetic, network design, sets and partitions, storage and retrieval, sequencing and schedul- ing, mathematical programming, algebra and number theory, games and puzzles, automata and language theory, program optimization, biology, chemistry, physics, and more. In this section, we shall use the reduction methodology to provide NP- completeness proofs for a variety of problems drawn from graph theory and set partitioning.</p><p>Figure 34.13 outlines the structure of the NP-completeness proofs in this section and Section 34.4. We prove each language in the figure to be NP-complete by reduction from the language that points to it. At the root is CIRCUIT-SAT, which we proved NP-complete in Theorem 34.7.</p><ol><li data-list-text="34.5.1"><p>The clique problem</p><p>=         ⊆</p><p>A <i><b>clique </b></i>in an undirected graph G   (V, E) is a subset V 0 V of vertices, each pair of which is connected by an edge in E. In other words, a clique is a complete subgraph of G. The <i><b>size </b></i>of a clique is the number of vertices it contains. The <i><b>clique problem </b></i>is the optimization problem of finding a clique of maximum size in</p><p>CIRCUIT-SAT</p><p>SAT</p><p>3-CNF-SAT</p><p>CLIQUE</p><p>SUBSET-SUM</p><p>VERTEX-COVER</p><p>HAM-CYCLE TSP</p><h4>Figure 34.13 The structure of NP-completeness proofs in Sections 34.4 and 34.5. All proofs ulti- mately follow by reduction from the NP-completeness of CIRCUIT-SAT.</h4><p>a graph. As a decision problem, we ask simply whether a clique of a given size k</p><p>exists in the graph. The formal definition is</p><p>CLIQUE = {(G, k): G is a graph containing a clique of size k} .</p><p>see whether it forms a clique. The running time of this algorithm is Ω(k !j j"),</p><p>=     | |</p><p>A naive algorithm for determining whether a graph G   (V, E) with V  ver- tices has a clique of size k is to list all k-subsets of V , and check each one to</p><p>2 V</p><p>k</p><p>| |</p><p>which is polynomial if k is a constant. In general, however, k could be near V /2,</p><p>in which case the algorithm runs in superpolynomial time. Indeed, an efficient algorithm for the clique problem is unlikely to exist.</p><h3>Theorem 34.11</h3><p>The clique problem is NP-complete.</p><p>e</p><p>⊆</p><p>e           =</p><p>Proof  To show that CLIQUE  NP, for a given graph G   (V, E), we use the set V 0 V of vertices in the clique as a certificate for G. We can check whether V 0 is a clique in polynomial time by checking whether, for each pair u, v  V 0, the edge (u, v) belongs to E.</p><p>≤</p><p>We next prove that 3-CNF-SAT  P CLIQUE, which shows that the clique prob- lem is NP-hard. You might be surprised that we should be able to prove such a result, since on the surface logical formulas seem to have little to do with graphs.</p><p>The reduction algorithm begins with an instance of 3-CNF-SAT. Let ø =</p><p>S1 ˆ S2 ˆ · · · ˆ Sk be a boolean formula in 3-CNF with k clauses. For r =</p><p>x1</p><p>¬x2</p><p>¬x3</p><p>¬x1</p><p>x1</p><p>x2</p><p>x2</p><p>x3</p><p>x3</p><p>S1 = x1 v ¬x2 v ¬x3</p><p>S2 = ¬x1 v x2 v x3 S3 = x1 v x2 v x3</p><p>Figure 34.14 The graph G derived from the 3-CNF formula ø = S1 ˆ S2 ˆ S3, where S1 = (x1 v ¬x2 v ¬x3), S2 = (¬x1 v x2 v x3), and S3 = (x1 v x2 v x3), in reducing 3-CNF-SAT to CLIQUE. A satisfying assignment of the formula has x2 = 0, x3 = l, and x1 either 0 or l. This assignment satisfies S1 with ¬x2, and it satisfies S2 and S3 with x3, corresponding to the clique with lightly shaded vertices.</p><p>l, 2, . . . , k, each clause Sr has exactly three distinct literals lr, lr , and lr . We shall</p><p>1  2    3</p><p>construct a graph G such that ø is satisfiable if and only if G has a clique of size k.</p><p>We construct the graph G = (V, E) as follows.  For each clause Sr =</p><p>1</p><p>2</p><p>3</p><p>1</p><p>2</p><p>3</p><p>(lr v lr v lr ) in ø, we place a triple of vertices vr , vr , and vr into V . We put</p><p>an edge between two vertices vr and vs if both of the following hold:</p><p>i   j</p><p>i</p><p>j</p><ul><li data-list-text="●"><p>vr and vs are in different triples, that is, r ≠ s, and</p></li><li data-list-text="●"><p>their corresponding literals are <i><b>consistent</b></i>, that is, lr is not the negation of ls.</p></li></ul><p>i             j</p><p>We can easily build this graph from ø in polynomial time. As an example of this construction, if we have</p><p>ø = (x1 v ¬x2 v ¬x3) ˆ (¬x1 v x2 v x3) ˆ (x1 v x2 v x3) , </p><p>then G is the graph shown in Figure 34.14.</p><p>We must show that this transformation of ø into G is a reduction. First, suppose</p><p>that ø has a satisfying assignment. Then each clause Sr contains at least one literal lr that is assigned l, and each such literal corresponds to a vertex vr . Picking</p><p>i i</p><p>one such “true” literal from each clause yields a set V 0 of k vertices. We claim that</p><p>i</p><p>j</p><p>V 0 is a clique. For any two vertices vr, vs e V 0, where r ≠ s, both corresponding</p><p>literals lr and ls map to l by the given satisfying assignment, and thus the literals</p><p>i   j</p><p>cannot be complements. Thus, by the construction of G, the edge (vr, vs) belongs</p><p>i  j</p><p>to E.</p><p>Conversely, suppose that G has a clique V 0 of size k. No edges in G connect vertices in the same triple, and so V 0 contains exactly one vertex per triple. We can</p><p>i</p><p>i</p><p>assign l to each literal lr such that vr e V 0 without fear of assigning l to both a</p><p>literal and its complement, since G contains no edges between inconsistent literals.</p><p>Each clause is satisfied, and so ø is satisfied. (Any variables that do not correspond to a vertex in the clique may be set arbitrarily.)</p><p>¬</p><p>¬</p><p>=                =</p><p>=</p><p>In the example of Figure 34.14, a satisfying assignment of ø has x2 0 and x3 l. A corresponding clique of size k  3 consists of the vertices correspond- ing to  x2 from the first clause, x3 from the second clause, and x3 from the third clause. Because the clique contains no vertices corresponding to either x1 or  x1, we can set x1 to either 0 or l in this satisfying assignment.</p><p>Observe that in the proof of Theorem 34.11, we reduced an arbitrary instance of 3-CNF-SAT to an instance of CLIQUE with a particular structure. You might think that we have shown only that CLIQUE is NP-hard in graphs in which the vertices are restricted to occur in triples and in which there are no edges between vertices in the same triple. Indeed, we have shown that CLIQUE is NP-hard only in this restricted case, but this proof suffices to show that CLIQUE is NP-hard in general graphs. Why? If we had a polynomial-time algorithm that solved CLIQUE on general graphs, it would also solve CLIQUE on restricted graphs.</p><p>The opposite approach—reducing instances of 3-CNF-SAT with a special struc- ture to general instances of CLIQUE—would not have sufficed, however. Why not? Perhaps the instances of 3-CNF-SAT that we chose to reduce from were “easy,” and so we would not have reduced an NP-hard problem to CLIQUE.</p><p>Observe also that the reduction used the instance of 3-CNF-SAT, but not the solution. We would have erred if the polynomial-time reduction had relied on knowing whether the formula ø is satisfiable, since we do not know how to decide whether ø is satisfiable in polynomial time.</p></li><li data-list-text="34.5.2"><p>The vertex-cover problem</p><p>{ }</p><p>e     e    e</p><p>=         ⊆</p><p>A <i><b>vertex cover </b></i>of an undirected graph G  (V, E) is a subset V 0 V such that if (u, v)  E, then u  V 0 or v  V 0 (or both). That is, each vertex “covers” its incident edges, and a vertex cover for G is a set of vertices that covers all the edges in E. The <i><b>size </b></i>of a vertex cover is the number of vertices in it. For example, the graph in Figure 34.15(b) has a vertex cover r, ´ of size 2.</p><p>The <i><b>vertex-cover problem </b></i>is to find a vertex cover of minimum size in a given graph. Restating this optimization problem as a decision problem, we wish to</p><p>u</p><p>v</p><p>z</p><p>w</p><p>y</p><p>x</p><p>u</p><p>v</p><p>z</p><p>w</p><p>y</p><p>x</p><p>(a)                       (b)</p><p>Figure 34.15 Reducing CLIQUE to VERTEX-COVER. (a) An undirected graph G = (V, E) with clique V 0 = {u, v, x, y}. (b) The graph G produced by the reduction algorithm that has vertex cover V — V 0 = {r, ´}.</p><p>determine whether a graph has a vertex cover of a given size k. As a language, we define</p><p>VERTEX-COVER = {(G, k): graph G has a vertex cover of size k} .</p><p>The following theorem shows that this problem is NP-complete.</p><h3>Theorem 34.12</h3><p>The vertex-cover problem is NP-complete.</p><p>e     e     e</p><p>| | =</p><p>=                                 ⊆</p><p>e</p><p>Proof  We first show that VERTEX-COVER  NP. Suppose we are given a graph G  (V, E) and an integer k. The certificate we choose is the vertex cover V 0 V itself. The verification algorithm affirms that V 0 k, and then it checks, for each edge (u, v)  E, that u  V 0 or v  V 0. We can easily verify the certificate in polynomial time.</p><p>≤</p><p>=         = {   :   e   ≠       /e  }</p><p>=</p><p>We prove that the vertex-cover problem is NP-hard by showing that CLIQUE  P VERTEX-COVER. This reduction relies on the notion of the “complement” of a gra<u>ph</u>. Giv<u>en </u>an undire<u>ct</u>ed graph G   (V, E), we define the <i><b>complement </b></i>of G as G   (V, E), where E   (u, v)  u, v  V, u  v, and (u, v)  E . In other words, G is the graph containing exactly those edges that are not in G. Figure 34.15 shows a graph and its complement and illustrates the reduction from CLIQUE to VERTEX-COVER.</p><p>(  )</p><p>( | | — )</p><p>The reduction algorithm ta<u>ke</u>s as input an instance G, k of the clique problem. It computes the complement G, which we can easily do in polynomial time. The output of the reduction algorithm is the instance G, V   k of the vertex-cover problem. To complete the proof, we show that this transformation is indeed a</p><p>reduction: the graph G has a clique of size k if and only if the graph G has a vertex cover of size V   k.</p><p>/e</p><p>⊆    <u>|</u>  | =         —</p><p>| | —</p><p>Suppose that <u> G</u> has a clique V 0 V with <u>V</u> 0 k. We claim that V  V 0 is a vertex cover in G. Let (u, v) be any edge in E. Then, (u, v)  E, which implies that at least one of u or v does not belong to V 0, since every pair of vertices in V 0 is</p><p>connected by an edge of E. Equivalently, at least one of u or v is in V — V 0, which means that edge (u, v) is covered by V — V 0. Since (u, v) was chosen arbitrarily from E, every edge of E is covered by a vertex in <u>V</u> — V 0. Hence, the set V — V 0, which has size |V | — k, forms a vertex cover for G.</p><p>Conversely, suppose that G has a vertex cover V 0 ⊆ V , where |V 0| = |V | — k. Then, for all u, v e V , if (u, v) e E, then u e V 0 or v e V 0 or both. The contrapositive of this implication is that for all u, v e V , if u /e V 0 and v /e V 0, then (u, v) e E. In other words, V —V 0 is a clique, and it has size |V |—|V 0| = k.</p><p>Since VERTEX-COVER is NP-complete, we don’t expect to find a polynomial- time algorithm for finding a minimum-size vertex cover. Section 35.1 presents a polynomial-time “approximation algorithm,” however, which produces “approxi- mate” solutions for the vertex-cover problem. The size of a vertex cover produced by the algorithm is at most twice the minimum size of a vertex cover.</p><p>Thus, we shouldn’t give up hope just because a problem is NP-complete. We may be able to design a polynomial-time approximation algorithm that obtains near-optimal solutions, even though finding an optimal solution is NP-complete. Chapter 35 gives several approximation algorithms for NP-complete problems.</p></li><li data-list-text="34.5.3"><p>The hamiltonian-cycle problem</p><p>We now return to the hamiltonian-cycle problem defined in Section 34.2.</p><h3>Theorem 34.13</h3><p>The hamiltonian cycle problem is NP-complete.</p><p>| |</p><p>=</p><p><i><b>Proof  </b></i>We first show that HAM-CYCLE belongs to NP. Given a graph G (V, E), our certificate is the sequence of V vertices that makes up the hamiltonian cycle. The verification algorithm checks that this sequence contains each vertex in V exactly once and that with the first vertex repeated at the end, it forms a cycle in G. That is, it checks that there is an edge between each pair of consecutive vertices and between the first and last vertices. We can verify the certificate in polynomial time.</p><p>We now prove that VERTEX-COVER ≤P HAM-CYCLE, which shows that HAM-CYCLE is NP-complete. Given an undirected graph G = (V, E) and an</p><p>[<i>u</i>,<i>v</i>,1]</p><p>[<i>u</i>,<i>v</i>,2]</p><p>[<i>u</i>,<i>v</i>,3]</p><p>[<i>u</i>,<i>v</i>,4]</p><p>[<i>u</i>,<i>v</i>,5]</p><p>[<i>u</i>,<i>v</i>,6]</p><p>[<i>v</i>,<i>u</i>,1]</p><p>[<i>u</i>,<i>v</i>,1]</p><p>[<i>v</i>,<i>u</i>,1]</p><p>Wuv</p><p>[<i>u</i>,<i>v</i>,6]</p><p>[<i>v</i>,<i>u</i>,6]</p><p>[<i>u</i>,<i>v</i>,1]</p><p>[<i>v</i>,<i>u</i>,1]</p><p>Wuv</p><p>[<i>u</i>,<i>v</i>,6]</p><p>[<i>v</i>,<i>u</i>,6]</p><p>[<i>u</i>,<i>v</i>,1]</p><p>[<i>v</i>,<i>u</i>,1]</p><p>Wuv</p><p>[<i>u</i>,<i>v</i>,6]</p><p>[<i>v</i>,<i>u</i>,6]</p><p>Wuv</p><p>[<i>v</i>,<i>u</i>,2]</p><p>[<i>v</i>,<i>u</i>,3]</p><p>[<i>v</i>,<i>u</i>,4]</p><p>[<i>v</i>,<i>u</i>,5]</p><p>[<i>v</i>,<i>u</i>,6]</p><p>(a)</p><p>(b)</p><p>(c)</p><p>(d)</p><p>Figure 34.16  The widget used in reducing the vertex-cover problem to the hamiltonian-cycle prob- lem. An edge (u, v) of graph G corresponds to widget Wuv in the graph G0 created in the reduction.</p><h4>(a) The widget, with individual vertices labeled. (b)–(d) The shaded paths are the only possible ones</h4><p>through the widget that include all vertices, assuming that the only connections from the widget to the remainder of G0 are through vertices [u, v, l], [u, v, 6], [v, u, l], and [v, u, 6].</p><p>=</p><p>integer k, we construct an undirected graph G0 (V 0,E0) that has a hamiltonian cycle if and only if G has a vertex cover of size k.</p><p>e</p><p>≤ ≤</p><p>Our construction uses a widget, which is a piece of a graph that enforces certain properties. Figure 34.16(a) shows the widget we use. For each edge (u, v)  E, the graph G0 that we construct will contain one copy of this widget, which we denote by Wuv. We denote each vertex in Wuv by [u, v, i ] or [v, u, i ], where l  i  6, so that each widget Wuv contains l2 vertices. Widget Wuv also contains the l4 edges shown in Figure 34.16(a).</p><p>Along with the internal structure of the widget, we enforce the properties we want by limiting the connections between the widget and the remainder of the graph G0 that we construct. In particular, only vertices [u, v, l], [u, v, 6], [v, u, l], and [v, u, 6] will have edges incident from outside Wuv. Any hamiltonian cycle of G0 must traverse the edges of Wuv in one of the three ways shown in Fig- ures 34.16(b)–(d). If the cycle enters through vertex [u, v, l], it must exit through vertex [u, v, 6], and it either visits all l2 of the widget’s vertices (Figure 34.16(b)) or the six vertices [u, v, l] through [u, v, 6] (Figure 34.16(c)). In the latter case, the cycle will have to reenter the widget to visit vertices [v, u, l] through [v, u, 6]. Similarly, if the cycle enters through vertex [v, u, l], it must exit through ver- tex [v, u, 6], and it either visits all l2 of the widget’s vertices (Figure 34.16(d)) or the six vertices [v, u, l] through [v, u, 6] (Figure 34.16(c)). No other paths through the widget that visit all l2 vertices are possible. In particular, it is impossible to construct two vertex-disjoint paths, one of which connects [u, v, l] to [v, u, 6] and the other of which connects [v, u, l] to [u, v, 6], such that the union of the two paths contains all of the widget’s vertices.</p><p>w</p><p>x</p><p>z</p><p>y</p><p>(a)</p><p>s1</p><p>s2</p><p>[<i>w</i>,<i>x</i>,1]</p><p>[<i>x</i>,<i>w</i>,1]  [<i>x</i>,<i>y</i>,1]</p><p>[<i>y</i>,<i>x</i>,1]  [<i>w</i>,<i>y</i>,1]</p><p>[<i>y</i>,<i>w</i>,1]</p><p>[<i>w</i>,<i>z</i>,1]</p><p>[<i>z</i>,<i>w</i>,1]</p><p>Wwx</p><p>Wxy</p><p>Wwy</p><p>Wwz</p><p>[<i>w</i>,<i>x</i>,6]</p><p>[<i>x</i>,<i>w</i>,6]  [<i>x</i>,<i>y</i>,6]</p><p>[<i>y</i>,<i>x</i>,6]  [<i>w</i>,<i>y</i>,6]</p><p>[<i>y</i>,<i>w</i>,6]</p><p>[<i>w</i>,<i>z</i>,6]</p><p>[<i>z</i>,<i>w</i>,6]</p><p>(b)</p><p><b>Figure 34.17 </b>Reducing an instance of the vertex-cover problem to an instance of the hamiltonian- cycle problem. <b>(a) </b>An undirected graph G with a vertex cover of size 2, consisting of the lightly shaded vertices r and y. <b>(b) </b>The undirected graph G0 produced by the reduction, with the hamilto- nian path corresponding to the vertex cover shaded. The vertex cover {r, y} corresponds to edges</p><p>(s1, [r, x, l]) and (s2, [y, x, l]) appearing in the hamiltonian cycle.</p><p>The only other vertices in V 0 other than those of widgets are selector vertices s1, s2,. . . , sk. We use edges incident on selector vertices in G0 to select the k vertices of the cover in G.</p><p>e</p><p>e</p><p>{           :</p><p>In addition to the edges in widgets, E0 contains two other types of edges, which Figure 34.17 shows. First, for each vertex u   V , we add edges to join pairs of widgets in order to form a path containing all widgets corresponding to edges incident on u in G.  We arbitrarily order the vertices adjacent to each vertex u   V as u(1), u(2),. . . , u(degree(u)), where degree(u) is the number of vertices adjacent to u.  We create a path in G0 through all the widgets corresponding to edges incident on u by adding to E0 the edges  ([u, u(i), 6], [u, u(iC1), l])</p><p>≤ ≤     — }</p><p>l  i  degree(u)  l . In Figure 34.17, for example, we order the vertices ad- jacent to r as x, y, ´, and so graph G0 in part (b) of the figure includes the edges</p><p>e</p><p>([r, x, 6], [r, y, l]) and ([r, y, 6], [r, ´, l]). For each vertex u  V , these edges in G0 fill in a path containing all widgets corresponding to edges incident on u in G.</p><p>e</p><p>The intuition behind these edges is that if we choose a vertex u  V in the vertex cover of G, we can construct a path from [u, u(1), l] to [u, u(degree(u)), 6] in G0 that “covers” all widgets corresponding to edges incident on u. That is, for each of these widgets, say Wu,u(i) , the path either includes all l2 vertices (if u is in the vertex cover but u(i) is not) or just the six vertices [u, u(i), l], [u, u(i), 2],. . . , [u, u(i), 6] (if both u and u(i) are in the vertex cover).</p><p>The final type of edge in E0 joins the first vertex [u, u(1), l] and the last vertex [u, u(degree(u)), 6] of each of these paths to each of the selector vertices. That is, we include the edges</p><p>(1)</p><p>{(sj , [u,u </p><p>, l]) u  V and l  j  k</p><p>:  e    ≤  ≤ }</p><p>(degree(u))</p><p>[ {(sj , [u,u    , 6]) : u e V and l ≤ j ≤ k} .</p><p>≤ | |</p><p>Next, we show that the size of G0 is polynomial in the size of G, and hence we can construct G0 in time polynomial in the size of G. The vertices of G0 are those in the widgets, plus the selector vertices. With l2 vertices per widget, plus k   V selector vertices, we have a total of</p><p>|V 0| = l2 |E| + k</p><p>≤ l2 |E| + |V |</p><p>X</p><p>| |                e             —</p><p>vertices. The edges of G0 are those in the widgets, those that go between widgets, and those connecting selector vertices to widgets. Each widget contains l4 edges, totaling l4 E in all widgets. For each vertex u  V , graph G0 has degree(u)  l edges going between widgets, so that summed over all vertices in V ,</p><p>(degree(u) — l) = 2 |E| — |V |</p><p>u2V</p><p>| |</p><p>edges go between widgets. Finally, G0 has two edges for each pair consisting of a selector vertex and a vertex of V , totaling 2k V such edges. The total number of edges of G0 is therefore</p><p>|E0| = (l4 |E|) + (2 |E| — |V |) + (2k |V |)</p><p>= l6 |E| + (2k — l) |V |</p><p>≤ l6 |E| + (2 |V | — l) |V | .</p><p>Now we show that the transformation from graph G to G0 is a reduction. That is, we must show that G has a vertex cover of size k if and only if G0 has a hamiltonian cycle.</p><p>j e</p><p>= {      }</p><p>=               ⊆</p><p>Suppose that G   (V, E) has a vertex cover V * V of size k.  Let V * u1, u2,. . . , uk .  As Figure 34.17 shows, we form a hamiltonian cy- cle in G0 by including the following edges10 for each vertex u   V *. Include</p><p>j</p><p>j</p><p>j</p><p>j</p><p>j</p><p>edges ˚([u , u(i), 6], [u , u(iC1), l]) : l ≤ i ≤ degree(u ) — l#, which connect all</p><p>widgets corresponding to edges incident on uj . We also include the edges within</p><p>these widgets as Figures 34.16(b)–(d) show, depending on whether the edge is cov- ered by one or two vertices in V *. The hamiltonian cycle also includes the edges</p><p>(1)</p><p>{(sj , [uj , uj , l]) : l ≤ j ≤ k}</p><p>(degree(uj ))</p><p>[ {(sjC1, [uj , uj    , 6]) : l ≤ j ≤ k — l}</p><p>(degree(uk))</p><p>[ {(s1, [uk, uk    , 6])} .</p><p>By inspecting Figure 34.17, you can verify that these edges form a cycle. The cycle starts at s1, visits all widgets corresponding to edges incident on u1, then visits s2, visits all widgets corresponding to edges incident on u2, and so on, until it returns to s1. The cycle visits each widget either once or twice, depending on whether one or two vertices of V * cover its corresponding edge. Because V * is a vertex cover for G, each edge in E is incident on some vertex in V *, and so the cycle visits each vertex in each widget of G0. Because the cycle also visits every selector vertex, it is hamiltonian.</p><p>=                ⊆</p><p>Conversely, suppose that G0 (V 0,E0) has a hamiltonian cycle S  E0. We claim that the set</p><p>V * = {u e V : (sj , [u, u(1), l]) e S for some l ≤ j ≤ k}        (34.4)</p><p>e</p><p>e</p><p>e</p><p>e</p><p>is a vertex cover for G. To see why, partition S into maximal paths that start at some selector vertex si , traverse an edge (si, [u, u(1), l]) for some u  V , and end at a selector vertex sj without passing through any other selector vertex. Let us call each such path a “cover path.” From how G0 is constructed, each cover path must start at some si , take the edge (si , [u, u(1), l]) for some vertex u  V , pass through all the widgets corresponding to edges in E incident on u, and then end at some selector vertex sj . We refer to this cover path as pu, and by equation (34.4), we put u into V *. Each widget visited by pu must be Wuv or Wvu for some v  V . For each widget visited by pu, its vertices are visited by either one or two cover paths. If they are visited by one cover path, then edge (u, v)  E is covered in G</p><p>by vertex u. If two cover paths visit the widget, then the other cover path must be pv, which implies that v e V *, and edge (u, v) e E is covered by both u and v.</p><p>10Technically, we define a cycle in terms of vertices rather than edges (see Section B.4). In the interest of clarity, we abuse notation here and define the hamiltonian cycle in terms of edges.</p><p>u</p><p>4</p><p>v</p><p>3</p><p>2</p><p>x</p><p>5</p><p>w</p><div><p>1</p><p>1</p></div></li></ol></li></ol></ol></li></ol></li></ol></ol><p><b>Figure 34.18 </b>An instance of the traveling-salesman problem. Shaded edges represent a minimum- cost tour, with cost 7.</p><p>Because each vertex in each widget is visited by some cover path, we see that each edge in E is covered by some vertex in V *.</p><ol><ol><ol><li data-list-text="34.5.4"><p>The traveling-salesman problem</p><p>(    )</p><p>In the <i><b>traveling-salesman problem</b></i>, which is closely related to the hamiltonian- cycle problem, a salesman must visit n cities. Modeling the problem as a complete graph with n vertices, we can say that the salesman wishes to make a <i><b>tour</b></i>, or hamiltonian cycle, visiting each city exactly once and finishing at the city he starts from. The salesman incurs a nonnegative integer cost c(i, j) to travel from city i to city j , and the salesman wishes to make the tour whose total cost is minimum, where the total cost is the sum of the individual costs along the edges of the tour. For example, in Figure 34.18, a minimum-cost tour is u, r, v, x, u , with cost 7. The formal language for the corresponding decision problem is</p><p>TSP = {(G, c, k): G = (V, E) is a complete graph,</p><p>c is a function from V × V → Z, k e Z, and</p><p>G has a traveling-salesman tour with cost at most k} .</p><p>The following theorem shows that a fast algorithm for the traveling-salesman problem is unlikely to exist.</p><h3>Theorem 34.14</h3><p>The traveling-salesman problem is NP-complete.</p><p><i><b>Proof  </b></i>We first show that TSP belongs to NP. Given an instance of the problem, we use as a certificate the sequence of n vertices in the tour. The verification algorithm checks that this sequence contains each vertex exactly once, sums up the edge costs, and checks whether the sum is at most k. This process can certainly be done in polynomial time.</p><p>= (</p><p>To prove that TSP is NP-hard, we show that HAM-CYCLE ≤P TSP. Let G = (V, E) be an instance of HAM-CYCLE. We construct an instance of TSP as follows. We form the complete graph G0 = (V, E0), where E0 = {(i, j) : i, j e V and i ≠ j }, and we define the cost function c by</p><p>c(i, j)   0 if (i, j) e E , </p><p>l if (i, j) /e E . </p><p>e               (    )</p><p>=</p><p>(Note that because G is undirected, it has no self-loops, and so c(v, v)  l for all vertices v  V .) The instance of TSP is then G0, c, 0 , which we can easily create in polynomial time.</p><p>We now show that graph G has a hamiltonian cycle if and only if graph G0 has a tour of cost at most 0. Suppose that graph G has a hamiltonian cycle h. Each edge in h belongs to E and thus has cost 0 in G0. Thus, h is a tour in G0 with cost 0. Conversely, suppose that graph G0 has a tour h0 of cost at most 0. Since the costs of the edges in E0 are 0 and l, the cost of tour h0 is exactly 0 and each edge on the tour must have cost 0. Therefore, h0 contains only edges in E. We conclude that h0 is a hamiltonian cycle in graph G.</p></li><li data-list-text="34.5.5"><p>The subset-sum problem</p></li></ol></ol></ol><p>= {                                }</p><p>⊆</p><p>We next consider an arithmetic NP-complete problem. In the <i><b>subset-sum problem</b></i>, we are given a finite set S of positive integers and an integer <i><b>target </b></i>t &gt; 0. We ask whether there exists a subset S0 S whose elements sum to t . For example, if S   l, 2, 7, l4, 49, 98, 343, 686, 2409, 2793, l6808, l7206, ll7705, ll7993</p><p>=             = {                   }</p><p>and t  l38457, then the subset S0 l, 2, 7, 98, 343, 686, 2409, l7206, ll7705</p><p>is a solution.</p><p>P</p><p>As usual, we define the problem as a language:</p><p>SUBSET-SUM = {(S, t ): there exists a subset S0 ⊆ S such that t =  s2S0 s} .</p><p>As with any arithmetic problem, it is important to recall that our standard encoding assumes that the input integers are coded in binary. With this assumption in mind, we can show that the subset-sum problem is unlikely to have a fast algorithm.</p><h3>Theorem 34.15</h3><p>The subset-sum problem is NP-complete.</p><p>P</p><p>=</p><p>( )</p><p><i><b>Proof  </b></i>To show that SUBSET-SUM is in NP, for an instance S, t of the problem, we let the subset S0 be the certificate. A verification algorithm can check whether t    s2S0 s in polynomial time.</p><p>≤</p><p>We now show that 3-CNF-SAT P SUBSET-SUM. Given a 3-CNF formula ø</p><p>over variables x1, x2,. . . , xn with clauses S1, S2,. . . , Sk, each containing exactly</p><p>( )</p><p>three distinct literals, the reduction algorithm constructs an instance S, t of the subset-sum problem such that ø is satisfiable if and only if there exists a subset of S whose sum is exactly t . Without loss of generality, we make two simplifying assumptions about the formula ø. First, no clause contains both a variable and its negation, for such a clause is automatically satisfied by any assignment of values to the variables. Second, each variable appears in at least one clause, because it does not matter what value is assigned to a variable that appears in no clauses.</p><p>+</p><p>The reduction creates two numbers in set S for each variable xi and two numbers in S for each clause Sj . We shall create numbers in base l0, where each number contains n  k digits and each digit corresponds to either one variable or one clause. Base l0 (and other bases, as we shall see) has the property we need of preventing carries from lower digits to higher digits.</p><p>As Figure 34.19 shows, we construct set S and target t as follows. We label each digit position by either a variable or a clause. The least significant k digits are labeled by the clauses, and the most significant n digits are labeled by variables.</p><ul><li data-list-text="●"><p>The target t has a l in each digit labeled by a variable and a 4 in each digit labeled by a clause.</p><p>¬</p></li><li data-list-text="●"><p>For each variable xi , set S contains two integers vi and vi0 . Each of vi and vi0 has a l in the digit labeled by xi and 0s in the other variable digits. If literal xi appears in clause Sj , then the digit labeled by Sj in vi contains a l. If lit- eral xi appears in clause Sj , then the digit labeled by Sj in vi0 contains a l. All other digits labeled by clauses in vi and vi0 are 0.</p><p>≠</p><p>¬</p><p>¬</p><p>All vi and vi0 values in set S are unique. Why? For l  i , no vl or vl0 values can equal vi and vi0 in the most significant n digits. Furthermore, by our simplifying assumptions above, no vi and vi0 can be equal in all k least significant digits. If vi and vi0 were equal, then xi and  xi would have to appear in exactly the same set of clauses. But we assume that no clause contains both xi and  xi</p><p>¬</p><p>and that either xi or  xi appears in some clause, and so there must be some</p><p>clause Sj for which vi and vi0 differ.</p></li><li data-list-text="●"><p>For each clause Sj , set S contains two integers sj and sj0 . Each of sj and sj0 has 0s in all digits other than the one labeled by Sj . For sj , there is a l in the Sj digit, and sj0 has a 2 in this digit. These integers are “slack variables,” which we use to get each clause-labeled digit position to add to the target value of 4.</p></li></ul><p>Simple inspection of Figure 34.19 demonstrates that all sj and sj0</p><p>are unique in set S.</p><p>values in S</p><p>Note that the greatest sum of digits in any one digit position is 6, which occurs in the digits labeled by clauses (three ls from the vi and vi0 values, plus l and 2 from</p><p>x1  x2  x3  S1  S2  S3  S4</p><table cellspacing="0"><tr><td bgcolor="#A7A9AC"><p>v1</p></td><td bgcolor="#A7A9AC"><p>=</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>1</p></td></tr><tr><td bgcolor="#E7E8E8"><p>v10</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td></tr><tr><td bgcolor="#A7A9AC"><p>v2</p></td><td bgcolor="#A7A9AC"><p>=</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>1</p></td></tr><tr><td bgcolor="#E7E8E8"><p>v20</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td></tr><tr><td bgcolor="#E7E8E8"><p>v3</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>1</p></td></tr><tr><td bgcolor="#A7A9AC"><p>v30</p></td><td bgcolor="#A7A9AC"><p>=</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td></tr><tr><td bgcolor="#E7E8E8"><p>s1</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td></tr><tr><td bgcolor="#E7E8E8"><p>s10</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>2</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td></tr><tr><td bgcolor="#A7A9AC"><p>s2</p></td><td bgcolor="#A7A9AC"><p>=</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>1</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td></tr><tr><td bgcolor="#E7E8E8"><p>s20</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>2</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td></tr><tr><td bgcolor="#E7E8E8"><p>s3</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td><td bgcolor="#E7E8E8"><p>0</p></td></tr><tr><td bgcolor="#A7A9AC"><p>s30</p></td><td bgcolor="#A7A9AC"><p>=</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>0</p></td><td bgcolor="#A7A9AC"><p>2</p></td><td bgcolor="#A7A9AC"><p>0</p></td></tr><tr><td bgcolor="#E7E8E8"><p>s4</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>1</p></td></tr><tr><td bgcolor="#E7E8E8"><p>s40</p></td><td bgcolor="#E7E8E8"><p>=</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>0</p></td><td bgcolor="#E7E8E8"><p>2</p></td></tr></table><p>t  =  1  1  1  4  4  4  4</p><p>Figure 34.19  The reduction of 3-CNF-SAT to SUBSET-SUM. The formula in 3-CNF is ø = S1ˆS2 ˆS3ˆS4, where S1 = (x1v¬x2 v¬x3), S2 = (¬x1v¬x2v¬x3), S3 = (¬x1v¬x2vx3), and S4 = (x1 v x2 v x3). A satisfying assignment of ø is (x1 = 0, x2 = 0, x3 = l). The set S produced by the reduction consists of the base-l0 numbers shown; reading from top to bottom, S =</p><p>{l00l00l, l000ll0, l0000l, l0lll0, l00ll, lll00, l000, 2000, l00, 200, l0, 20, l, 2}. The target t is lll4444. The subset S0 ⊆ S is lightly shaded, and it contains v10 , v20 , and v3, corresponding to the satisfying assignment. It also contains slack variables s1, s10 , s20 , s3, s4, and s40 to achieve the target value of 4 in the digits labeled by S1 through S4.</p><p>the sj and sj0 values). Interpreting these numbers in base l0, therefore, no carries</p><p>can occur from lower digits to higher digits.11</p><p>+         +</p><p>+</p><p>+</p><p>We can perform the reduction in polynomial time. The set S contains 2n  2k values, each of which has n  k digits, and the time to produce each digit is poly- nomial in n  k. The target t has n  k digits, and the reduction produces each in constant time.</p><p>=       =</p><p>⊆</p><p>We now show that the 3-CNF formula ø is satisfiable if and only if there exists a subset S0 S whose sum is t . First, suppose that ø has a satisfying assignment. For i  l, 2, . . . , n, if xi l in this assignment, then include vi in S0. Otherwise, include vi0 . In other words, we include in S0 exactly the vi and vi0 values that cor-</p><p>11In fact, any base b, where b ≥ 7, would work. The instance at the beginning of this subsection is the set S and target t in Figure 34.19 interpreted in base 7, with S listed in sorted order.</p><p>¬ ¬</p><p>{  }</p><p>respond to literals with the value l in the satisfying assignment. Having included either vi or vi0 , but not both, for all i , and having put 0 in the digits labeled by variables in all sj and sj0 , we see that for each variable-labeled digit, the sum of the values of S0 must be l, which matches those digits of the target t . Because each clause is satisfied, the clause contains some literal with the value l. Therefore, each digit labeled by a clause has at least one l contributed to its sum by a vi or vi0 value in S0. In fact, l, 2, or 3 literals may be l in each clause, and so each clause- labeled digit has a sum of l, 2, or 3 from the vi and vi0 values in S0. In Figure 34.19 for example, literals  x1,  x2, and x3 have the value l in a satisfying assignment. Each of clauses S1 and S4 contains exactly one of these literals, and so together v10 , v20 , and v3 contribute l to the sum in the digits for S1 and S4. Clause S2 contains two of these literals, and v10 , v20 , and v3 contribute 2 to the sum in the digit for S2. Clause S3 contains all three of these literals, and v10 , v20 , and v3 contribute 3 to the sum in the digit for S3. We achieve the target of 4 in each digit labeled by clause Sj by including in S0 the appropriate nonempty subset of slack variables sj , sj0 . In Figure 34.19, S0 includes s1, s10 , s20 , s3, s4, and s40 . Since we have matched the target in all digits of the sum, and no carries can occur, the values of S0 sum to t .</p><p>⊆</p><p>Now, suppose that there is a subset S0 S that sums to t . The subset S0 must</p><p>=    e</p><p>e         =                   =</p><p>e      =</p><p>=</p><p>include exactly one of vi and vi0 for each i  l, 2, . . . , n, for otherwise the digits labeled by variables would not sum to l. If vi S0, we set xi l. Otherwise, vi0  S0, and we set xi 0. We claim that every clause Sj , for j  l, 2, . . . , k, is satisfied by this assignment. To prove this claim, note that to achieve a sum of 4 in the digit labeled by Sj , the subset S0 must include at least one vi or vi0 value that has a 1 in the digit labeled by Sj , since the contributions of the slack variables sj and sj0 together sum to at most 3. If S0 includes a vi that has a 1 in Sj ’s position, then the literal xi appears in clause Sj . Since we have set xi l when vi S0, clause Sj is satisfied. If S0 includes a vi0 that has a 1 in that position, then the</p><p>literal ¬xi appears in Sj . Since we have set xi = 0 when vi0 e S0, clause Sj is</p><p>again satisfied. Thus, all clauses of ø are satisfied, which completes the proof.</p><p>Exercises</p><h3>34.5-1</h3><p>The <i><b>subgraph-isomorphism problem </b></i>takes two undirected graphs G1 and G2, and it asks whether G1 is isomorphic to a subgraph of G2. Show that the subgraph- isomorphism problem is NP-complete.</p><h3>34.5-2</h3><p>×</p><p>Given an integer m  n matrix A and an integer m-vector b, the <i><b>0-1 integer- programming problem </b></i>asks whether there exists an integer n-vector x with ele-</p><p>{  }      ≤</p><p>ments in the set 0, l such that Ax  b. Prove that 0-1 integer programming is NP-complete. (<i>Hint: </i>Reduce from 3-CNF-SAT.)</p><h3>34.5-3</h3><p>The <i><b>integer linear-programming problem </b></i>is like the 0-1 integer-programming problem given in Exercise 34.5-2, except that the values of the vector x may be any integers rather than just 0 or l. Assuming that the 0-1 integer-programming problem is NP-hard, show that the integer linear-programming problem is NP- complete.</p><h3>34.5-4</h3><p>Show how to solve the subset-sum problem in polynomial time if the target value t is expressed in unary.</p><h3>34.5-5</h3><p>The <i><b>set-partition problem </b></i>takes as input a set S of numbers. The question is</p><p>whetPher the numPbers can be partitioned into two sets A and A = S — A such</p><p>that</p><h3>34.5-6</h3><p>x2A x =</p><p>x2A x. Show that the set-partition problem is NP-complete.</p><p>Show that the hamiltonian-path problem is NP-complete.</p><h3>34.5-7</h3><p>The <i><b>longest-simple-cycle problem </b></i>is the problem of determining a simple cycle (no repeated vertices) of maximum length in a graph. Formulate a related decision problem, and show that the decision problem is NP-complete.</p><h3>34.5-8</h3><p>In the <i><b>half 3-CNF satisfiability </b></i>problem, we are given a 3-CNF formula ø with n variables and m clauses, where m is even. We wish to determine whether there exists a truth assignment to the variables of ø such that exactly half the clauses evaluate to 0 and exactly half the clauses evaluate to l. Prove that the half 3-CNF satisfiability problem is NP-complete.</p><h2>Problems</h2><ol><ol><li data-list-text="34-1"><h3>Independent set</h3><p>=         ⊆</p><p>An <i><b>independent set </b></i>of a graph G   (V, E) is a subset V 0 V of vertices such that each edge in E is incident on at most one vertex in V 0. The <i><b>independent-set problem </b></i>is to find a maximum-size independent set in G.</p><ol><li data-list-text="a."><p>Formulate a related decision problem for the independent-set problem, and prove that it is NP-complete. (<i>Hint: </i>Reduce from the clique problem.)</p></li><li data-list-text="b."><p>Suppose that you are given a “black-box” subroutine to solve the decision prob- lem you defined in part (a). Give an algorithm to find an independent set of max- imum size. The running time of your algorithm should be polynomial in |V |</p><p>and |E|, counting queries to the black box as a single step.</p><p>Although the independent-set decision problem is NP-complete, certain special cases are polynomial-time solvable.</p></li><li data-list-text="c."><p>Give an efficient algorithm to solve the independent-set problem when each ver- tex in G has degree 2. Analyze the running time, and prove that your algorithm works correctly.</p></li><li data-list-text="d."><p>Give an efficient algorithm to solve the independent-set problem when G is bipartite. Analyze the running time, and prove that your algorithm works cor- rectly. (<i>Hint: </i>Use the results of Section 26.3.)</p></li></ol></li><li data-list-text="34-2"><h3>Bonnie and Clyde</h3><p>Bonnie and Clyde have just robbed a bank. They have a bag of money and want to divide it up. For each of the following scenarios, either give a polynomial-time algorithm, or prove that the problem is NP-complete. The input in each case is a list of the n items in the bag, along with the value of each.</p><ol><li data-list-text="a."><p>The bag contains n coins, but only 2 different denominations: some coins are worth x dollars, and some are worth y dollars. Bonnie and Clyde wish to divide the money exactly evenly.</p></li><li data-list-text="b."><p>The bag contains n coins, with an arbitrary number of different denominations, but each denomination is a nonnegative integer power of 2, i.e., the possible denominations are l dollar, 2 dollars, 4 dollars, etc. Bonnie and Clyde wish to divide the money exactly evenly.</p></li><li data-list-text="c."><p>The bag contains n checks, which are, in an amazing coincidence, made out to “Bonnie or Clyde.” They wish to divide the checks so that they each get the exact same amount of money.</p></li><li data-list-text="d."><p>The bag contains n checks as in part (c), but this time Bonnie and Clyde are willing to accept a split in which the difference is no larger than l00 dollars.</p></li></ol></li><li data-list-text="34-3"><h3>Graph coloring</h3><p>=</p><p>e</p><p>:  → {     }       ≠</p><p>Mapmakers try to use as few colors as possible when coloring countries on a map, as long as no two countries that share a border have the same color. We can model this problem with an undirected graph G   (V, E) in which each vertex repre- sents a country and vertices whose respective countries share a border are adjacent. Then, a k-coloring is a function c  V    l, 2, . . . , k such that c(u)  c(v) for every edge (u, v)  E. In other words, the numbers l, 2, . . . , k represent the k col- ors, and adjacent vertices must have different colors. The graph-coloring problem is to determine the minimum number of colors needed to color a given graph.</p><ol><li data-list-text="a."><p>Give an efficient algorithm to determine a 2-coloring of a graph, if one exists.</p></li><li data-list-text="b."><p>Cast the graph-coloring problem as a decision problem. Show that your deci- sion problem is solvable in polynomial time if and only if the graph-coloring problem is solvable in polynomial time.</p></li><li data-list-text="c."><p>Let the language 3-COLOR be the set of graphs that can be 3-colored. Show that if 3-COLOR is NP-complete, then your decision problem from part (b) is NP-complete.</p><p>=</p><p>To prove that 3-COLOR is NP-complete, we use a reduction from 3-CNF-SAT. Given a formula ø of m clauses on n variables x1, x2,. . . , xn, we construct a graph G   (V, E) as follows. The set V consists of a vertex for each variable, a vertex for the negation of each variable, 5 vertices for each clause, and 3 special vertices: TRUE, FALSE, and RED. The edges of the graph are of two types: “literal” edges that are independent of the clauses and “clause” edges that depend on the clauses. The literal edges form a triangle on the special vertices and also form a triangle on</p><p>xi , ¬xi , and RED for i = l, 2, . . . , n.</p></li><li data-list-text="d."><p>Argue that in any 3-coloring c of a graph containing the literal edges, exactly one of a variable and its negation is colored c(TRUE) and the other is colored c(FALSE). Argue that for any truth assignment for ø, there exists a 3-coloring of the graph containing just the literal edges.</p><p>v v</p><p>The widget shown in Figure 34.20 helps to enforce the condition corresponding to a clause (x  y  ´). Each clause requires a unique copy of the 5 vertices that are heavily shaded in the figure; they connect as shown to the literals of the clause and the special vertex TRUE.</p></li><li data-list-text="e."><p>Argue that if each of x, y, and ´ is colored c(TRUE) or c(FALSE), then the widget is 3-colorable if and only if at least one of x, y, or ´ is colored c(TRUE).</p></li><li data-list-text="f."><p>Complete the proof that 3-COLOR is NP-complete.</p></li></ol><p>x</p><p>y</p><p>TRUE</p><p>z</p><p>Figure 34.20 The widget corresponding to a clause (x v y v ´), used in Problem 34-3.</p></li><li data-list-text="34-4"><h3>Scheduling with profits and deadlines</h3></li></ol></ol><p>Suppose that we have one machine and a set of n tasks a1, a2,. . . , an, each of which requires time on the machine. Each task aj requires tj time units on the machine (its processing time), yields a profit of pj , and has a deadline dj . The machine can process only one task at a time, and task aj must run without inter- ruption for tj consecutive time units. If we complete task aj by its deadline dj , we receive a profit pj , but if we complete it after its deadline, we receive no profit. As an optimization problem, we are given the processing times, profits, and deadlines for a set of n tasks, and we wish to find a schedule that completes all the tasks and returns the greatest amount of profit. The processing times, profits, and deadlines are all nonnegative numbers.</p><ol><li data-list-text="a."><p>State this problem as a decision problem.</p></li><li data-list-text="b."><p>Show that the decision problem is NP-complete.</p></li><li data-list-text="c."><p>Give a polynomial-time algorithm for the decision problem, assuming that all processing times are integers from l to n. (<i>Hint: </i>Use dynamic programming.)</p></li><li data-list-text="d."><p>Give a polynomial-time algorithm for the optimization problem, assuming that all processing times are integers from l to n.</p></li></ol><h2>Chapter notes</h2><p>The book by Garey and Johnson [129] provides a wonderful guide to NP-complete- ness, discussing the theory at length and providing a catalogue of many problems that were known to be NP-complete in 1979. The proof of Theorem 34.13 is adapted from their book, and the list of NP-complete problem domains at the begin- ning of Section 34.5 is drawn from their table of contents. Johnson wrote a series</p><p>of 23 columns in the <i>Journal of Algorithms </i>between 1981 and 1992 reporting new developments in NP-completeness. Hopcroft, Motwani, and Ullman [177], Lewis and Papadimitriou [236], Papadimitriou [270], and Sipser [317] have good treat- ments of NP-completeness in the context of complexity theory. NP-completeness and several reductions also appear in books by Aho, Hopcroft, and Ullman [5]; Dasgupta, Papadimitriou, and Vazirani [82]; Johnsonbaugh and Schaefer [193]; and Kleinberg and Tardos [208].</p><p>≠</p><p>The class P was introduced in 1964 by Cobham [72] and, independently, in 1965 by Edmonds [100], who also introduced the class NP and conjectured that P  NP. The notion of NP-completeness was proposed in 1971 by Cook [75], who gave the first NP-completeness proofs for formula satisfiability and 3-CNF satisfiabil- ity. Levin [234] independently discovered the notion, giving an NP-completeness proof for a tiling problem. Karp [199] introduced the methodology of reductions in 1972 and demonstrated the rich variety of NP-complete problems. Karp’s pa- per included the original NP-completeness proofs of the clique, vertex-cover, and hamiltonian-cycle problems. Since then, thousands of problems have been proven to be NP-complete by many researchers. In a talk at a meeting celebrating Karp’s 60th birthday in 1995, Papadimitriou remarked, “about 6000 papers each year have the term ‘NP-complete’ on their title, abstract, or list of keywords. This is more than each of the terms ‘compiler,’ ‘database,’ ‘expert,’ ‘neural network,’ or ‘oper- ating system.’ ”</p><p>Recent work in complexity theory has shed light on the complexity of computing approximate solutions. This work gives a new definition of NP using “probabilis- tically checkable proofs.” This new definition implies that for problems such as clique, vertex cover, the traveling-salesman problem with the triangle inequality, and many others, computing good approximate solutions is NP-hard and hence no easier than computing optimal solutions. An introduction to this area can be found in Arora’s thesis [20]; a chapter by Arora and Lund in Hochbaum [172]; a survey article by Arora [21]; a book edited by Mayr, Pro¨mel, and Steger [246]; and a survey article by Johnson [191].</p><ol><li data-list-text="35"><h1>Approximation Algorithms</h1><p>Many problems of practical significance are NP-complete, yet they are too impor- tant to abandon merely because we don’t know how to find an optimal solution in polynomial time. Even if a problem is NP-complete, there may be hope. We have at least three ways to get around NP-completeness. First, if the actual inputs are small, an algorithm with exponential running time may be perfectly satisfactory. Second, we may be able to isolate important special cases that we can solve in polynomial time. Third, we might come up with approaches to find <i>near-optimal </i>solutions in polynomial time (either in the worst case or the expected case). In practice, near- optimality is often good enough. We call an algorithm that returns near-optimal solutions an <i><b>approximation algorithm</b></i>. This chapter presents polynomial-time ap- proximation algorithms for several NP-complete problems.</p><p>Performance ratios for approximation algorithms</p><p>Suppose that we are working on an optimization problem in which each potential solution has a positive cost, and we wish to find a near-optimal solution. Depending on the problem, we may define an optimal solution as one with maximum possi- ble cost or one with minimum possible cost; that is, the problem may be either a maximization or a minimization problem.</p><p>We say that an algorithm for a problem has an <i><b>approximation ratio </b></i>of p(n) if, for any input of size n, the cost S of the solution produced by the algorithm is within a factor of p(n) of the cost S* of an optimal solution:</p><p>S*</p><p>S</p><p>max ! <u> S </u>, S* " ! p(n) .                           (35.1)</p><p>!</p><p>If an algorithm achieves an approximation ratio of p(n), we call it a !<i>(n)</i>-approx- imation algorithm.  The definitions of the approximation ratio and of a p(n)- approximation algorithm apply to both minimization and maximization problems. For a maximization problem, 0 &lt; S   S*, and the ratio S*/S gives the factor by which the cost of an optimal solution is larger than the cost of the approximate</p><p>!</p><p>!       "</p><p>solution. Similarly, for a minimization problem, 0 &lt; S* S, and the ratio S/S* gives the factor by which the cost of the approximate solution is larger than the cost of an optimal solution. Because we assume that all solutions have positive cost, these ratios are always well defined. The approximation ratio of an approx- imation algorithm is never less than l, since S/S* l implies S*/S  l. Therefore, a l-approximation algorithm1 produces an optimal solution, and an ap- proximation algorithm with a large approximation ratio may return a solution that is much worse than optimal.</p><p>For many problems, we have polynomial-time approximation algorithms with small constant approximation ratios, although for other problems, the best known polynomial-time approximation algorithms have approximation ratios that grow as functions of the input size n. An example of such a problem is the set-cover problem presented in Section 35.3.</p><p>Some NP-complete problems allow polynomial-time approximation algorithms that can achieve increasingly better approximation ratios by using more and more computation time. That is, we can trade computation time for the quality of the approximation. An example is the subset-sum problem studied in Section 35.5. This situation is important enough to deserve a name of its own.</p><p>An <i><b>approximation scheme </b></i>for an optimization problem is an approximation al- gorithm that takes as input not only an instance of the problem, but also a value</p><p>C</p><p>‹ &gt; 0 such that for any fixed ‹, the scheme is a (l  ‹)-approximation algorithm. We say that an approximation scheme is a <i><b>polynomial-time approximation scheme </b></i>if for any fixed ‹ &gt; 0, the scheme runs in time polynomial in the size n of its input instance.</p><p>The running time of a polynomial-time approximation scheme can increase very rapidly as ‹ decreases. For example, the running time of a polynomial-time ap- proximation scheme might be O(n2/!). Ideally, if ‹ decreases by a constant factor, the running time to achieve the desired approximation should not increase by more than a constant factor (though not necessarily the same constant factor by which ‹ decreased).</p><p>We say that an approximation scheme is a fully polynomial-time approximation scheme if it is an approximation scheme and its running time is polynomial in both l/‹ and the size n of the input instance. For example, the scheme might have a running time of O((l/‹)2n3). With such a scheme, any constant-factor decrease in ‹ comes with a corresponding constant-factor increase in the running time.</p><p>1When the approximation ratio is independent of n, we use the terms “approximation ratio of p” and “p-approximation algorithm,” indicating no dependence on n.</p><p>1108      Chapter 35 Approximation Algorithms</p><p>Chapter outline</p><p>D</p><p>"</p><p>The first four sections of this chapter present some examples of polynomial-time approximation algorithms for NP-complete problems, and the fifth section presents a fully polynomial-time approximation scheme. Section 35.1 begins with a study of the vertex-cover problem, an NP-complete minimization problem that has an approximation algorithm with an approximation ratio of 2. Section 35.2 presents an approximation algorithm with an approximation ratio of 2 for the case of the traveling-salesman problem in which the cost function satisfies the triangle in- equality. It also shows that without the triangle inequality, for any constant p  l, a p-approximation algorithm cannot exist unless P   NP. In Section 35.3, we show how to use a greedy method as an effective approximation algorithm for the set-covering problem, obtaining a covering whose cost is at worst a logarithmic factor larger than the optimal cost. Section 35.4 presents two more approximation algorithms. First we study the optimization version of 3-CNF satisfiability and give a simple randomized algorithm that produces a solution with an expected ap- proximation ratio of 8/7. Then we examine a weighted variant of the vertex-cover problem and show how to use linear programming to develop a 2-approximation algorithm. Finally, Section 35.5 presents a fully polynomial-time approximation scheme for the subset-sum problem.</p><ol><li data-list-text="35.1"><h2>The vertex-cover problem</h2><p>2  2</p><p>D          ⊆</p><p>Section 34.5.2 defined the vertex-cover problem and proved it NP-complete. Recall that a <i><b>vertex cover </b></i>of an undirected graph G   (V, E) is a subset V 0 V such that if (u, v) is an edge of G, then either u  V 0 or v  V 0 (or both). The size of a vertex cover is the number of vertices in it.</p><p>The <i><b>vertex-cover problem </b></i>is to find a vertex cover of minimum size in a given undirected graph. We call such a vertex cover an <i><b>optimal vertex cover</b></i>. This prob- lem is the optimization version of an NP-complete decision problem.</p><p>Even though we don’t know how to find an optimal vertex cover in a graph G in polynomial time, we can efficiently find a vertex cover that is near-optimal. The following approximation algorithm takes as input an undirected graph G and returns a vertex cover whose size is guaranteed to be no more than twice the size of an optimal vertex cover.</p><p>Chapter outline</p><p>D</p><p>!</p><p>The first four sections of this chapter present some examples of polynomial-time approximation algorithms for NP-complete problems, and the fifth section presents a fully polynomial-time approximation scheme. Section 35.1 begins with a study of the vertex-cover problem, an NP-complete minimization problem that has an approximation algorithm with an approximation ratio of 2. Section 35.2 presents an approximation algorithm with an approximation ratio of 2 for the case of the traveling-salesman problem in which the cost function satisfies the triangle in- equality. It also shows that without the triangle inequality, for any constant p  l, a p-approximation algorithm cannot exist unless P   NP. In Section 35.3, we show how to use a greedy method as an effective approximation algorithm for the set-covering problem, obtaining a covering whose cost is at worst a logarithmic factor larger than the optimal cost. Section 35.4 presents two more approximation algorithms. First we study the optimization version of 3-CNF satisfiability and give a simple randomized algorithm that produces a solution with an expected ap- proximation ratio of 8/7. Then we examine a weighted variant of the vertex-cover problem and show how to use linear programming to develop a 2-approximation algorithm. Finally, Section 35.5 presents a fully polynomial-time approximation scheme for the subset-sum problem.</p><ol><ol><li data-list-text="35.1"><h2>The vertex-cover problem</h2><p>2  2</p><p>D         "</p><p>Section 34.5.2 defined the vertex-cover problem and proved it NP-complete. Recall that a vertex cover of an undirected graph G   (V, E) is a subset V 0 V such that if (u, v) is an edge of G, then either u  V 0 or v  V 0 (or both). The size of a vertex cover is the number of vertices in it.</p><p>The <i><b>vertex-cover problem </b></i>is to find a vertex cover of minimum size in a given undirected graph. We call such a vertex cover an <i><b>optimal vertex cover</b></i>. This prob- lem is the optimization version of an NP-complete decision problem.</p><p>Even though we don’t know how to find an optimal vertex cover in a graph G in polynomial time, we can efficiently find a vertex cover that is near-optimal. The following approximation algorithm takes as input an undirected graph G and returns a vertex cover whose size is guaranteed to be no more than twice the size of an optimal vertex cover.</p><ol><ol><li data-list-text="35.1"><p>The vertex-cover problem                             1109</p><p>b</p><p>c</p><p>d</p><p>a</p><p>e</p><p>f</p><p>g</p><p>b</p><p>c</p><p>d</p><p>a</p><p>e</p><p>f</p><p>g</p><p>(a)                       (b)</p><p>b</p><p>c</p><p>d</p><p>a</p><p>e</p><p>f</p><p>g</p><p>b</p><p>c</p><p>d</p><p>a</p><p>e</p><p>f</p><p>g</p><p>(c)                       (d)</p><p>b</p><p>c</p><p>d</p><p>a</p><p>e</p><p>f</p><p>g</p><p>b</p><p>c</p><p>d</p><p>a</p><p>e</p><p>f</p><p>g</p><p>(e)                       (f)</p><p><b>Figure 35.1 </b>The operation of APPROX-VERTEX-COVER. <b>(a) </b>The input graph G, which has 7 vertices and 8 edges. <b>(b) </b>The edge (b, c), shown heavy, is the first edge chosen by APPROX-VERTEX- COVER. Vertices b and c, shown lightly shaded, are added to the set S containing the vertex cover being created. Edges (a, b), (c, e), and (c, d), shown dashed, are removed since they are now covered by some vertex in S . <b>(c) </b>Edge (e, f ) is chosen; vertices e and f are added to S . <b>(d) </b>Edge (d, g) is chosen; vertices d and g are added to S . <b>(e) </b>The set S , which is the vertex cover produced by APPROX-VERTEX-COVER, contains the six vertices b, c, d, e, f, g. <b>(f) </b>The optimal vertex cover for this problem contains only three vertices: b, d , and e.</p><p>Approx-Vertex-Cover (G)</p><p>D ; </p><ol><li data-list-text="1"><p>S</p><p>D</p></li><li data-list-text="2"><p>E0 G.E</p><p>¤; </p></li><li data-list-text="3"><p>while E0</p></li><li data-list-text="4"><p>let (u, v) be an arbitrary edge of E0</p><p>D [ f  g</p></li><li data-list-text="5"><p>S  S  u, v</p></li><li data-list-text="6"><p>remove from E0 every edge incident on either u or v</p></li><li data-list-text="7"><p>return S</p></li></ol><p>Figure 35.1 illustrates how APPROX-VERTEX-COVER operates on an example graph. The variable S contains the vertex cover being constructed. Line 1 ini- tializes S to the empty set. Line 2 sets E0 to be a copy of the edge set G.<i>E </i>of the graph. The loop of lines 3–6 repeatedly picks an edge (u, v) from E0, adds its</p><p>endpoints u and v to S , and deletes all edges in E0 that are covered by either u or v. Finally, line 7 returns the vertex cover S . The running time of this algorithm is O(V C E), using adjacency lists to represent E0.</p><h3>Theorem 35.1</h3><p>APPROX-VERTEX-COVER is a polynomial-time 2-approximation algorithm.</p><p><i><b>Proof  </b></i>We have already shown that APPROX-VERTEX-COVER runs in polyno- mial time.</p><p>The set S of vertices that is returned by APPROX-VERTEX-COVER is a vertex cover, since the algorithm loops until every edge in G.<i>E </i>has been covered by some vertex in S .</p><p>To see that APPROX-VERTEX-COVER returns a vertex cover that is at most twice the size of an optimal cover, let A denote the set of edges that line 4 of APPROX- VERTEX-COVER picked. In order to cover the edges in A, any vertex cover—in particular, an optimal cover S *—must include at least one endpoint of each edge in A. No two edges in A share an endpoint, since once an edge is picked in line 4, all other edges that are incident on its endpoints are deleted from E0 in line 6. Thus, no two edges in A are covered by the same vertex from S *, and we have the lower bound</p><p>jS *j ! jAj                              (35.2)</p><p>on the size of an optimal vertex cover. Each execution of line 4 picks an edge for which neither of its endpoints is already in S , yielding an upper bound (an exact upper bound, in fact) on the size of the vertex cover returned:</p><p>jS j D 2 jAj .                                   (35.3)</p><p>Combining equations (35.2) and (35.3), we obtain</p><p>jS j D 2 jAj</p><p>≤ 2 jS *j ,</p><p>thereby proving the theorem.</p><p>Let us reflect on this proof. At first, you might wonder how we can possibly prove that the size of the vertex cover returned by APPROX-VERTEX-COVER is at most twice the size of an optimal vertex cover, when we do not even know the size of an optimal vertex cover. Instead of requiring that we know the exact size of an optimal vertex cover, we rely on a lower bound on the size. As Exercise 35.1-2 asks you to show, the set A of edges that line 4 of APPROX-VERTEX-COVER selects is actually a maximal matching in the graph G. (A <i><b>maximal matching </b></i>is a matching that is not a proper subset of any other matching.) The size of a maximal matching</p></li><li data-list-text="35.2"><p>The traveling-salesman problem                          1111</p></li></ol></ol><p>is, as we argued in the proof of Theorem 35.1, a lower bound on the size of an optimal vertex cover. The algorithm returns a vertex cover whose size is at most twice the size of the maximal matching A. By relating the size of the solution returned to the lower bound, we obtain our approximation ratio. We will use this methodology in later sections as well.</p><p>Exercises</p><h3>35.1-1</h3><p>Give an example of a graph for which APPROX-VERTEX-COVER always yields a suboptimal solution.</p><h3>35.1-2</h3><p>Prove that the set of edges picked in line 4 of APPROX-VERTEX-COVER forms a maximal matching in the graph G.</p><h3>35.1-3 x</h3><p>Professor Bu¨ndchen proposes the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its in- cident edges. Give an example to show that the professor’s heuristic does not have an approximation ratio of 2. (<i>Hint: </i>Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.)</p><h3>35.1-4</h3><p>Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time.</p><h3>35.1-5</h3><p>From the proof of Theorem 34.12, we know that the vertex-cover problem and the NP-complete clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer.</p></li><li data-list-text="35.2"><h2>The traveling-salesman problem</h2><p>2</p><p>D</p><p>In the traveling-salesman problem introduced in Section 34.5.4, we are given a complete undirected graph G  (V, E) that has a nonnegative integer cost c(u, v) associated with each edge (u, v)  E, and we must find a hamiltonian cycle (a tour) of G with minimum cost. As an extension of our notation, let c(A) denote the total cost of the edges in the subset A " E:</p><p>is, as we argued in the proof of Theorem 35.1, a lower bound on the size of an optimal vertex cover. The algorithm returns a vertex cover whose size is at most twice the size of the maximal matching A. By relating the size of the solution returned to the lower bound, we obtain our approximation ratio. We will use this methodology in later sections as well.</p><p>Exercises</p><h3>35.1-1</h3><p>Give an example of a graph for which APPROX-VERTEX-COVER always yields a suboptimal solution.</p><h3>35.1-2</h3><p>Prove that the set of edges picked in line 4 of APPROX-VERTEX-COVER forms a maximal matching in the graph G.</p><h3>35.1-3 x</h3><p>Professor Bu¨ndchen proposes the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its in- cident edges. Give an example to show that the professor’s heuristic does not have an approximation ratio of 2. (<i>Hint: </i>Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.)</p><h3>35.1-4</h3><p>Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time.</p><h3>35.1-5</h3><p>From the proof of Theorem 34.12, we know that the vertex-cover problem and the NP-complete clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer.</p></li></ol></ol></li><li data-list-text="35.2"><h2>The traveling-salesman problem</h2><p>2</p><p>D</p><p>In the traveling-salesman problem introduced in Section 34.5.4, we are given a complete undirected graph G   (V, E) that has a nonnegative integer cost c(u, v) associated with each edge (u, v)   E, and we must find a hamiltonian cycle (a tour) of G with minimum cost. As an extension of our notation, let c(A) denote the total cost of the edges in the subset A ! E:</p><p>X</p><p>c(A) D</p><p>(u,v)2A</p><p>c(u, v) .</p><p>In many practical situations, the least costly way to go from a place u to a place r is to go directly, with no intermediate steps. Put another way, cutting out an inter- mediate stop never increases the cost. We formalize this notion by saying that the cost function c satisfies the triangle inequality if, for all vertices u, v, r 2 V ,</p><p>c(u, r) " c(u, v) C c(v, r) .</p><p>The triangle inequality seems as though it should naturally hold, and it is au- tomatically satisfied in several applications. For example, if the vertices of the graph are points in the plane and the cost of traveling between two vertices is the ordinary euclidean distance between them, then the triangle inequality is satisfied. Furthermore, many cost functions other than euclidean distance satisfy the triangle inequality.</p><p>As Exercise 35.2-2 shows, the traveling-salesman problem is NP-complete even if we require that the cost function satisfy the triangle inequality. Thus, we should not expect to find a polynomial-time algorithm for solving this problem exactly. Instead, we look for good approximation algorithms.</p><p>In Section 35.2.1, we examine a 2-approximation algorithm for the traveling- salesman problem with the triangle inequality. In Section 35.2.2, we show that without the triangle inequality, a polynomial-time approximation algorithm with a constant approximation ratio does not exist unless P D NP.</p><ol><li data-list-text="35.2.1"><p>The traveling-salesman problem with the triangle inequality</p><p>Applying the methodology of the previous section, we shall first compute a struc- ture—a minimum spanning tree—whose weight gives a lower bound on the length of an optimal traveling-salesman tour. We shall then use the minimum spanning tree to create a tour whose cost is no more than twice that of the minimum spanning tree’s weight, as long as the cost function satisfies the triangle inequality. The fol- lowing algorithm implements this approach, calling the minimum-spanning-tree algorithm MST-PRIM from Section 23.2 as a subroutine. The parameter G is a complete undirected graph, and the cost function c satisfies the triangle inequality.</p><p>Approx-TSP-Tour(G, c)</p><p>2</p><ol><li data-list-text="1"><p>select a vertex r  G.<i>V </i>to be a “root” vertex</p></li><li data-list-text="2"><p>compute a minimum spanning tree T for G from root r</p><p>using MST-PRIM(G, c, r)</p></li><li data-list-text="3"><p>let H be a list of vertices, ordered according to when they are first visited in a preorder tree walk of T</p></li><li data-list-text="4"><p>return the hamiltonian cycle H</p></li></ol><p>a</p><p>d</p><p>e</p><p>b</p><p>f</p><p>g</p><p>c</p><p>h</p><p>a</p><p>d</p><p>e</p><p>b</p><p>f</p><p>g</p><p>c</p><p>h</p><p>a</p><p>d</p><p>e</p><p>b</p><p>f</p><p>g</p><p>c</p><p>h</p><p>(a)</p><p>(b)</p><p>(c)</p><p>a</p><p>d</p><p>e</p><p>b</p><p>f</p><p>g</p><p>c</p><p>h</p><p>a</p><p>d</p><p>e</p><p>b</p><p>f</p><p>g</p><p>c</p><p>h</p><p>(d)                (e)</p><p><b>Figure 35.2 </b>The operation of APPROX-TSP-TOUR. <b>(a) </b>A complete undirected graph. Vertices lie on intersections of integer grid lines. For example, f is one unit to the right and two units up from h. The cost function between two points is the ordinary euclidean distance. <b>(b) </b>A minimum spanning tree T of the complete graph, as computed by MST-PRIM. Vertex a is the root vertex. Only edges in the minimum spanning tree are shown. The vertices happen to be labeled in such a way that they are added to the main tree by MST-PRIM in alphabetical order. <b>(c) </b>A walk of T , starting at a. A full walk of the tree visits the vertices in the order a, b, c, b, h, b, a, d, e, f, e, g, e, d, a. A preorder walk of T lists a vertex just when it is first encountered, as indicated by the dot next to each vertex, yielding the ordering a, b, c, h, d, e, f, g. <b>(d) </b>A tour obtained by visiting the vertices in the order given by the preorder walk, which is the tour H returned by APPROX-TSP-TOUR. Its total cost is approximately 19.074. <b>(e) </b>An optimal tour H * for the original complete graph. Its total cost is approximately 14.715.</p><p>Recall from Section 12.1 that a preorder tree walk recursively visits every vertex in the tree, listing a vertex when it is first encountered, before visiting any of its children.</p><p>Figure 35.2 illustrates the operation of APPROX-TSP-TOUR. Part (a) of the fig- ure shows a complete undirected graph, and part (b) shows the minimum spanning tree T grown from root vertex a by MST-PRIM. Part (c) shows how a preorder walk of T visits the vertices, and part (d) displays the corresponding tour, which is the tour returned by APPROX-TSP-TOUR. Part (e) displays an optimal tour, which is about 23% shorter.</p><p>By Exercise 23.2-2, even with a simple implementation of MST-PRIM, the run- ning time of APPROX-TSP-TOUR is ①(V 2). We now show that if the cost function for an instance of the traveling-salesman problem satisfies the triangle inequality, then APPROX-TSP-TOUR returns a tour whose cost is not more than twice the cost of an optimal tour.</p><h3>Theorem 35.2</h3><p>APPROX-TSP-TOUR is a polynomial-time 2-approximation algorithm for the traveling-salesman problem with the triangle inequality.</p><p><i><b>Proof  </b></i>We have already seen that APPROX-TSP-TOUR runs in polynomial time. Let H * denote an optimal tour for the given set of vertices. We obtain a spanning tree by deleting any edge from a tour, and each edge cost is nonnegative. Therefore, the weight of the minimum spanning tree T computed in line 2 of APPROX-TSP-</p><p>TOUR provides a lower bound on the cost of an optimal tour:</p><p>c(T ) " c(H *) .                                 (35.4)</p><p>A <i><b>full walk </b></i>of T lists the vertices when they are first visited and also whenever they are returned to after a visit to a subtree. Let us call this full walk W . The full walk of our example gives the order</p><p>a, b, c, b, h, b, a, d, e, f, e, g, e, d, a .</p><p>Since the full walk traverses every edge of T exactly twice, we have (extending our definition of the cost c in the natural manner to handle multisets of edges) c(W ) D 2c(T ) .                             (35.5)</p><p>Inequality (35.4) and equation (35.5) imply that</p><p>c(W ) " 2c(H *) ,                                (35.6)</p><p>and so the cost of W is within a factor of 2 of the cost of an optimal tour.</p><p>Unfortunately, the full walk W is generally not a tour, since it visits some ver- tices more than once. By the triangle inequality, however, we can delete a visit to any vertex from W and the cost does not increase. (If we delete a vertex v from W between visits to u and r, the resulting ordering specifies going directly from u to r.) By repeatedly applying this operation, we can remove from W all but the first visit to each vertex. In our example, this leaves the ordering</p><p>a, b, c, h, d, e, f, g .</p><p>This ordering is the same as that obtained by a preorder walk of the tree T . Let H be the cycle corresponding to this preorder walk. It is a hamiltonian cycle, since ev-</p><p>ery vertex is visited exactly once, and in fact it is the cycle computed by APPROX- TSP-TOUR. Since H is obtained by deleting vertices from the full walk W , we have</p><p>c(H) " c(W ) .                                 (35.7)</p><p>"</p><p>Combining inequalities (35.6) and (35.7) gives c(H)  2c(H *), which completes the proof.</p><p>In spite of the nice approximation ratio provided by Theorem 35.2, APPROX- TSP-TOUR is usually not the best practical choice for this problem. There are other approximation algorithms that typically perform much better in practice. (See the references at the end of this chapter.)</p></li><li data-list-text="35.2.2"><p>The general traveling-salesman problem</p><p>If we drop the assumption that the cost function c satisfies the triangle inequality, then we cannot find good approximate tours in polynomial time unless P D NP.</p><h3>Theorem 35.3</h3><p>¤            ≥</p><p>If P  NP, then for any constant p  1, there is no polynomial-time approximation algorithm with approximation ratio p for the general traveling-salesman problem.</p><p>≥</p><p>D</p><p><i><b>Proof  </b></i>The proof is by contradiction. Suppose to the contrary that for some num- ber p   1, there is a polynomial-time approximation algorithm A with approx- imation ratio p. Without loss of generality, we assume that p is an integer, by rounding it up if necessary. We shall then show how to use A to solve instances of the hamiltonian-cycle problem (defined in Section 34.2) in polynomial time. Since Theorem 34.13 tells us that the hamiltonian-cycle problem is NP-complete, Theorem 34.4 implies that if we can solve it in polynomial time, then P  NP.</p><p>D</p><p>D</p><p>Let G   (V, E) be an instance of the hamiltonian-cycle problem. We wish to determine efficiently whether G contains a hamiltonian cycle by making use of the hypothesized approximation algorithm A. We turn G into an instance of the traveling-salesman problem as follows. Let G0 (V, E0) be the complete graph on V ; that is,</p><p>E0 D f(u, v) W u, v 2 V and u ¤ vg .</p><p>D (</p><p>Assign an integer cost to each edge in E0 as follows:</p><p>c(u, v)   1     if (u, v) 2 E , </p><p>p jV j C 1  otherwise .</p><p>We can create representations of G0 and c from a representation of G in time poly- nomial in jV j and jEj.</p><p>j j</p><p>Now, consider the traveling-salesman problem (G0, c). If the original graph G has a hamiltonian cycle H , then the cost function c assigns to each edge of H a cost of 1, and so (G0, c) contains a tour of cost V . On the other hand, if G does not contain a hamiltonian cycle, then any tour of G0 must use some edge not in E.</p><p>But any tour that uses an edge not in E has a cost of at least</p><p>(p jV j C 1) C (jV j — 1)  D p jV j C jV j</p><p>&gt; p jV j .</p><p>C</p><p>j j C j j</p><p>j j</p><p>j j</p><p>Because edges not in G are so costly, there is a gap of at least p V between the cost of a tour that is a hamiltonian cycle in G (cost V ) and the cost of any other tour (cost at least p V   V ). Therefore, the cost of a tour that is not a hamiltonian cycle in G is at least a factor of p  1 greater than the cost of a tour that is a hamiltonian cycle in G.</p><p>j j</p><p>Now, suppose that we apply the approximation algorithm A to the traveling- salesman problem (G0, c). Because A is guaranteed to return a tour of cost no more than p times the cost of an optimal tour, if G contains a hamiltonian cycle, then A must return it. If G has no hamiltonian cycle, then A returns a tour of cost more than p V . Therefore, we can use A to solve the hamiltonian-cycle problem in polynomial time.</p><p>D</p><p>The proof of Theorem 35.3 serves as an example of a general technique for proving that we cannot approximate a problem very well. Suppose that given an NP-hard problem X , we can produce in polynomial time a minimization prob- lem Y such that “yes” instances of X correspond to instances of Y with value at most k (for some k), but that “no” instances of X correspond to instances of Y with value greater than pk. Then, we have shown that, unless P  NP, there is no polynomial-time p-approximation algorithm for problem Y .</p><p>Exercises</p><h3>35.2-1</h3><p>Suppose that a complete undirected graph G D (V, E) with at least 3 vertices has a cost function c that satisfies the triangle inequality. Prove that c(u, v) ≥ 0 for all u, v 2 V .</p><h3>35.2-2</h3><p>Show how in polynomial time we can transform one instance of the traveling- salesman problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why such a polynomial-time transformation does not contradict Theorem 35.3, as- suming that P ¤ NP.</p><h3>35.2-3</h3><p>Consider the following <i><b>closest-point heuristic </b></i>for building an approximate trav- eling-salesman tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex u that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest u is vertex v. Extend the cycle to include u by inserting u just after v. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour.</p><h3>35.2-4</h3><p>In the <i><b>bottleneck traveling-salesman problem</b></i>, we wish to find the hamiltonian cy- cle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial- time approximation algorithm with approximation ratio 3 for this problem. (<i>Hint: </i>Show recursively that we can visit all the nodes in a bottleneck spanning tree, as discussed in Problem 23-3, exactly once by taking a full walk of the tree and skip- ping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost that is at most the cost of the costliest edge in a bottleneck hamiltonian cycle.)</p><h3>35.2-5</h3><p>Suppose that the vertices for an instance of the traveling-salesman problem are points in the plane and that the cost c(u, v) is the euclidean distance between points u and v. Show that an optimal tour never crosses itself.</p></li></ol></li><li data-list-text="35.3"><h2>The set-covering problem</h2></li></ol></li></ol><p>The set-covering problem is an optimization problem that models many problems that require resources to be allocated. Its corresponding decision problem general- izes the NP-complete vertex-cover problem and is therefore also NP-hard. The ap- proximation algorithm developed to handle the vertex-cover problem doesn’t apply here, however, and so we need to try other approaches. We shall examine a simple greedy heuristic with a logarithmic approximation ratio. That is, as the size of the instance gets larger, the size of the approximate solution may grow, relative to the size of an optimal solution. Because the logarithm function grows rather slowly, however, this approximation algorithm may nonetheless give useful results.</p><p>j j</p><p>Now, consider the traveling-salesman problem (G0, c). If the original graph G has a hamiltonian cycle H , then the cost function c assigns to each edge of H a cost of l, and so (G0, c) contains a tour of cost V . On the other hand, if G does not contain a hamiltonian cycle, then any tour of G0 must use some edge not in E.</p><p>But any tour that uses an edge not in E has a cost of at least</p><p>(p jV j C l) C (jV j — l)  D p jV j C jV j</p><p>&gt; p jV j .</p><p>C</p><p>j j C j j</p><p>j j</p><p>j j</p><p>Because edges not in G are so costly, there is a gap of at least p V between the cost of a tour that is a hamiltonian cycle in G (cost V ) and the cost of any other tour (cost at least p V   V ). Therefore, the cost of a tour that is not a hamiltonian cycle in G is at least a factor of p  l greater than the cost of a tour that is a hamiltonian cycle in G.</p><p>j j</p><p>Now, suppose that we apply the approximation algorithm A to the traveling- salesman problem (G0, c). Because A is guaranteed to return a tour of cost no more than p times the cost of an optimal tour, if G contains a hamiltonian cycle, then A must return it. If G has no hamiltonian cycle, then A returns a tour of cost more than p V . Therefore, we can use A to solve the hamiltonian-cycle problem in polynomial time.</p><p>D</p><p>The proof of Theorem 35.3 serves as an example of a general technique for proving that we cannot approximate a problem very well. Suppose that given an NP-hard problem X , we can produce in polynomial time a minimization prob- lem Y such that “yes” instances of X correspond to instances of Y with value at most k (for some k), but that “no” instances of X correspond to instances of Y with value greater than pk. Then, we have shown that, unless P  NP, there is no polynomial-time p-approximation algorithm for problem Y .</p><p>Exercises</p><h3>35.2-1</h3><p>Suppose that a complete undirected graph G D (V, E) with at least 3 vertices has a cost function c that satisfies the triangle inequality. Prove that c(u, v) ≥ 0 for all u, v 2 V .</p><h3>35.2-2</h3><p>Show how in polynomial time we can transform one instance of the traveling- salesman problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why such a polynomial-time transformation does not contradict Theorem 35.3, as- suming that P ¤ NP.</p><h3>35.2-3</h3><p>Consider the following <i><b>closest-point heuristic </b></i>for building an approximate trav- eling-salesman tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex u that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest u is vertex v. Extend the cycle to include u by inserting u just after v. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour.</p><h3>35.2-4</h3><p>In the <i><b>bottleneck traveling-salesman problem</b></i>, we wish to find the hamiltonian cy- cle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial- time approximation algorithm with approximation ratio 3 for this problem. (<i>Hint: </i>Show recursively that we can visit all the nodes in a bottleneck spanning tree, as discussed in Problem 23-3, exactly once by taking a full walk of the tree and skip- ping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost that is at most the cost of the costliest edge in a bottleneck hamiltonian cycle.)</p><h3>35.2-5</h3><p>Suppose that the vertices for an instance of the traveling-salesman problem are points in the plane and that the cost c(u, v) is the euclidean distance between points u and v. Show that an optimal tour never crosses itself.</p><ol><ol><li data-list-text="35.3"><h2>The set-covering problem</h2><p>The set-covering problem is an optimization problem that models many problems that require resources to be allocated. Its corresponding decision problem general- izes the NP-complete vertex-cover problem and is therefore also NP-hard. The ap- proximation algorithm developed to handle the vertex-cover problem doesn’t apply here, however, and so we need to try other approaches. We shall examine a simple greedy heuristic with a logarithmic approximation ratio. That is, as the size of the instance gets larger, the size of the approximate solution may grow, relative to the size of an optimal solution. Because the logarithm function grows rather slowly, however, this approximation algorithm may nonetheless give useful results.</p><p>S1</p><p>S2</p><p>S6</p><p>S3</p><p>S4</p><p>S5</p><p>Figure 35.3 An instance (X, Ỹ ) of the set-covering problem, where X consists of the l2 black points and Ỹ D fS1, S2, S3, S4, S5, S6g. A minimum-size set cover is 9 D fS3, S4, S5g, with size 3. The greedy algorithm produces a cover of size 4 by selecting either the sets S1, S4, S5, and S3 or the sets S1, S4, S5, and S6, in order.</p><p>[</p><p>An instance (X, Ỹ ) of the <i><b>set-covering problem </b></i>consists of a finite set X and a family Ỹ of subsets of X , such that every element of X belongs to at least one subset in Ỹ :</p><p>X D  S . </p><p>S 2F</p><p>[</p><p>We say that a subset S 2 Ỹ <i><b>covers </b></i>its elements. The problem is to find a minimum- size subset 9 ⊆ Ỹ whose members cover all of X :</p><p>X D   S .                                   (35.8)</p><p>S 2C</p><p>j j</p><p>We say that any 9 satisfying equation (35.8) <i><b>covers </b></i>X . Figure 35.3 illustrates the set-covering problem. The size of 9 is the number of sets it contains, rather than the number of individual elements in these sets, since every subset 9 that covers X must contain all X individual elements. In Figure 35.3, the minimum set cover has size 3.</p><p>The set-covering problem abstracts many commonly arising combinatorial prob- lems. As a simple example, suppose that X represents a set of skills that are needed to solve a problem and that we have a given set of people available to work on the problem. We wish to form a committee, containing as few people as possible, such that for every requisite skill in X , at least one member of the committee has that skill. In the decision version of the set-covering problem, we ask whether a covering exists with size at most k, where k is an additional parameter specified in the problem instance. The decision version of the problem is NP-complete, as Exercise 35.3-2 asks you to show.</p><p>A greedy approximation algorithm</p><p>The greedy method works by picking, at each stage, the set S that covers the great- est number of remaining elements that are uncovered.</p><p>Greedy-Set-Cover (X, Ỹ )</p><p>D</p><ol><li data-list-text="1"><p>U  X</p><p>D ; </p></li><li data-list-text="2"><p>9</p><p>¤; </p></li><li data-list-text="3"><p>while U</p><p>2         j \  j</p></li><li data-list-text="4"><p>select an S  Ỹ that maximizes S  U</p><p>D —</p></li><li data-list-text="5"><p>U  U  S</p><p>D [ f g</p></li><li data-list-text="6"><p>9  9  S</p></li><li data-list-text="7"><p>return 9</p><p>In the example of Figure 35.3, GREEDY-SET-COVER adds to 9 , in order, the sets</p><p>S1, S4, and S5, followed by either S3 or S6.</p><p>The algorithm works as follows. The set U contains, at each stage, the set of remaining uncovered elements. The set 9 contains the cover being constructed. Line 4 is the greedy decision-making step, choosing a subset S that covers as many uncovered elements as possible (breaking ties arbitrarily). After S is selected, line 5 removes its elements from U , and line 6 places S into 9 . When the algorithm terminates, the set 9 contains a subfamily of Ỹ that covers X .</p><p>j j j  j                    j j j  j   j j j  j</p><p>j j j j</p><p>j j</p><p>j j</p><p>We can easily implement GREEDY-SET-COVER to run in time polynomial in X and Ỹ . Since the number of iterations of the loop on lines 3–6 is bounded from above by min( X , Ỹ ), and we can implement the loop body to run in time O( X  Ỹ ),a simple implementation runs in time O( X  Ỹ min( X , Ỹ )). Ex- ercise 35.3-3 asks for a linear-time algorithm.</p><p>Analysis</p><p>D P</p><p>We now show that the greedy algorithm returns a set cover that is not too much larger than an optimal set cover. For convenience, in this chapter we denote the d th</p><p>harmonic number Hd</p><p>d</p><p>i D1</p><p>l/i (see Section A.1) by H(d). As a boundary</p><p>condition, we define H(0) D 0.</p><h3>Theorem 35.4</h3><p>GREEDY-SET-COVER is a polynomial-time p(n)-approximation algorithm, where</p><p>p(n) D H(max fjSj W S 2 Ỹ g) . </p><p><i><b>Proof  </b></i>We have already shown that GREEDY-SET-COVER runs in polynomial time.</p><p>2</p><p>To show that GREEDY-SET-COVER is a p(n)-approximation algorithm, we as- sign a cost of l to each set selected by the algorithm, distribute this cost over the elements covered for the first time, and then use these costs to derive the de- sired relationship between the size of an optimal set cover 9 * and the size of the set cover 9 returned by the algorithm. Let Si denote the i th subset selected by GREEDY-SET-COVER; the algorithm incurs a cost of l when it adds Si to 9 . We spread this cost of selecting Si evenly among the elements covered for the first time by Si . Let cx denote the cost allocated to element x, for each x  X . Each element is assigned a cost only once, when it is covered for the first time. If x is covered for the first time by Si , then</p><p>l</p><p>i</p><p>cx D jS</p><p>— (S1</p><p>[ S2</p><p>.</p><p>[ · · · [ Si—1)j</p><p>X</p><p>Each step of the algorithm assigns l unit of cost, and so</p><p>j9 j D   cx .                                  (35.9)</p><p>x2X</p><p>Each element x 2 X is in at least one set in the optimal cover 9 *, and so we have</p><p>S 2Ch x2S</p><p>x2X</p><p>X X cx ≥ X cx .                             (35.10)</p><p>X X</p><p>Combining equation (35.9) and inequality (35.10), we have that</p><p>j9 j ≤     cx .                               (35.11)</p><p>S 2Ch x2S</p><p>X</p><p>The remainder of the proof rests on the following key inequality, which we shall prove shortly. For any set S belonging to the family Ỹ ,</p><p>cx ≤ H(jSj) .                                (35.12)</p><p>x2S</p><p>X</p><p>From inequalities (35.11) and (35.12), it follows that</p><p>j9 j ≤   H(jSj)</p><p>S 2Ch</p><p>≤ j9 *j · H(max fjSj W S 2 Ỹ g) , </p><p>thus proving the theorem.</p><p>All that remains is to prove inequality (35.12). Consider any set S 2 Ỹ and any</p><p>i D l, 2, . . . , j9 j, and let</p><p>ui D jS — (S1 [ S2 [ · · · [ Si )j</p><p>be the number of elements in S that remain uncovered after the algorithm has selected sets S1, S2,. . . , Si . We define u0 D jSj to be the number of elements</p><p>D</p><p>—                            D</p><p>of S, which are all initially uncovered. Let k be the least index such that uk 0, so that every element in S is covered by at least one of the sets S1, S2,. . . , Sk and some element in S is uncovered by S1 [ S2 [ · · · [ Sk—1. Then, ui—1 ≥ ui , and ui—1 ui elements of S are covered for the first time by Si , for i  l, 2, . . . , k. Thus,</p><p>k</p><p>x</p><p>i—1</p><p>i</p><p>jSi</p><p>— (S1</p><p>[ S2</p><p>[ · · · [ Si—1</p><p>)j</p><p>X c  D  X(u  — u ) · <u>              l                  </u> .</p><p>x2S</p><p>i D1</p><p>Observe that</p><p>jSi — (S1 [ S2 [ · · · [ Si—1)j ≥ jS — (S1 [ S2 [ · · · [ Si—1)j</p><p>D ui—1 ,</p><p>because the greedy choice of Si guarantees that S cannot cover more new ele- ments than Si does (otherwise, the algorithm would have chosen S instead of Si ). Consequently, we obtain</p><p>k</p><p>x</p><p>i—1</p><p>i</p><p>ui—1</p><p>X c  ≤ X(u  — u ) · <u>  l </u> .</p><p>x2S</p><p>i D1</p><p>We now bound this quantity as follows:</p><p>k</p><p>X c  ≤ X(u  — u ) · <u> l  </u></p><p>i</p><p>x</p><p>x2S     i D1</p><p>k</p><p>X</p><p>k</p><p>i D1 j Dui C1 ui—1</p><p>X</p><p>D X</p><p>i—1</p><p>ui—1</p><p>ui—1</p><p> l  </p><p>X</p><p>ui—1  l</p><p>≤      j</p><p>(because j ≤ ui—1)</p><p> X</p><p>!</p><p>i D1 j Dui C1</p><p>k</p><p>D Xi D1</p><p>X</p><p>k</p><p>ui —1  l</p><p>j —</p><p>j D1</p><p>ui  l j</p><p>X</p><p>j D1</p><p>D   (H(ui—1) — H(ui ))</p><p>i D1</p><p>D H(u0) — H(uk)     (because the sum telescopes)</p><p>D H(u0) — H(0)</p><p>D H(u0)          (because H(0) D 0)</p><p>D H(jSj) , </p><p>which completes the proof of inequality (35.12).</p><h3>Corollary 35.5</h3><p>GREEDY-SET-COVER is a polynomial-time (ln jX j C l)-approximation algorithm.</p><h3>Proof Use inequality (A.14) and Theorem 35.4.</h3><p>D</p><p>fj j W 2 g</p><p>In some applications, max  S  S  Ỹ  is a small constant, and so the solution returned by GREEDY-SET-COVER is at most a small constant times larger than optimal. One such application occurs when this heuristic finds an approximate vertex cover for a graph whose vertices have degree at most 3. In this case, the solution found by GREEDY-SET-COVER is not more than H(3)   ll/6 times as large as an optimal solution, a performance guarantee that is slightly better than that of APPROX-VERTEX-COVER.</p><p>Exercises</p><h3>35.3-1</h3><p>g</p><p>f</p><p>Consider each of the following words as a set of letters:  arid, dash, drain, heard, lost, nose, shun, slate, snare, thread . Show which set cover GREEDY-SET-COVER produces when we break ties in favor of the word that ap- pears first in the dictionary.</p><h3>35.3-2</h3><p>Show that the decision version of the set-covering problem is NP-complete by reducing it from the vertex-cover problem.</p><h3>35.3-3</h3><p>!P  "</p><p>Show how to implement GREEDY-SET-COVER in such a way that it runs in time</p><p>O  S 2F jSj .</p><h3>35.3-4</h3><p>Show that the following weaker form of Theorem 35.4 is trivially true:</p><p>j9 j ≤ j9 *j max fjSj W S 2 Ỹ g .</p><h3>35.3-5</h3><p>GREEDY-SET-COVER can return a number of different solutions, depending on how we break ties in line 4. Give a procedure BAD-SET-COVER-INSTANCE (n) that returns an n-element instance of the set-covering problem for which, depend- ing on how we break ties in line 4, GREEDY-SET-COVER can return a number of different solutions that is exponential in n.</p></li></ol></li><li data-list-text="35.4"><h2>Randomization and linear programming</h2><p>In this section, we study two useful techniques for designing approximation algo- rithms: randomization and linear programming. We shall give a simple randomized algorithm for an optimization version of 3-CNF satisfiability, and then we shall use linear programming to help design an approximation algorithm for a weighted ver- sion of the vertex-cover problem. This section only scratches the surface of these two powerful techniques. The chapter notes give references for further study of these areas.</p><p>A randomized approximation algorithm for MAX-3-CNF satisfiability</p><p>Just as some randomized algorithms compute exact solutions, some randomized algorithms compute approximate solutions. We say that a randomized algorithm for a problem has an <i><b>approximation ratio </b></i>of p(n) if, for any input of size n, the <i>expected </i>cost S of the solution produced by the randomized algorithm is within a factor of p(n) of the cost S * of an optimal solution:</p><p>S *</p><p>S</p><p>max ! <u> S </u>, S * " ≤ p(n) .                          (35.13)</p><p>We call a randomized algorithm that achieves an approximation ratio of p(n) a <i><b>randomized </b></i>!(n)<i><b>-approximation algorithm. </b></i>In other words, a randomized ap- proximation algorithm is like a deterministic approximation algorithm, except that the approximation ratio is for an expected cost.</p><p>A particular instance of 3-CNF satisfiability, as defined in Section 34.4, may or may not be satisfiable. In order to be satisfiable, there must exist an assignment of the variables so that every clause evaluates to l. If an instance is not satisfiable, we may want to compute how “close” to satisfiable it is, that is, we may wish to find an assignment of the variables that satisfies as many clauses as possible. We call the resulting maximization problem <i><b>MAX-3-CNF satisfiability</b></i>. The input to MAX-3- CNF satisfiability is the same as for 3-CNF satisfiability, and the goal is to return an assignment of the variables that maximizes the number of clauses evaluating to l. We now show that randomly setting each variable to l with probability l/2 and to 0 with probability l/2 yields a randomized 8/7-approximation algorithm. According to the definition of 3-CNF satisfiability from Section 34.4, we require each clause to consist of exactly three distinct literals. We further assume that no clause contains both a variable and its negation. (Exercise 35.4-1 asks you to remove this last assumption.)</p><h3>Theorem 35.6</h3><p>Given an instance of MAX-3-CNF satisfiability with n variables x1, x2,. . . , xn and m clauses, the randomized algorithm that independently sets each vari- able to l with probability l/2 and to 0 with probability l/2 is a randomized 8/7-approximation algorithm.</p><p>D</p><p><i><b>Proof  </b></i>Suppose that we have independently set each variable to l with probabil- ity l/2 and to 0 with probability l/2. For i   l, 2, . . . , m, we define the indicator random variable</p><p>Yi D I fclause i is satisfiedg ,</p><p>D</p><p>so that Yi l as long as we have set at least one of the literals in the i th clause to l. Since no literal appears more than once in the same clause, and since we have assumed that no variable and its negation appear in the same clause, the settings of the three literals in each clause are independent. A clause is not satisfied only if all three of its literals are set to 0, and so Pr fclause i is not satisfiedg D (l/2)3 D l/8.</p><p>Thus, we have Pr fclause i is satisfiedg D l — l/8 D 7/8, and by Lemma 5.1,</p><p>we have E [Yi ] D 7/8. Let Y be the number of satisfied clauses overall, so that</p><p>"X</p><p>Y D Y1 C Y2 C · · · C Ym. Then, we have</p><p>E [Y ] D E</p><p>m</p><p>i D1</p><p>Yi #</p><p>X</p><p>m</p><p>D   E [Yi ]  (by linearity of expectation)</p><p>X</p><p>i D1 m</p><p>D  7/8</p><p>i D1</p><p>D 7m/8 .</p><p>Clearly, m is an upper bound on the number of satisfied clauses, and hence the approximation ratio is at most m/(7m/8) D 8/7.</p><p>Approximating weighted vertex cover using linear programming</p><p>In the <i><b>minimum-weight vertex-cover problem</b></i>, we are given an undirected graph</p><p>G D (V, E) in which each vertex v 2 V has an associated positive weight r(v).</p><p>P</p><p>For any vertex cover V 0 ⊆ V , we define the weight of the vertex cover r(V 0) D</p><p>2</p><p>v V 0 r(v). The goal is to find a vertex cover of minimum weight.</p><p>We cannot apply the algorithm used for unweighted vertex cover, nor can we use</p><p>a random solution; both methods may return solutions that are far from optimal. We shall, however, compute a lower bound on the weight of the minimum-weight</p><p>vertex cover, by using a linear program. We shall then “round” this solution and use it to obtain a vertex cover.</p><p>X</p><p>C  ≥</p><p>D</p><p>2</p><p>2</p><p>Suppose that we associate a variable x(v) with each vertex v  V , and let us require that x(v) equals either 0 or l for each v  V . We put v into the vertex cover if and only if x(v)  l. Then, we can write the constraint that for any edge (u, v), at least one of u and v must be in the vertex cover as x(u)  x(v)  l. This view gives rise to the following 0-1 integer program for finding a minimum-weight vertex cover:</p><p>minimize    r(v) x(v)                           (35.14)</p><p>v2<i>V</i></p><p>subject to</p><p>x(u) C x(v)  ≥ l    for each (u, v) 2 E       (35.15)</p><p>x(v)  2 f0, lg for each v 2 V .         (35.16)</p><p>X</p><p>≤  ≤</p><p>2 f g</p><p>In the special case in which all the weights r(v) are equal to l, this formu- lation is the optimization version of the NP-hard vertex-cover problem. Sup- pose, however, that we remove the constraint that x(v)  0, l and replace it by 0  x(v)  l. We then obtain the following linear program, which is known as the linear-programming relaxation:</p><p>minimize    r(v) x(v)                           (35.17)</p><p>v2<i>V</i></p><p>subject to</p><p>x(u) C x(v)  ≥ l  for each (u, v) 2 E          (35.18)</p><p>x(v)  ≤ l  for each v 2 V            (35.19)</p><p>x(v)  ≥  0  for each v 2 V .            (35.20)</p><p>Any feasible solution to the 0-1 integer program in lines (35.14)–(35.16) is also a feasible solution to the linear program in lines (35.17)–(35.20). Therefore, the value of an optimal solution to the linear program gives a lower bound on the value of an optimal solution to the 0-1 integer program, and hence a lower bound on the optimal weight in the minimum-weight vertex-cover problem.</p><p>The following procedure uses the solution to the linear-programming relaxation to construct an approximate solution to the minimum-weight vertex-cover problem:</p><p>Approx-Min-Weight-VC (G, r)</p><p>D ; </p><ol><li data-list-text="1"><p>S</p><p>N</p></li><li data-list-text="2"><p>compute x, an optimal solution to the linear program in lines (35.17)–(35.20)</p><p>2</p></li><li data-list-text="3"><p>for each v  V</p><p>N  ≥</p></li><li data-list-text="4"><p>if x(v)  l/2</p><p>D [ f g</p></li><li data-list-text="5"><p>S  S  v</p></li><li data-list-text="6"><p>return S</p><p>N  ≥</p><p>N     ≤ N  ≤</p><p>The APPROX-MIN-WEIGHT-VC procedure works as follows. Line 1 initial- izes the vertex cover to be empty.  Line 2 formulates the linear program in lines (35.17)–(35.20) and then solves this linear program. An optimal solution gives each vertex v an associated value x(v), where 0   x(v)   l. We use this value to guide the choice of which vertices to add to the vertex cover S in lines 3–5. If x(v)  l/2, we add v to S ; otherwise we do not. In effect, we are “rounding” each fractional variable in the solution to the linear program to 0 or l in order to obtain a solution to the 0-1 integer program in lines (35.14)–(35.16). Finally, line 6 returns the vertex cover S .</p><h3>Theorem 35.7</h3><p>Algorithm APPROX-MIN-WEIGHT-VC is a polynomial-time 2-approximation al- gorithm for the minimum-weight vertex-cover problem.</p><p><i><b>Proof  </b></i>Because there is a polynomial-time algorithm to solve the linear program in line 2, and because the <b>for </b>loop of lines 3–5 runs in polynomial time, APPROX- MIN-WEIGHT-VC is a polynomial-time algorithm.</p><p>Now we show that APPROX-MIN-WEIGHT-VC is a 2-approximation algo- rithm. Let S * be an optimal solution to the minimum-weight vertex-cover prob- lem, and let ´* be the value of an optimal solution to the linear program in lines (35.17)–(35.20). Since an optimal vertex cover is a feasible solution to the linear program, ´* must be a lower bound on r(S *), that is,</p><p>´* ≤ r(S *) .                                  (35.21)</p><p>C   ≥                N    N</p><p>2</p><p>≤</p><p>N</p><p>Next, we claim that by rounding the fractional values of the variables x(v), we produce a set S that is a vertex cover and satisfies r(S)  2´*. To see that S is a vertex cover, consider any edge (u, v)  E. By constraint (35.18), we know that x(u)  x(v)  l, which implies that at least one of x(u) and x(v) is at least l/2. Therefore, at least one of u and v is included in the vertex cover, and so every edge is covered.</p><p>Now, we consider the weight of the cover. We have</p><p>X</p><p>´* D  r(v) xN (v)</p><p>v2<i>V</i></p><p>v2V WzN (v)≥1/2</p><p>≥  X r(v) xN (v)</p><p>X</p><p>l</p><p>≥     r(v) · 2</p><p>v2V WzN (v)≥1/2</p><p>2</p><p>D X r(v) · <u>l</u></p><p>v2<i>S</i></p><p>2</p><p>D <u>l</u> X r(v)</p><p>v2<i>S</i></p><p>l</p><p>D</p><p>r(S) .                               (35.22)</p><p>2</p><p>Combining inequalities (35.21) and (35.22) gives</p><p>r(S) ≤ 2´* ≤ 2r(S *) , </p><p>and hence APPROX-MIN-WEIGHT-VC is a 2-approximation algorithm.</p><p>Exercises</p><h3>35.4-1</h3><p>Show that even if we allow a clause to contain both a variable and its negation, ran- domly setting each variable to l with probability l/2 and to 0 with probability l/2 still yields a randomized 8/7-approximation algorithm.</p><h3>35.4-2</h3><p>The <i><b>MAX-CNF satisfiability problem </b></i>is like the MAX-3-CNF satisfiability prob- lem, except that it does not restrict each clause to have exactly 3 literals. Give a randomized 2-approximation algorithm for the MAX-CNF satisfiability problem.</p><h3>35.4-3</h3><p>—</p><p>D</p><p>—</p><p>In the MAX-CUT problem, we are given an unweighted undirected graph G (V, E). We define a cut (S, V  S) as in Chapter 23 and the <i><b>weight </b></i>of a cut as the number of edges crossing the cut. The goal is to find a cut of maximum weight. Suppose that for each vertex v, we randomly and independently place v in S with probability l/2 and in V  S with probability l/2. Show that this algorithm is a randomized 2-approximation algorithm.</p><h3>35.4-4</h3><p>Show that the constraints in line (35.19) are redundant in the sense that if we re- move them from the linear program in lines (35.17)–(35.20), any optimal solution to the resulting linear program must satisfy x(v) ≤ 1 for each v 2 V .</p></li></ol></li><li data-list-text="35.5"><h2>The subset-sum problem</h2><p>f      g</p><p>Recall from Section 34.5.5 that an instance of the subset-sum problem is a pair (S, t), where S is a set x1, x2,. . . , xn of positive integers and t is a posi- tive integer. This decision problem asks whether there exists a subset of S that adds up exactly to the target value t . As we saw in Section 34.5.5, this problem is NP-complete.</p><p>f      g</p><p>The optimization problem associated with this decision problem arises in prac- tical applications.  In the optimization problem, we wish to find a subset of x1, x2,. . . , xn whose sum is as large as possible but not larger than t . For ex- ample, we may have a truck that can carry no more than t pounds, and n different boxes to ship, the i th of which weighs xi pounds. We wish to fill the truck with as heavy a load as possible without exceeding the given weight limit.</p><p>In this section, we present an exponential-time algorithm that computes the op- timal value for this optimization problem, and then we show how to modify the algorithm so that it becomes a fully polynomial-time approximation scheme. (Re- call that a fully polynomial-time approximation scheme has a running time that is polynomial in 1/‹ as well as in the size of the input.)</p><p>An exponential-time exact algorithm</p><p>Suppose that we computed, for each subset S0 of S, the sum of the elements in S0, and then we selected, among the subsets whose sum does not exceed t , the one whose sum was closest to t . Clearly this algorithm would return the op- timal solution, but it could take exponential time. To implement this algorithm, we could use an iterative procedure that, in iteration i , computes the sums of</p><p>f      g</p><p>all subsets of fx1, x2,. . . , xi g, using as a starting point the sums of all subsets of x1, x2,. . . , xi—1 . In doing so, we would realize that once a particular subset S0 had a sum exceeding t , there would be no reason to maintain it, since no super-</p><p>set of S0 could be the optimal solution. We now give an implementation of this strategy.</p><p>D f     g</p><p>The procedure EXACT-SUBSET-SUM takes an input set S   x1, x2,. . . , xn</p><p>and a target value t ; we’ll see its pseudocode in a moment. This procedure it-</p><p>f    g</p><p>eratively computes Li , the list of sums of all subsets of x1,. . . , xi that do not exceed t , and then it returns the maximum value in Ln.</p><p>D h     i    C  D h     i</p><p>C</p><p>If L is a list of positive integers and x is another positive integer, then we let L  x denote the list of integers derived from L by increasing each element of L by x. For example, if L  1, 2, 3, 5, 9 , then L  2  3, 4, 5, 7, 11 . We also use this notation for sets, so that</p><p>S C x D fs C x W s 2 Sg .</p><p>L  .</p><p>We also use an auxiliary procedure MERGE-LISTS (L, L0), which returns the sorted list that is the merge of its two sorted input lists L and L0 with duplicate values removed. Like the MERGE procedure we used in merge sort (Section 2.3.1), MERGE-LISTS runs in time O(jLj C jL0j). We omit the pseudocode for MERGE-</p><p>ISTS</p><p>Exact-Subset-Sum (S, t)</p><ol><li data-list-text="1"><p>n D jSj</p></li><li data-list-text="2"><p>L0 D h0i</p></li><li data-list-text="3"><p>for i D 1 to n</p><p>D             C</p></li><li data-list-text="4"><p>Li MERGE-LISTS (Li—1, Li—1 xi )</p></li><li data-list-text="5"><p>remove from Li every element that is greater than t</p></li><li data-list-text="6"><p>return the largest element in Ln</p></li></ol></li></ol></ol><p>To see how EXACT-SUBSET-SUM works, let Pi denote the set of all values obtained by selecting a (possibly empty) subset of fx1, x2,. . . , xi g and summing its members. For example, if S D f1, 4, 5g, then</p><p>P1 D f0, 1g ,</p><p>P2 D f0, 1, 4, 5g ,</p><p>P3 D f0, 1, 4, 5, 6, 9, 10g .</p><p>Given the identity</p><p>Pi D Pi—1 [ (Pi—1 C xi) ,                          (35.23)</p><p>we can prove by induction on i (see Exercise 35.5-1) that the list Li is a sorted list containing every element of Pi whose value is not more than t . Since the length of Li can be as much as 2i , EXACT-SUBSET-SUM is an exponential-time algorithm in general, although it is a polynomial-time algorithm in the special cases in which t is polynomial in jSj or all the numbers in S are bounded by a polynomial in jSj.</p><p>A fully polynomial-time approximation scheme</p><p>We can derive a fully polynomial-time approximation scheme for the subset-sum problem by “trimming” each list Li after it is created. The idea behind trimming is</p><p>that if two values in L are close to each other, then since we want just an approxi- mate solution, we do not need to maintain both of them explicitly. More precisely, we use a trimming parameter ı such that 0 &lt; ı &lt; 1. When we <i><b>trim </b></i>a list L by ı, we remove as many elements from L as possible, in such a way that if L0 is the result of trimming L, then for every element y that was removed from L, there is an element ´ still in L0 that approximates y, that is,</p><p>y</p><p>1 C ı ≤ ´ ≤ y .                                (35.24)</p><p>We can think of such a ´ as “representing” y in the new list L0. Each removed element y is represented by a remaining element ´ satisfying inequality (35.24). For example, if ı D 0.1 and</p><p>L D h10, 11, 12, 15, 20, 21, 22, 23, 24, 29i ,</p><p>then we can trim L to obtain</p><p>L0 D h10, 12, 15, 20, 23, 29i ,</p><p>where the deleted value 11 is represented by 10, the deleted values 21 and 22 are represented by 20, and the deleted value 24 is represented by 23. Because every element of the trimmed version of the list is also an element of the original version of the list, trimming can dramatically decrease the number of elements kept while keeping a close (and slightly smaller) representative value in the list for each deleted element.</p><p>D h     i</p><p>The following procedure trims list L   y1, y2,. . . , ym in time ‚(m), given L and ı, and assuming that L is sorted into monotonically increasing order. The output of the procedure is a trimmed, sorted list.</p><p>Trim(L, ı)</p><ol><li data-list-text="1"><p>let m be the length of L</p><p>D h i</p></li><li data-list-text="2"><p>L0 y1</p><p>D</p></li><li data-list-text="3"><p>last  y1</p><p>D</p></li><li data-list-text="4"><p>for i  2 to m</p><p>·  C      ≥</p></li><li data-list-text="5"><p>if yi &gt; last (1  ı)   // yi last because L is sorted</p></li><li data-list-text="6"><p>append yi onto the end of L0</p><p>D</p></li><li data-list-text="7"><p>last  yi</p></li><li data-list-text="8"><p>return L0</p><p>The procedure scans the elements of L in monotonically increasing order. A num- ber is appended onto the returned list L0 only if it is the first element of L or if it cannot be represented by the most recent number placed into L0.</p><p>D f     g</p><p>Given the procedure TRIM, we can construct our approximation scheme as fol- lows. This procedure takes as input a set S   x1, x2,. . . , xn of n integers (in arbitrary order), a target integer t , and an “approximation parameter” ‹, where</p><p>0 &lt; ‹ &lt; 1 .                                   (35.25)</p><p>It returns a value ´ whose value is within a 1 C ‹ factor of the optimal solution. APPROX-SUBSET-SUM (S, t, ‹)</p><ol><li data-list-text="1"><p>n D jSj</p></li><li data-list-text="2"><p>L0 D h0i</p></li><li data-list-text="3"><p>for i D 1 to n</p><p>D             C</p></li><li data-list-text="4"><p>Li MERGE-LISTS (Li—1, Li—1 xi )</p><p>D</p></li><li data-list-text="5"><p>Li TRIM(Li, ‹/2n)</p></li><li data-list-text="6"><p>remove from Li every element that is greater than t</p></li><li data-list-text="7"><p>let ´* be the largest value in Ln</p></li><li data-list-text="8"><p>return ´*</p></li></ol></li></ol><p>Line 2 initializes the list L0 to be the list containing just the element 0. The <b>for </b>loop in lines 3–6 computes Li as a sorted list containing a suitably trimmed ver- sion of the set Pi , with all elements larger than t removed. Since we create Li from Li—1, we must ensure that the repeated trimming doesn’t introduce too much compounded inaccuracy. In a moment, we shall see that APPROX-SUBSET-SUM returns a correct approximation if one exists.</p><p>As an example, suppose we have the instance</p><p>S D h104, 102, 201, 101i</p><p>D      D                   D</p><p>with t  308 and ‹  0.40. The trimming parameter ı is ‹/8  0.05. APPROX- SUBSET-SUM computes the following values on the indicated lines:</p><table cellspacing="0"><tr><td><p>line 2:</p></td><td><p>L0</p></td><td><p>D</p></td><td><p>h0i ,</p></td></tr><tr><td><p>line 4:</p></td><td><p>L1</p></td><td><p>D</p></td><td><p>h0, 104i ,</p></td></tr><tr><td><p>line 5:</p></td><td><p>L1</p></td><td><p>D</p></td><td><p>h0, 104i ,</p></td></tr><tr><td><p>line 6:</p></td><td><p>L1</p></td><td><p>D</p></td><td><p>h0, 104i ,</p></td></tr><tr><td><p>line 4:</p></td><td><p>L2</p></td><td><p>D</p></td><td><p>h0, 102, 104, 206i ,</p></td></tr><tr><td><p>line 5:</p></td><td><p>L2</p></td><td><p>D</p></td><td><p>h0, 102, 206i ,</p></td></tr><tr><td><p>line 6:</p></td><td><p>L2</p></td><td><p>D</p></td><td><p>h0, 102, 206i ,</p></td></tr><tr><td><p>line 4:</p></td><td><p>L3</p></td><td><p>D</p></td><td><p>h0, 102, 201, 206, 303, 407i ,</p></td></tr><tr><td><p>line 5:</p></td><td><p>L3</p></td><td><p>D</p></td><td><p>h0, 102, 201, 303, 407i ,</p></td></tr><tr><td><p>line 6:</p></td><td><p>L3</p></td><td><p>D</p></td><td><p>h0, 102, 201, 303i ,</p></td></tr><tr><td><p>line 4:</p></td><td><p>L4</p></td><td><p>D</p></td><td><p>h0, 101, 102, 201, 203, 302, 303, 404i ,</p></td></tr><tr><td><p>line 5:</p></td><td><p>L4</p></td><td><p>D</p></td><td><p>h0, 101, 201, 302, 404i ,</p></td></tr><tr><td><p>line 6:</p></td><td><p>L4</p></td><td><p>D</p></td><td><p>h0, 101, 201, 302i .</p></td></tr></table><p>The algorithm returns ´* D 302 as its answer, which is well within ‹ D 40% of the optimal answer 307 D 104 C 102 C 101; in fact, it is within 2%.</p><h3>Theorem 35.8</h3><p>APPROX-SUBSET-SUM is a fully polynomial-time approximation scheme for the subset-sum problem.</p><p>≤ C</p><p>≤</p><p>2</p><p>Proof The operations of trimming Li in line 5 and removing from Li every ele- ment that is greater than t maintain the property that every element of Li is also a member of Pi . Therefore, the value ´* returned in line 8 is indeed the sum of some subset of S. Let y* Pn denote an optimal solution to the subset-sum problem. Then, from line 6, we know that ´* y*. By inequality (35.1), we need to show that y*/´* 1  ‹. We must also show that the running time of this algorithm is polynomial in both 1/‹ and the size of the input.</p><p>As Exercise 35.5-2 asks you to show, for every element y in Pi that is at most t , there exists an element ´ 2 Li such that</p><p>y</p><p>(1 C ‹/2n)i ≤ ´ ≤ y .                            (35.26)</p><p>Inequality (35.26) must hold for y* 2 Pn, and therefore there exists an element</p><p>´ 2 Ln such that</p><p>y*</p><p>(1 C ‹/2n)n</p><p>and thus</p><p>≤ ´ ≤ y* ,</p><p>´ ≤</p><p>1 C 2n</p><p>.                             (35.27)</p><p>y* !  <u> ‹ </u> "n</p><p>2</p><p>Since there exists an element ´  Ln fulfilling inequality (35.27), the inequality must hold for ´*, which is the largest value in Ln; that is,</p><p>´* ≤</p><p>1 C 2n</p><p>.                             (35.28)</p><p>y* !  <u> ‹ </u> "n</p><p>Now, we show that y*/´* ≤ 1 C ‹. We do so by showing that (1 C ‹/2n)n ≤</p><p>C                    C    D</p><p>1  ‹. By equation (3.14), we have limn-∞(1  ‹/2n)n e!/2. Exercise 35.5-3</p><p>asks you to show that</p><p>!</p><p>"</p><p><u> d </u> <u> ‹ </u> n</p><p>dn 1 C 2n</p><p>&gt; 0 .                             (35.29)</p><p>C</p><p>Therefore, the function (1  ‹/2n)n increases with n as it approaches its limit of e!/2, and we have</p><p>!   ‹  "n    !/2</p><p>1 C 2n   ≤ e</p><p>2</p><p>≤ 1 C ‹/2 C (‹/2)  (by inequality (3.13))</p><p>≤ 1 C ‹        (by inequality (35.25)) .      (35.30)</p><p>Combining inequalities (35.28) and (35.30) completes the analysis of the approxi- mation ratio.</p><p>differ by a factor of at least 1 C #‹/2n. Each˘list, therefore, contains the value 0,</p><p>To show that APPROX-SUBSET-SUM is a fully polynomial-time approximation scheme, we derive a bound on the length of Li . After trimming, successive ele- ments ´ and ´0 of Li must have the relationship ´0/´ &gt; 1C‹/2n. That is, they must</p><p>possibly the value 1, and up to elements in each list Li is at most</p><p>ln t</p><p>log1C!/2n t</p><p>additional values. The number of</p><p>log1C!/2n t C 2  D ln(1 C ‹/2n) C 2</p><p>‹</p><p>≤  2n(1 <u>C </u>‹/2n) ln t C 2  (by inequality (3.17))</p><p>3n ln t</p><p>&lt;   ‹  C 2       (by inequality (35.25)) .</p><p>This bound is polynomial in the size of the input—which is the number of bits lg t needed to represent t plus the number of bits needed to represent the set S, which is in turn polynomial in n—and in 1/‹. Since the running time of APPROX-SUBSET- SUM is polynomial in the lengths of the Li , we conclude that APPROX-SUBSET- SUM is a fully polynomial-time approximation scheme.</p><p>Exercises</p><h3>35.5-1</h3><p>Prove equation (35.23). Then show that after executing line 5 of EXACT-SUBSET- SUM, Li is a sorted list containing every element of Pi whose value is not more than t .</p><h3>35.5-2</h3><p>Using induction on i , prove inequality (35.26).</p><h3>35.5-3</h3><p>Prove inequality (35.29).</p><h3>35.5-4</h3><p>How would you modify the approximation scheme presented in this section to find a good approximation to the smallest value not less than t that is a sum of some subset of the given input list?</p><h3>35.5-5</h3><p>Modify the APPROX-SUBSET-SUM procedure to also return the subset of S that sums to the value ´*.</p><h2>Problems</h2><ol><ol><li data-list-text="35-1"><h3>Bin packing</h3><p>Suppose that we are given a set of n objects, where the size si of the i th object satisfies 0 &lt; si &lt; 1. We wish to pack all the objects into the minimum number of unit-size bins. Each bin can hold any subset of the objects whose total size does not exceed 1.</p><ol><li data-list-text="a."><p>Prove that the problem of determining the minimum number of bins required is NP-hard. (<i>Hint: </i>Reduce from the subset-sum problem.)</p><p>D P</p><p>The <i><b>first-fit </b></i>heuristic takes each object in turn and places it into the first bin that</p><p>can accommodate it. Let S   n</p><p>i D1</p><p>si .</p></li><li data-list-text="b."><p>Argue that the optimal number of bins required is at least dSe.</p></li><li data-list-text="c."><p>Argue that the first-fit heuristic leaves at most one bin less than half full.</p></li><li data-list-text="d."><p>Prove that the number of bins used by the first-fit heuristic is never more than d2S e.</p></li><li data-list-text="e."><p>Prove an approximation ratio of 2 for the first-fit heuristic.</p></li><li data-list-text="f."><p>Give an efficient implementation of the first-fit heuristic, and analyze its running time.</p></li></ol></li><li data-list-text="35-2"><h3>Approximating the size of a maximum clique</h3><p>D                   ≥</p><p>Let G  (V, E) be an undirected graph. For any k  1, define G(k) to be the undi- rected graph (V (k),E(k)), where V (k) is the set of all ordered k-tuples of vertices from V and E(k) is defined so that (v1, v2,. . . , vk) is adjacent to (r1, r2,. . . , rk) if and only if for i D 1, 2, . . . , k, either vertex vi is adjacent to ri in G, or else</p><p>vi D ri .</p><ol><li data-list-text="a."><p>Prove that the size of the maximum clique in G(k) is equal to the kth power of the size of the maximum clique in G.</p></li><li data-list-text="b."><p>Argue that if there is an approximation algorithm that has a constant approxi- mation ratio for finding a maximum-size clique, then there is a polynomial-time approximation scheme for the problem.</p></li></ol></li><li data-list-text="35-3"><h3>Weighted set-covering problem</h3><p>P 2</p><p>D</p><p>i</p><p>Suppose that we generalize the set-covering problem so that each set Si in the family Ỹ has an associated weight ri and the weight of a cover 9 is  S C ri . We wish to determine a minimum-weight cover. (Section 35.3 handles the case in which ri 1 for all i .)</p><p>Show how to generalize the greedy set-covering heuristic in a natural manner to provide an approximate solution for any instance of the weighted set-covering problem. Show that your heuristic has an approximation ratio of H(d), where d is the maximum size of any set Si .</p></li><li data-list-text="35-4"><h3>Maximum matching</h3><p>Recall that for an undirected graph G, a matching is a set of edges such that no two edges in the set are incident on the same vertex. In Section 26.3, we saw how to find a maximum matching in a bipartite graph. In this problem, we will look at matchings in undirected graphs in general (i.e., the graphs are not required to be bipartite).</p><ol><li data-list-text="a."><p>A <i><b>maximal matching </b></i>is a matching that is not a proper subset of any other matching. Show that a maximal matching need not be a maximum matching by exhibiting an undirected graph G and a maximal matching M in G that is not a maximum matching. (<i>Hint: </i>You can find such a graph with only four vertices.)</p><p>D</p></li><li data-list-text="b."><p>Consider an undirected graph G   (V, E). Give an O(E)-time greedy algo- rithm to find a maximal matching in G.</p><p>In this problem, we shall concentrate on a polynomial-time approximation algo- rithm for maximum matching. Whereas the fastest known algorithm for maximum matching takes superlinear (but polynomial) time, the approximation algorithm here will run in linear time. You will show that the linear-time greedy algorithm for maximal matching in part (b) is a 2-approximation algorithm for maximum matching.</p></li><li data-list-text="c."><p>Show that the size of a maximum matching in G is a lower bound on the size of any vertex cover for G.</p></li><li data-list-text="d."><p>Consider a maximal matching M in G D (V, E). Let</p><p>T D fv 2 V W some edge in M is incident on vg .</p><p>What can you say about the subgraph of G induced by the vertices of G that are not in T ?</p></li><li data-list-text="e."><p>Conclude from part (d) that 2 jM j is the size of a vertex cover for G.</p></li><li data-list-text="f."><p>Using parts (c) and (e), prove that the greedy algorithm in part (b) is a 2-approx- imation algorithm for maximum matching.</p></li></ol></li><li data-list-text="35-5"><h3>Parallel machine scheduling</h3><p>D</p><p>In the <i><b>parallel-machine-scheduling problem</b></i>, we are given n jobs, J1, J2,. . . , Jn, where each job Jk has an associated nonnegative processing time of pk. We are also given m identical machines, M1, M2,. . . , Mm. Any job can run on any ma- chine. A <i><b>schedule </b></i>specifies, for each job Jk, the machine on which it runs and the time period during which it runs. Each job Jk must run on some machine Mi for pk consecutive time units, and during that time period no other job may run on Mi . Let Sk denote the <i><b>completion time </b></i>of job Jk, that is, the time at which job Jk completes processing. Given a schedule, we define Smax  max1≤j≤n Sj to be the <i><b>makespan </b></i>of the schedule. The goal is to find a schedule whose makespan is minimum.</p><p>D   D      D</p><p>D   D</p><p>D   D      D</p><p>D  D</p><p>D    D    D     D</p><p>For example, suppose that we have two machines M1 and M2 and that we have four jobs J1, J2, J3, J4, with p1 2, p2 12, p3 4, and p4 5. Then one possible schedule runs, on machine M1, job J1 followed by job J2, and on ma- chine M2, it runs job J4 followed by job J3. For this schedule, S1 2, S2 14, S3 9, S4 5, and Smax 14. An optimal schedule runs J2 on machine M1, and it runs jobs J1, J3, and J4 on machine M2. For this schedule, S1 2, S2 12, S3 6, S4 11, and Smax 12.</p><p>Given a parallel-machine-scheduling problem, we let Sm*ax denote the makespan of an optimal schedule.</p><ol><li data-list-text="a."><p>Show that the optimal makespan is at least as large as the greatest processing time, that is,</p><p>≥</p><p>Sm*ax  max pk .</p><p>1≤k≤n</p></li><li data-list-text="b."><p>Show that the optimal makespan is at least as large as the average machine load, that is,</p><p>Sm*ax</p><p>1</p><p>X</p><p>≥ m   pk .</p><p>1≤k≤n</p><p>Suppose that we use the following greedy algorithm for parallel machine schedul- ing: whenever a machine is idle, schedule any job that has not yet been scheduled.</p></li><li data-list-text="c."><p>Write pseudocode to implement this greedy algorithm. What is the running time of your algorithm?</p></li><li data-list-text="d."><p>For the schedule returned by the greedy algorithm, show that</p></li></ol><p>1</p><p>Smax ≤</p><p>X pk</p><p>C max pk .</p><p>m</p><p>1≤k≤n</p><p>1≤k≤n</p><p>Conclude that this algorithm is a polynomial-time 2-approximation algorithm.</p></li><li data-list-text="35-6"><h3>Approximating a maximum spanning tree</h3><p>D f   W 2 g</p><p>2           2        D      f    g</p><p>Let G D (V, E) be an undirected graph with distinct edge weights r(u, v) on each edge (u, v)  E. For each vertex v  V , let max(v)  max(u;v)2E r(u, v) be the maximum-weight edge incident on that vertex. Let SG max(v) v  V</p><p>P</p><p>be the set of maximum-weight edges incident on each vertex, and let TG be the maximum-weight spanning tree of G, that is, the spanning tree of maximum total weight. For any subset of edges E0 $ E, define r(E0) D  (u;v)2E0 r(u, v).</p><ol><li data-list-text="a."><p>Give an example of a graph with at least 4 vertices for which SG D TG.</p></li><li data-list-text="b."><p>Give an example of a graph with at least 4 vertices for which SG ¤ TG.</p></li><li data-list-text="c."><p>Prove that SG $ TG for any graph G.</p></li><li data-list-text="d."><p>Prove that r(TG) ≥ r(SG)/2 for any graph G.</p><p>C</p></li><li data-list-text="e."><p>Give an O(V  E)-time algorithm to compute a 2-approximation to the maxi- mum spanning tree.</p></li></ol></li><li data-list-text="35-7"><h3>An approximation algorithm for the 0-1 knapsack problem</h3></li></ol></ol><p>≥ ≥ " " " ≥</p><p>Recall the knapsack problem from Section 16.2. There are n items, where the i th item is worth vi dollars and weighs ri pounds. We are also given a knapsack that can hold at most W pounds. Here, we add the further assumptions that each weight ri is at most W and that the items are indexed in monotonically decreasing order of their values: v1 v2 vn.</p><p>In the 0-1 knapsack problem, we wish to find a subset of the items whose total weight is at most W and whose total value is maximum. The fractional knapsack problem is like the 0-1 knapsack problem, except that we are allowed to take a fraction of each item, rather than being restricted to taking either all or none of</p><p>≤  ≤</p><p>each item. If we take a fraction xi of item i , where 0   xi 1, we contribute xi ri to the weight of the knapsack and receive value xivi . Our goal is to develop a polynomial-time 2-approximation algorithm for the 0-1 knapsack problem.</p><p>D                   —</p><p>In order to design a polynomial-time algorithm, we consider restricted instances of the 0-1 knapsack problem. Given an instance I of the knapsack problem, we form restricted instances Ij , for j   1, 2, . . . , n, by removing items 1, 2, . . . , j  1 and requiring the solution to include item j (all of item j in both the fractional and 0-1 knapsack problems). No items are removed in instance I1. For instance Ij , let Pj denote an optimal solution to the 0-1 problem and Qj denote an optimal solution to the fractional problem.</p><ol><li data-list-text="a."><p>Argue that an optimal solution to instance I of the 0-1 knapsack problem is one of fP1, P2,. . . , Png.</p></li><li data-list-text="b."><p>Prove that we can find an optimal solution Qj to the fractional problem for in- stance Ij by including item j and then using the greedy algorithm in which at each step, we take as much as possible of the unchosen item in the set fj C 1, j C 2, . . . , ng with maximum value per pound vi/ri .</p></li><li data-list-text="c."><p>Prove that we can always construct an optimal solution Qj to the fractional problem for instance Ij that includes at most one item fractionally. That is, for all items except possibly one, we either include all of the item or none of the item in the knapsack.</p><p>≥   ≥</p></li><li data-list-text="d."><p>Given an optimal solution Qj to the fractional problem for instance Ij , form solution Rj from Qj by deleting any fractional items from Qj . Let v(S) denote the total value of items taken in a solution S. Prove that v(Rj )   v(Qj )/2 v(Pj )/2.</p><p>f      g</p></li><li data-list-text="e."><p>Give a polynomial-time algorithm that returns a maximum-value solution from the set R1, R2,. . . , Rn , and prove that your algorithm is a polynomial-time 2-approximation algorithm for the 0-1 knapsack problem.</p></li></ol><h2>Chapter notes</h2><p>Although methods that do not necessarily compute exact solutions have been known for thousands of years (for example, methods to approximate the value of v), the notion of an approximation algorithm is much more recent. Hochbaum</p><p>[172] credits Garey, Graham, and Ullman [128] and Johnson [190] with formal- izing the concept of a polynomial-time approximation algorithm. The first such algorithm is often credited to Graham [149].</p><p>Since this early work, thousands of approximation algorithms have been de- signed for a wide range of problems, and there is a wealth of literature on this field. Recent texts by Ausiello et al. [26], Hochbaum [172], and Vazirani [345] deal exclusively with approximation algorithms, as do surveys by Shmoys [315] and Klein and Young [207]. Several other texts, such as Garey and Johnson [129] and Papadimitriou and Steiglitz [271], have significant coverage of approximation algorithms as well. Lawler, Lenstra, Rinnooy Kan, and Shmoys [225] provide an extensive treatment of approximation algorithms for the traveling-salesman prob- lem.</p><p>—</p><p>Papadimitriou and Steiglitz attribute the algorithm APPROX-VERTEX-COVER to F. Gavril and M. Yannakakis. The vertex-cover problem has been studied exten- sively (Hochbaum [172] lists 16 different approximation algorithms for this prob- lem), but all the approximation ratios are at least 2  o(1).</p><p>The algorithm APPROX-TSP-TOUR appears in a paper by Rosenkrantz, Stearns, and Lewis [298]. Christofides improved on this algorithm and gave a 3/2-approx- imation algorithm for the traveling-salesman problem with the triangle inequality. Arora [22] and Mitchell [257] have shown that if the points are in the euclidean plane, there is a polynomial-time approximation scheme. Theorem 35.3 is due to Sahni and Gonzalez [301].</p><p>The analysis of the greedy heuristic for the set-covering problem is modeled after the proof published by Chva´tal [68] of a more general result; the basic result as presented here is due to Johnson [190] and Lova´sz [238].</p><p>The algorithm APPROX-SUBSET-SUM and its analysis are loosely modeled after related approximation algorithms for the knapsack and subset-sum problems by Ibarra and Kim [187].</p><p>Problem 35-7 is a combinatorial version of a more general result on approximat- ing knapsack-type integer programs by Bienstock and McClosky [45].</p><p>The randomized algorithm for MAX-3-CNF satisfiability is implicit in the work of Johnson [190]. The weighted vertex-cover algorithm is by Hochbaum [171]. Section 35.4 only touches on the power of randomization and linear program- ming in the design of approximation algorithms. A combination of these two ideas yields a technique called “randomized rounding,” which formulates a problem as an integer linear program, solves the linear-programming relaxation, and interprets the variables in the solution as probabilities. These probabilities then help guide the solution of the original problem. This technique was first used by Raghavan and Thompson [290], and it has had many subsequent uses. (See Motwani, Naor, and Raghavan [261] for a survey.) Several other notable recent ideas in the field of approximation algorithms include the primal-dual method (see Goemans and Williamson [135] for a survey), finding sparse cuts for use in divide-and-conquer algorithms [229], and the use of semidefinite programming [134].</p><p>As mentioned in the chapter notes for Chapter 34, recent results in probabilisti- cally checkable proofs have led to lower bounds on the approximability of many problems, including several in this chapter. In addition to the references there, the chapter by Arora and Lund [23] contains a good description of the relation- ship between probabilistically checkable proofs and the hardness of approximating various problems.</p>
  </main>
</body>
</html>
